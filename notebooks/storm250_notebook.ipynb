{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3F4RBd2l6hl"
      },
      "source": [
        "# **Set Up Environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXfRLT8YnrKt"
      },
      "source": [
        "### **Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SA5Ou1el1b9",
        "outputId": "32526dbb-ecaf-48c1-9103-a2e3408a5a2e"
      },
      "outputs": [],
      "source": [
        "# Install latest package versions (Potential conflicts but updated)\n",
        "!pip install s3fs\n",
        "!pip install boto3\n",
        "!pip install arm_pyart\n",
        "!pip install pygrib\n",
        "!pip install pyhdfâ€ \n",
        "!pip install typing_extensions\n",
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb2Gofxrlwif"
      },
      "outputs": [],
      "source": [
        "# Install specific package versions (No conflicts but potentially outdated)\n",
        "# Last updated: MM-DD-YYYY\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhnJuTYOnvoe"
      },
      "source": [
        "### **Authentication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70CWZBwDnxpA",
        "outputId": "071a6e07-c916-4044-d2e8-b3e59e5cce51"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Login to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory to project path\n",
        "project_path = '/content/drive/MyDrive/RadarReflectivityProject'\n",
        "%cd $project_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddavf8_kqYrP"
      },
      "source": [
        "### **Package Versions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-6wMPjtrSV5",
        "outputId": "9960b398-b8bd-46e2-9408-a4da9a891109"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "List exact versions and save a timestamped requirements file under Logs/environment\n",
        "'''\n",
        "import os, sys, subprocess\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "\n",
        "def run(cmd):\n",
        "    return subprocess.check_output(cmd, text=True).strip()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Show interpreter & pip info (handy when reproducing)\n",
        "py_ver = sys.version.split()[0]\n",
        "pip_ver = run([sys.executable, \"-m\", \"pip\", \"--version\"])\n",
        "print(f\"Python: {py_ver}\")\n",
        "print(pip_ver)\n",
        "\n",
        "# Get fully resolved pins (includes direct URL installs, editable pkgs, etc.)\n",
        "lines = run([sys.executable, \"-m\", \"pip\", \"freeze\"]).splitlines()\n",
        "lines = sorted(lines, key=lambda s: s.lower())\n",
        "\n",
        "# Print a requirements-style block to the cell output\n",
        "print(\"\\n# ----- requirements.txt (pin everything) -----\")\n",
        "print(\"\\n\".join(lines))\n",
        "\n",
        "# Optional: also print as 'pip install ...' lines to the output\n",
        "print(\"\\n# ----- pip install lines (optional) -----\")\n",
        "for line in lines:\n",
        "    if line and not line.startswith(\"#\"):\n",
        "        print(f\"pip install {line}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Save to Logs/environment with a timestamped filename\n",
        "ts = datetime.now(timezone.utc).astimezone().strftime(\"%Y%m%d_%H%M%S%z\")\n",
        "out_dir = os.path.join(\"Logs\", \"environment\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "req_path = os.path.join(out_dir, f\"requirements_{ts}.txt\")\n",
        "\n",
        "# Include header comments with environment info (pip ignores lines starting with '#')\n",
        "header = [\n",
        "    f\"# Generated: {datetime.now(timezone.utc).astimezone().isoformat()}\",\n",
        "    f\"# Python: {py_ver}\",\n",
        "    f\"# {pip_ver}\",\n",
        "    \"# -------------------------\"\n",
        "]\n",
        "with open(req_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(header + lines) + \"\\n\")\n",
        "\n",
        "print(f\"\\nWrote {req_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpfuNvl6l10s"
      },
      "source": [
        "# **ðŸŸ¥ Build Full Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oz4JilSozHe"
      },
      "source": [
        "### **Filtering Logic**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGWuyJbRTvSm"
      },
      "source": [
        "#### **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "lLcd9JJJTyCw",
        "outputId": "19f913b5-f856-4b96-a632-bf9a2cc57a8e"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "General purpose helpers used across multiple modules\n",
        "'''\n",
        "import pyart\n",
        "import s3fs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import threading\n",
        "import queue\n",
        "import os\n",
        "import io\n",
        "import tarfile\n",
        "import pickle\n",
        "import re\n",
        "import ctypes\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from IPython import get_ipython\n",
        "from multiprocessing import Manager\n",
        "from IPython.display import display\n",
        "from datetime import datetime, timedelta, date\n",
        "from scipy.ndimage import label\n",
        "from shapely.geometry import Point\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyart\n",
        "\n",
        "\n",
        "\n",
        "########################################################################### PLOTTING ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "# Allow matplotlib to work concurrently\n",
        "mgr = Manager()\n",
        "raw_queue = mgr.Queue() # Workers write raw data here\n",
        "fig_queue = mgr.Queue() # Plotter thread hands back completed Figure objects here\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "\n",
        "def _save_plot(fig, plot_dir, func_name, stub, debug):\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot_dir:\n",
        "            subdir = os.path.join(plot_dir, func_name)\n",
        "            os.makedirs(subdir, exist_ok=True)\n",
        "            name = stub or func_name\n",
        "            out_path = os.path.join(subdir, f\"{name}.png\")\n",
        "            fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
        "            if debug:\n",
        "                print(f\"[{func_name}] saved debug plot -> {out_path}\")\n",
        "            plt.close(fig)\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"[{func_name}] saving/plotting failed: {e}\")\n",
        "\n",
        "\n",
        "def _plotter_loop():\n",
        "    \"\"\"\n",
        "    NOTE -> DEPRECATED, but keep for backwards compat until refactoring is done.\n",
        "    Decouple figure construction from main thread to eliminate blocking locks while plotting figures.\n",
        "    \"\"\"\n",
        "    print(\"[_plotter_loop] Queue monitoring started\")\n",
        "    while True:\n",
        "        item = raw_queue.get()\n",
        "        if item is None:                # sentinel to shut down\n",
        "            break\n",
        "\n",
        "        # Unpack the item in the queue\n",
        "        *payload, plot_type = item\n",
        "        print(f\"[_plotter_loop] Plotting figure for plot_type: {plot_type}\")\n",
        "\n",
        "        # Have the last \"argument\" in a queue's item determine the plot type\n",
        "        if plot_type == 'level_three':\n",
        "            T, R, echo_clean = payload\n",
        "            fig = plt.figure()\n",
        "            ax  = fig.add_subplot(projection='polar')\n",
        "            ax.set_theta_zero_location('N')\n",
        "            ax.set_theta_direction(-1)\n",
        "            pcm = ax.pcolormesh(T, R, echo_clean, shading='flat')\n",
        "            plt.colorbar(pcm, label='Echo Classification Code', ax=ax)\n",
        "            ax.set_title('Radar Echo Classification (Polar View)')\n",
        "\n",
        "            # Add processed fig to fig_queue\n",
        "            fig_queue.put(fig)\n",
        "\n",
        "        # EXAMPLE (add more elif blocks for new plot types)\n",
        "        elif plot_type == 'contour':\n",
        "            x, y, z = payload\n",
        "            fig, ax = plt.subplots()\n",
        "            cs = ax.contour(x, y, z)\n",
        "            ax.clabel(cs)\n",
        "            # And again, fig_queue.put(fig)\n",
        "\n",
        "\n",
        "\n",
        "############################################################################# MISC. ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "# Haversine distance (km) between two lon/lat points\n",
        "def haversine(lon1, lat1, lon2, lat2):\n",
        "    R = 6371.0  # Earth radius in km\n",
        "    dlon = radians(lon2 - lon1)\n",
        "    dlat = radians(lat2 - lat1)\n",
        "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
        "    return 2*R*asin(sqrt(a))\n",
        "\n",
        "\n",
        "def section_seperator(lines_to_print):\n",
        "    for _ in range(lines_to_print):\n",
        "      print()\n",
        "\n",
        "# Helper: ensure directory exists\n",
        "def _ensure_dir(path: str):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "######################################################################## RAM CLEANUP ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "def _safe_close(obj):\n",
        "    # Best-effort close for things that may hold file handles or buffers\n",
        "    for name in (\"close\", \"shutdown\", \"stop\", \"terminate\", \"flush\"):\n",
        "        try:\n",
        "            getattr(obj, name)()\n",
        "            return\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def aggressive_memory_cleanup(locals_dict):\n",
        "    # 1) Close plotting state (matplotlib holds a LOT)\n",
        "    try:\n",
        "        plt.close('all')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Shut down any executors/threads/queues you created\n",
        "    for k, v in list(locals_dict.items()):\n",
        "        # queues, threads, executors, file systems, netCDF datasets, s3fs/gcsfs, etc.\n",
        "        try:\n",
        "            _safe_close(v)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3) Drop large objects by name (customize this list for your code)\n",
        "    big_names = [\n",
        "        \"lsr_df\", \"spc_df\", \"grs_df\",\n",
        "        \"grouped\", \"linked_radar_df\", \"bboxed_df\",\n",
        "        \"files\", \"full_df\",\n",
        "        \"raw_queue\", \"plot_thread\",\n",
        "        # add anything else large that might linger (arrays, caches, etc.)\n",
        "    ]\n",
        "    for name in big_names:\n",
        "        if name in locals_dict:\n",
        "            locals_dict[name] = None\n",
        "\n",
        "    # 4) Clear module-level caches if you have them\n",
        "    for cache_name in [\n",
        "        \"_GRID_PLAN_CACHE\",     # your own cache variable (example from your earlier code)\n",
        "        \"_s3fs\", \"_s3client\",   # s3fs/boto (if you created globals)\n",
        "    ]:\n",
        "        if cache_name in globals():\n",
        "            try:\n",
        "                obj = globals()[cache_name]\n",
        "                _safe_close(obj)\n",
        "            except Exception:\n",
        "                pass\n",
        "            globals()[cache_name] = None\n",
        "\n",
        "    # 5) Free TF/PyTorch contexts if they exist (safe no-ops if not installed)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        tf.keras.backend.clear_session()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        import torch\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 6) Force Python GC\n",
        "    gc.collect()\n",
        "    gc.collect()  # call twice sometimes helps drop finalizers/cycles\n",
        "\n",
        "    # 7) Ask glibc to return free arenas to the OS (Linux only; works on Colab)\n",
        "    try:\n",
        "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXBbj56MWH4b"
      },
      "source": [
        "#### **Filter LSR + SPC Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjv-ZQXJ8qqy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "from datetime import datetime, date, time, timedelta\n",
        "from IPython.display import display\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "######################################################################## FILTER LSR DATA ###############################################################################\n",
        "\n",
        "\n",
        "def load_raw_lsr(start: Union[date, datetime],\n",
        "                 end:   Union[date, datetime],\n",
        "                 debug: bool = False,\n",
        "                 cache_dir: Union[str, Path] = \"Datasets/surface_obs_datasets/lsr_reports\",\n",
        "                 force_refresh: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pull raw Local Storm Reports (LSRs) from the IEM archive CSV service\n",
        "    for each calendar-day between `start` and `end` (inclusive).\n",
        "\n",
        "    Caching:\n",
        "      - a cache CSV is saved as {cache_dir}/YYYYMMDD_YYYYMMDD.csv\n",
        "      - if that file exists and force_refresh is False, it will be loaded and returned\n",
        "      - corrupted cache files are removed and the fetch is retried\n",
        "\n",
        "    Returns a DataFrame with columns:\n",
        "      - time (UTC datetime of the exact report)\n",
        "      - lat, lon (location of report)\n",
        "      - gust (the reported magnitude, as float)\n",
        "      - remarks (text comments, if any)\n",
        "      - type (the original TYPETEXT)\n",
        "    \"\"\"\n",
        "    cache_dir = Path(cache_dir)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Coerce start/end to datetimes (treat date inputs as full-day ranges) ---\n",
        "    if isinstance(start, date) and not isinstance(start, datetime):\n",
        "        start_dt = datetime.combine(start, time.min)\n",
        "    else:\n",
        "        start_dt = start\n",
        "\n",
        "    if isinstance(end, date) and not isinstance(end, datetime):\n",
        "        # Treat end date as inclusive through the end of day\n",
        "        end_dt = datetime.combine(end, time(23, 59, 59))\n",
        "    else:\n",
        "        end_dt = end\n",
        "\n",
        "    # Build cache filename using only YMD (matches your requested format)\n",
        "    cache_fname = cache_dir / f\"{start_dt:%Y%m%d}_{end_dt:%Y%m%d}.csv\"\n",
        "\n",
        "    # If present and not forcing refresh, try to load cache\n",
        "    if cache_fname.exists() and not force_refresh:\n",
        "        if debug:\n",
        "            print(f\"Loading LSRs from cache: {cache_fname}\")\n",
        "        try:\n",
        "            cached = pd.read_csv(cache_fname, parse_dates=['time'], engine='python')\n",
        "            # ensure columns are present and return subset to be safe\n",
        "            cols = ['time', 'lat', 'lon', 'gust', 'type']\n",
        "            missing = [c for c in cols if c not in cached.columns]\n",
        "            if missing:\n",
        "                raise ValueError(f\"Cached file missing columns: {missing}\")\n",
        "            return cached[cols]\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"Failed to read cache ({cache_fname}): {e}. Removing and refetching.\")\n",
        "            try:\n",
        "                cache_fname.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "            # fall through to re-fetch\n",
        "\n",
        "    # Fetch from IEM if cache not used ---\n",
        "    base_url = \"https://mesonet.agron.iastate.edu/cgi-bin/request/gis/lsr.py\"\n",
        "\n",
        "    # Format start/end as ISO8601 with trailing Z (include +1 sec to make end inclusive)\n",
        "    sts = start_dt.strftime('%Y-%m-%dT%H:%MZ')\n",
        "    ets = (end_dt + timedelta(seconds=1)).strftime('%Y-%m-%dT%H:%MZ')\n",
        "\n",
        "    params = {\n",
        "        'wfo': 'ALL',\n",
        "        'sts': sts,\n",
        "        'ets': ets,\n",
        "        'fmt': 'csv'\n",
        "    }\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\n Fetching LSRs from {sts} to {ets} with params {params} \\n\")\n",
        "\n",
        "    resp = requests.get(base_url, params=params)\n",
        "    resp.raise_for_status()\n",
        "    if debug:\n",
        "        print(\"Fetch HTTP\", resp.status_code, \"| Response snippet:\",\n",
        "              resp.text[:200].replace('\\n',' '))\n",
        "\n",
        "    # Read into pandas via StringIO\n",
        "    df = pd.read_csv(\n",
        "        StringIO(resp.text),\n",
        "        parse_dates=['VALID'],\n",
        "        engine='python',\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "    if debug:\n",
        "        print(f\"{len(df)} total reports before filtering\")\n",
        "\n",
        "    # Filter for thunderstorm-wind events with MG or EG\n",
        "    if debug:\n",
        "        display(df['TYPETEXT'].value_counts())\n",
        "    keep_codes = ['tstm wind',\n",
        "                  'tstm wnd gst',\n",
        "                  'non-tstm wnd gst',\n",
        "                  'marine tstm wind',\n",
        "                  ]\n",
        "    df = df[df['TYPETEXT'].str.lower().isin(keep_codes)]\n",
        "    df = df.reset_index(drop=True)\n",
        "    if debug:\n",
        "        print(f\" {len(df)} total reports remaining after TYPETEXT filtering\")\n",
        "\n",
        "    # Rename & select\n",
        "    df = df.rename(columns={\n",
        "        'VALID':  'time',\n",
        "        'LAT':    'lat',\n",
        "        'LON':    'lon',\n",
        "        'MAG':    'gust',\n",
        "        'REMARK': 'remarks',\n",
        "        'TYPETEXT': 'type'\n",
        "    })\n",
        "\n",
        "    # coerce gust to numeric then drop NaNs in critical fields\n",
        "    df['gust'] = pd.to_numeric(df['gust'], errors='coerce')\n",
        "    df = df.dropna(subset=['time','lat','lon','gust']).reset_index(drop=True)\n",
        "    if debug:\n",
        "        print(f\" {len(df)} total reports after dropping NaNs in critical fields\")\n",
        "\n",
        "    # keep only needed columns (and preserve order)\n",
        "    out = df[['time', 'lat', 'lon', 'gust', 'type']].copy()\n",
        "\n",
        "    # Save to cache atomically ---\n",
        "    try:\n",
        "        # write to a temporary file then atomically replace\n",
        "        with tempfile.NamedTemporaryFile(delete=False, dir=str(cache_dir), suffix='.csv') as tmp:\n",
        "            tmp_name = Path(tmp.name)\n",
        "        out.to_csv(tmp_name, index=False)\n",
        "        tmp_name.replace(cache_fname)\n",
        "        if debug:\n",
        "            print(f\"Saved LSR cache to {cache_fname}\")\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(\"Warning: failed to write cache:\", e)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def filter_lsr(lsr_df,\n",
        "               bounding_lat,\n",
        "               bounding_lon,\n",
        "               center_lat,\n",
        "               center_lon,\n",
        "               scan_time,\n",
        "               time_window=timedelta(minutes=5),\n",
        "               debug=True):\n",
        "    \"\"\"\n",
        "    - Filters pre-loaded lsr_df to time_window & bbox\n",
        "    - bounding_lat = (min_lat, max_lat)\n",
        "    - bounding_lon = (min_lon, max_lon)\n",
        "    Returns DataFrame of {source, time, station_lat, station_lon, gust, obs_distance}\n",
        "    \"\"\"\n",
        "    min_lat, max_lat = bounding_lat\n",
        "    min_lon, max_lon = bounding_lon\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] bounding_lat=({min_lat},{max_lat}), bounding_lon=({min_lon},{max_lon})\")\n",
        "        print(f\"[filter_lsr] scan_time={scan_time}, window=Â±{time_window}\")\n",
        "\n",
        "    # normalize scan_time to python datetime\n",
        "    if hasattr(scan_time, 'strftime') and not isinstance(scan_time, pd.Timestamp):\n",
        "        try:\n",
        "            scan_time_dt = datetime(\n",
        "                scan_time.year, scan_time.month, scan_time.day,\n",
        "                scan_time.hour, scan_time.minute, scan_time.second\n",
        "            )\n",
        "            if debug:\n",
        "                print(f\"[filter_lsr] converted scan_time to datetime: {scan_time_dt}\")\n",
        "        except Exception:\n",
        "            scan_time_dt = pd.to_datetime(str(scan_time))\n",
        "            if debug:\n",
        "                print(f\"[filter_lsr] fallback converted scan_time via to_datetime: {scan_time_dt}\")\n",
        "    else:\n",
        "        scan_time_dt = pd.Timestamp(scan_time)\n",
        "        if debug:\n",
        "            print(f\"[filter_lsr] scan_time is Timestamp: {scan_time_dt}\")\n",
        "\n",
        "    # copy dataframe\n",
        "    df = lsr_df.copy()\n",
        "\n",
        "    # Normalize time column to pandas Timestamps\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] coerced time dtype = {df['time'].dtype}\")\n",
        "\n",
        "    # Build pandas Timestamps for the window\n",
        "    start_ts = pd.Timestamp(scan_time_dt) - time_window\n",
        "    end_ts   = pd.Timestamp(scan_time_dt) + time_window\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] time window start={start_ts}, end={end_ts}\")\n",
        "\n",
        "    # Time filter\n",
        "    df = df[df['time'].between(start_ts, end_ts)]\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] after time filter: {len(df)} records\")\n",
        "\n",
        "\n",
        "    # spatial filter using bounding box\n",
        "    df = df[(df['lat'] >= min_lat) & (df['lat'] <= max_lat) &\n",
        "            (df['lon'] >= min_lon) & (df['lon'] <= max_lon)]\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] records after spatial filter: {len(df)}\")\n",
        "\n",
        "    # compute distance from cell center\n",
        "    df['obs_distance'] = df.apply(\n",
        "        lambda r: haversine(center_lon, center_lat, r['lon'], r['lat']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # rename station coordinates\n",
        "    df = df.rename(columns={'lat': 'station_lat', 'lon': 'station_lon'})\n",
        "    df['source'] = 'lsr_iastate'\n",
        "\n",
        "    # select and order columns to match filter_synoptic\n",
        "    result = df[['source', 'time', 'station_lat', 'station_lon', 'gust', 'obs_distance']]\n",
        "    if debug:\n",
        "        print(f\"[filter_lsr] final returned records: {len(result)}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "##################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def load_raw_spc(start: datetime,\n",
        "                 end:   datetime,\n",
        "                 spc_dir: str = \"Datasets/surface_obs_datasets/spc_reports\",\n",
        "                 debug: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load SPC per-year 'wind' CSV(s) from a folder and return a DataFrame with:\n",
        "      - time (python/pandas datetime)\n",
        "      - lat, lon (averaged from slat/elat and slon/elon)\n",
        "      - gust (float; from `mag`)\n",
        "      - type (string; 'spc_wind')\n",
        "\n",
        "    Parameters:\n",
        "    - start, end: datetime objects (inclusive range)\n",
        "    - spc_dir: path to folder containing YYYY_wind.csv files. Can be relative to\n",
        "               current working dir or a path under /content/drive/My Drive/...\n",
        "    - debug: enable debug printing\n",
        "    \"\"\"\n",
        "    # Build candidate folder paths to try (helps with Colab Drive mounts)\n",
        "    candidates = [spc_dir,\n",
        "                  os.path.join(\"/content/drive/My Drive\", spc_dir),\n",
        "                  os.path.join(\"/content/drive/MyDrive\", spc_dir),\n",
        "                  os.path.expanduser(spc_dir)]\n",
        "    folder = None\n",
        "    for p in candidates:\n",
        "        if p and os.path.isdir(p):\n",
        "            folder = p\n",
        "            break\n",
        "\n",
        "    if folder is None:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find SPC folder. Tried: {candidates}. \"\n",
        "            \"Make sure your Drive is mounted and the folder exists.\"\n",
        "        )\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[load_raw_spc] using folder: {folder}\")\n",
        "\n",
        "    # Determine which year files we need\n",
        "    years = list(range(start.year, end.year + 1))\n",
        "    if debug:\n",
        "        print(f\"[load_raw_spc] loading years: {years}\")\n",
        "\n",
        "    dfs = []\n",
        "    for y in years:\n",
        "        fname = f\"{y}_wind.csv\"\n",
        "        fpath = os.path.join(folder, fname)\n",
        "        if os.path.isfile(fpath):\n",
        "            if debug:\n",
        "                print(f\"[load_raw_spc] reading {fpath}\")\n",
        "            # read with pandas\n",
        "            try:\n",
        "                dfi = pd.read_csv(fpath, low_memory=False)\n",
        "                dfi['__source_file'] = fname\n",
        "                dfs.append(dfi)\n",
        "            except Exception as e:\n",
        "                if debug:\n",
        "                    print(f\"[load_raw_spc] failed to read {fpath}: {e}\")\n",
        "        else:\n",
        "            if debug:\n",
        "                print(f\"[load_raw_spc] file not found: {fpath} (skipping)\")\n",
        "\n",
        "    if not dfs:\n",
        "        raise FileNotFoundError(f\"No SPC wind CSVs found in {folder} for years {years}.\")\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "    if debug:\n",
        "        print(f\"[load_raw_spc] concatenated rows: {len(df)}\")\n",
        "\n",
        "    # Build time column from `date` + `time` (both expected in file)\n",
        "    # Examples in file: date='2024-12-29', time='03:45:00'\n",
        "    df['time'] = pd.to_datetime(df['date'].astype(str).str.strip() + ' ' +\n",
        "                                df['time'].astype(str).str.strip(),\n",
        "                                errors='coerce')\n",
        "\n",
        "    if debug:\n",
        "        n_bad_time = df['time'].isna().sum()\n",
        "        print(f\"[load_raw_spc] parsed time; {n_bad_time} rows failed to parse time\")\n",
        "\n",
        "    # Compute lat/lon as the mean of start/end lat/lon\n",
        "    # tolerantly coerce columns to numeric\n",
        "    for c in ['slat','elat','slon','elon','mag']:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "        else:\n",
        "            df[c] = np.nan\n",
        "\n",
        "    df['lat'] = df[['slat','elat']].mean(axis=1)\n",
        "    df['lon'] = df[['slon','elon']].mean(axis=1)\n",
        "\n",
        "    # gust from mag\n",
        "    df['gust'] = df['mag']\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[load_raw_spc] after coordinate/gust computation: rows={len(df)}\")\n",
        "        print(f\"  lat/nulls={df['lat'].isna().sum()}, lon/nulls={df['lon'].isna().sum()}, gust/nulls={df['gust'].isna().sum()}\")\n",
        "\n",
        "    # drop rows missing critical fields\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=['time','lat','lon','gust']).reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    if debug:\n",
        "        print(f\"[load_raw_spc] dropped {before-after} rows missing time/lat/lon/gust -> {len(df)} remaining\")\n",
        "\n",
        "    # Keep / rename columns to match your LSR loader\n",
        "    df_out = pd.DataFrame({\n",
        "        'time': df['time'],\n",
        "        'lat': df['lat'],\n",
        "        'lon': df['lon'],\n",
        "        'gust': df['gust'].astype(float),\n",
        "        'type': 'spc_wind'\n",
        "    })\n",
        "\n",
        "    return df_out[['time','lat','lon','gust','type']]\n",
        "\n",
        "\n",
        "def filter_spc(spc_df,\n",
        "               bounding_lat,\n",
        "               bounding_lon,\n",
        "               center_lat,\n",
        "               center_lon,\n",
        "               scan_time,\n",
        "               time_window=timedelta(minutes=5),\n",
        "               debug=True):\n",
        "    \"\"\"\n",
        "    Filters pre-loaded spc_df to time_window & bbox and returns:\n",
        "      ['source','time','station_lat','station_lon','gust','obs_distance']\n",
        "\n",
        "    - bounding_lat = (min_lat, max_lat)\n",
        "    - bounding_lon = (min_lon, max_lon)\n",
        "    - scan_time may be datetime or pandas Timestamp\n",
        "    \"\"\"\n",
        "    min_lat, max_lat = bounding_lat\n",
        "    min_lon, max_lon = bounding_lon\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] bounding_lat=({min_lat},{max_lat}), bounding_lon=({min_lon},{max_lon})\")\n",
        "        print(f\"[filter_spc] scan_time={scan_time}, window=Â±{time_window}\")\n",
        "\n",
        "    # Normalize scan_time to a naive datetime\n",
        "    if hasattr(scan_time, 'strftime') and not isinstance(scan_time, pd.Timestamp):\n",
        "        try:\n",
        "            scan_time_dt = datetime(\n",
        "                scan_time.year, scan_time.month, scan_time.day,\n",
        "                scan_time.hour, scan_time.minute, scan_time.second\n",
        "            )\n",
        "            if debug:\n",
        "                print(f\"[filter_spc] converted scan_time to datetime: {scan_time_dt}\")\n",
        "        except Exception:\n",
        "            scan_time_dt = pd.to_datetime(str(scan_time))\n",
        "            if debug:\n",
        "                print(f\"[filter_spc] fallback converted scan_time via to_datetime: {scan_time_dt}\")\n",
        "    else:\n",
        "        scan_time_dt = pd.Timestamp(scan_time)\n",
        "        if debug:\n",
        "            print(f\"[filter_spc] scan_time is Timestamp: {scan_time_dt}\")\n",
        "\n",
        "    # Work on a copy\n",
        "    df = spc_df.copy()\n",
        "\n",
        "    # Normalise and ensure time is datetime\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] coerced time dtype = {df['time'].dtype}\")\n",
        "\n",
        "    start_ts = pd.Timestamp(scan_time_dt) - time_window\n",
        "    end_ts   = pd.Timestamp(scan_time_dt) + time_window\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] time window start={start_ts}, end={end_ts}\")\n",
        "\n",
        "    # Time filter\n",
        "    df = df[df['time'].between(start_ts, end_ts)]\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] after time filter: {len(df)} records\")\n",
        "\n",
        "    # Spatial filter using bounding box\n",
        "    df = df[(df['lat'] >= min_lat) & (df['lat'] <= max_lat) &\n",
        "            (df['lon'] >= min_lon) & (df['lon'] <= max_lon)]\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] records after spatial filter: {len(df)}\")\n",
        "\n",
        "    # compute distance from cell center (center_lon, center_lat)\n",
        "    df['obs_distance'] = df.apply(\n",
        "        lambda r: haversine(center_lon, center_lat, r['lon'], r['lat']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # rename station coordinates\n",
        "    df = df.rename(columns={'lat': 'station_lat', 'lon': 'station_lon'})\n",
        "    df['source'] = 'spc'\n",
        "\n",
        "    result = df[['source', 'time', 'station_lat', 'station_lon', 'gust', 'obs_distance']].reset_index(drop=True)\n",
        "    if debug:\n",
        "        print(f\"[filter_spc] final returned records: {len(result)}\")\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9zVeQcJlAqb"
      },
      "source": [
        "#### **Filter Synoptic Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dTiGo3_kYti"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "from math import radians, cos\n",
        "\n",
        "\n",
        "SYNOPTIC_TOKEN = userdata.get('SYNOPTIC_TOKEN')\n",
        "\n",
        "\n",
        "def filter_synoptic(bounding_lat,\n",
        "                    bounding_lon,\n",
        "                    center_lat,\n",
        "                    center_lon,\n",
        "                    scan_time,\n",
        "                    time_window=timedelta(minutes=5),\n",
        "                    debug=True):\n",
        "    \"\"\"\n",
        "    - bounding_lat = (min_lat, max_lat)\n",
        "    - bounding_lon = (min_lon, max_lon)\n",
        "    - scan_time in UTC\n",
        "    Returns DataFrame of {source, time, lat, lon, gust, obs_distance}\n",
        "    \"\"\"\n",
        "    min_lat, max_lat = bounding_lat\n",
        "    min_lon, max_lon = bounding_lon\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[filter_synoptic] bbox lat=({min_lat},{max_lat}), lon=({min_lon},{max_lon})\")\n",
        "        print(f\"[filter_synoptic] scan_time={scan_time.isoformat()} window=Â±{time_window}\")\n",
        "\n",
        "    # Build bounding box string param\n",
        "    bbox = f\"{min_lon},{min_lat},{max_lon},{max_lat}\"\n",
        "    if debug:\n",
        "        print(f\"[filter_synoptic] bbox string = '{bbox}'\")\n",
        "\n",
        "    # Format time window in UTC as YYYYmmddHHMM\n",
        "    start = (scan_time - time_window).strftime(\"%Y%m%d%H%M\")\n",
        "    end   = (scan_time + time_window).strftime(\"%Y%m%d%H%M\")\n",
        "    if debug:\n",
        "        print(f\"[filter_synoptic] time window start={start}, end={end}\")\n",
        "\n",
        "    # Request the time series\n",
        "    url = \"https://api.synopticdata.com/v2/stations/timeseries\"\n",
        "    params = {\n",
        "      \"token\": SYNOPTIC_TOKEN,\n",
        "      \"bbox\":  bbox,\n",
        "      \"start\": start,\n",
        "      \"end\":   end,\n",
        "      \"vars\":   \"wind_gust\",\n",
        "\n",
        "      # ASOS/AWOS -> High Quality, airport data, sparse  |  ID = 1\n",
        "      # CWOP/APRSWXNET -> Individual stations, dense     |  ID = 65\n",
        "      # RAWS -> Off grid states, dense                   |  ID = 2\n",
        "      # HADS ->                                          |  ID = 106\n",
        "      # [Add more networks as needed]\n",
        "      \"network\": \"1, 65, 106, 2\",\n",
        "\n",
        "      \"qc\":     \"on\",\n",
        "      \"showemptystations\": 1,\n",
        "      \"units\":  \"english\"\n",
        "    }\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[filter_synoptic] GET {url} with params={params}\")\n",
        "    r = requests.get(url, params=params)\n",
        "    r.raise_for_status()\n",
        "    resp = r.json()\n",
        "\n",
        "\n",
        "    if debug:\n",
        "        # print top-level summary\n",
        "        print(\"SUMMARY:\", resp.get(\"SUMMARY\", {}))\n",
        "\n",
        "        # print how many stations\n",
        "        n = len(resp.get(\"STATION\", []))\n",
        "        print(\"STATION count:\", n)\n",
        "\n",
        "\n",
        "\n",
        "    if debug:\n",
        "        n_stations = len(resp.get(\"STATION\", []))\n",
        "        print(f\"[filter_synoptic] received data for {n_stations} stations\")\n",
        "\n",
        "    # Parse out each stationâ€™s observations\n",
        "    recs = []\n",
        "    for st in resp.get(\"STATION\", []):\n",
        "        st_lat = float(st.get(\"LATITUDE\"))\n",
        "        st_lon = float(st.get(\"LONGITUDE\"))\n",
        "        obs = st.get(\"OBSERVATIONS\", {})\n",
        "\n",
        "        # detect network id from common station fields (MNET_ID seen in dumps)\n",
        "        network_id = (\n",
        "            st.get(\"MNET_ID\")\n",
        "            or st.get(\"MNET\")\n",
        "            or st.get(\"NETWORK\")\n",
        "            or st.get(\"NETWORK_ID\")\n",
        "        )\n",
        "        # normalize to simple string (or None)\n",
        "        network_id = str(network_id) if network_id not in (None, \"\", \"None\") else None\n",
        "\n",
        "        # build source label like \"synoptic_1\" or fallback \"synoptic\"\n",
        "        src_label = f\"synoptic_{network_id}\" if network_id else \"synoptic\"\n",
        "\n",
        "        # Prefer wind_gust_set_1, fall back to wind_gust\n",
        "        gusts = obs.get(\"wind_gust_set_1\") or obs.get(\"wind_gust\") or []\n",
        "\n",
        "        # Try several possible time arrays, then fall back to date_time\n",
        "        times = (\n",
        "            obs.get(\"wind_gust_time_utc_1\")\n",
        "            or obs.get(\"wind_gust_time_utc\")\n",
        "            or obs.get(\"wind_gust_time\")\n",
        "            or obs.get(\"date_time\")\n",
        "            or []\n",
        "        )\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[filter_synoptic] Station {st['STID']} @ ({st_lat},{st_lon}): gusts={len(gusts)}, times={len(times)}\")\n",
        "\n",
        "        # zip will iterate up to the shortest list; that's fine\n",
        "        for t_str, g in zip(times, gusts):\n",
        "            # skip missing gusts\n",
        "            if g is None:\n",
        "                continue\n",
        "\n",
        "            # parse timestamp\n",
        "            try:\n",
        "                t = datetime.strptime(t_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "            except Exception:\n",
        "                if debug:\n",
        "                    print(f\"[filter_synoptic] skipping bad timestamp '{t_str}' for {st['STID']}\")\n",
        "                continue\n",
        "\n",
        "            # ensure numeric gust\n",
        "            try:\n",
        "                g_val = float(g)\n",
        "            except Exception:\n",
        "                if debug:\n",
        "                    print(f\"[filter_synoptic] skipping non-numeric gust '{g}' for {st['STID']}\")\n",
        "                continue\n",
        "\n",
        "            dist = haversine(st_lon, st_lat, center_lon, center_lat)\n",
        "\n",
        "            # NOTE -> this must be aligned with dataframe returned by filter_lsr\n",
        "            recs.append({\n",
        "                \"source\":      src_label,\n",
        "                \"time\":        t,\n",
        "                \"station_lat\": st_lat,\n",
        "                \"station_lon\": st_lon,\n",
        "                \"gust\":        g_val,\n",
        "                \"obs_distance\": dist\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(recs)\n",
        "    if debug:\n",
        "        print(f\"[filter_synoptic] total records = {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDlsMWYcf6Zp"
      },
      "source": [
        "#### **Filter GSR Tracks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DACrsH13WIic"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "############################################################################ HELPERS ############################################################################\n",
        "\n",
        "\n",
        "def _prepare_site_arrays(radar_info: dict):\n",
        "    \"\"\"\n",
        "    From dict-of-dicts radar_info -> (names, lat_array, lon_array, list_of_range_arrays)\n",
        "    Each range array is shape (R, 2) of int64 nanoseconds [start_ns, end_ns].\n",
        "    None -> open interval (min/max int64).\n",
        "    \"\"\"\n",
        "    names, lats, lons, ranges = [], [], [], []\n",
        "    int_min, int_max = np.iinfo(np.int64).min, np.iinfo(np.int64).max\n",
        "\n",
        "    for site, meta in radar_info.items():\n",
        "        pos = (meta or {}).get('position', {})\n",
        "        lat = pos.get('lat', None)\n",
        "        lon = pos.get('lon', None)\n",
        "        if lat is None or lon is None:\n",
        "            continue  # skip incomplete entries\n",
        "\n",
        "        # normalize active_ranges\n",
        "        ar = (meta or {}).get('active_ranges') or [{'start': None, 'end': None}]\n",
        "        rs = []\n",
        "        for span in ar:\n",
        "            s = span.get('start', None)\n",
        "            e = span.get('end', None)\n",
        "            # cast to UTC ns; None -> open\n",
        "            s_ns = int_min if s is None else pd.to_datetime(s, utc=True).value\n",
        "            e_ns = int_max if e is None else pd.to_datetime(e, utc=True).value\n",
        "            rs.append((s_ns, e_ns))\n",
        "\n",
        "        names.append(site)\n",
        "        lats.append(lat)\n",
        "        lons.append(lon)\n",
        "        ranges.append(np.asarray(rs, dtype=np.int64))  # (R, 2)\n",
        "\n",
        "    return (\n",
        "        names,\n",
        "        np.asarray(lats, dtype=np.float32),\n",
        "        np.asarray(lons, dtype=np.float32),\n",
        "        ranges,  # list of (R,2) int64 arrays per site\n",
        "    )\n",
        "\n",
        "\n",
        "def _active_mask_for_chunk(t_chunk_ns: np.ndarray, site_ranges: list) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a boolean mask [L, S] where True means site S is active at row-time L.\n",
        "    site_ranges[j] is an (R,2) ns array of [start,end] for site j (open intervals handled already).\n",
        "    \"\"\"\n",
        "    L = t_chunk_ns.shape[0]\n",
        "    S = len(site_ranges)\n",
        "    mask = np.zeros((L, S), dtype=bool)\n",
        "\n",
        "    # vectorize per-site (rows x ranges), then OR over ranges\n",
        "    for j, rs in enumerate(site_ranges):\n",
        "        if rs.size == 0:\n",
        "            continue\n",
        "        starts = rs[:, 0][None, :]  # [1, R]\n",
        "        ends   = rs[:, 1][None, :]  # [1, R]\n",
        "        tt     = t_chunk_ns[:, None]  # [L, 1]\n",
        "        m = (tt >= starts) & (tt <= ends)  # [L, R]\n",
        "        mask[:, j] = m.any(axis=1)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "######################################################################## LOAD GRS TRACKS ##########################################################################\n",
        "\n",
        "\n",
        "def load_grs_tracks(year: int,\n",
        "                    radar_info: dict,\n",
        "                    base_url: str = \"https://data-osdf.rda.ucar.edu/ncar/rda/d841006/tracks\",\n",
        "                    min_rows: int = 5,\n",
        "                    max_distance_km: float = 250.0,\n",
        "                    debug: bool = False,\n",
        "                    timeout: float = 20.0,\n",
        "                    save_dir: str = \"Datasets/cell_tracks/raw_grs\",\n",
        "                    processed_cache_dir: str = \"Datasets/cell_tracks/processed_grs\",\n",
        "                    max_workers: int = 10,\n",
        "                    chunk_size: int = 100_000,\n",
        "                    max_gap_hours: float = 6.0   # â† NEW: continuity gap threshold\n",
        "                    ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download & cache daily GR-S CSVs, parse them in parallel (fast C-engine if possible),\n",
        "    compute nearest radar site (vectorized chunked haversine), split multi-site storms\n",
        "    (vectorized), drop small groups, snake_case columns, and save processed-year cache.\n",
        "\n",
        "    All original debug printing and logic preserved.\n",
        "    \"\"\"\n",
        "    # ensure cache dirs\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(processed_cache_dir, exist_ok=True)\n",
        "\n",
        "    processed_parquet = os.path.join(processed_cache_dir, f\"{year}.parquet\")\n",
        "    processed_csv = os.path.join(processed_cache_dir, f\"{year}.csv\")\n",
        "\n",
        "    # Quick processed-cache short-circuit\n",
        "    if os.path.exists(processed_parquet):\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] loading processed cache {processed_parquet}\")\n",
        "        try:\n",
        "            return pd.read_parquet(processed_parquet)\n",
        "        except Exception:\n",
        "            if debug:\n",
        "                print(\"[load_grs_tracks] parquet read failed â€” will try CSV or re-process\")\n",
        "            if os.path.exists(processed_csv):\n",
        "                return pd.read_csv(processed_csv)\n",
        "\n",
        "    # helper to download a single day (safe atomic write)\n",
        "    def _download_day(url, local_path):\n",
        "        tmp = local_path + \".tmp\"\n",
        "        try:\n",
        "            r = requests.get(url, allow_redirects=True, timeout=timeout)\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[download] request error {url}: {e}\")\n",
        "            return False, url\n",
        "        if r.status_code != 200:\n",
        "            return False, url\n",
        "        content = r.content\n",
        "        if not content or len(content) < 50:\n",
        "            return False, url\n",
        "        try:\n",
        "            with open(tmp, \"wb\") as f:\n",
        "                f.write(content)\n",
        "            os.replace(tmp, local_path)\n",
        "            return True, url\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[download] write error for {local_path}: {e}\")\n",
        "            try:\n",
        "                if os.path.exists(tmp):\n",
        "                    os.remove(tmp)\n",
        "            except Exception:\n",
        "                pass\n",
        "            return False, url\n",
        "\n",
        "    # Build list of all YYYYMMDD dates for the year\n",
        "    start_date = datetime(year, 1, 1)\n",
        "    end_date = datetime(year, 12, 31)\n",
        "    dates = []\n",
        "    d = start_date\n",
        "    while d <= end_date:\n",
        "        dates.append(d)\n",
        "        d += timedelta(days=1)\n",
        "\n",
        "    # Check local cache and collect missing days\n",
        "    missing = []\n",
        "    for day in dates:\n",
        "        ymd = day.strftime(\"%Y%m%d\")\n",
        "        local_path = os.path.join(save_dir, f\"{ymd}.csv\")\n",
        "        if not (os.path.exists(local_path) and os.path.getsize(local_path) > 50):\n",
        "            missing.append((day, local_path))\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] {len(dates)} days in year, {len(missing)} missing cached files\")\n",
        "\n",
        "    # Download missing days in parallel (I/O bound)\n",
        "    if missing:\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] launching downloader with max_workers={max_workers} ...\")\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "            futures = {}\n",
        "            for day, local_path in missing:\n",
        "                ymd = day.strftime(\"%Y%m%d\")\n",
        "                url = f\"{base_url}/{year}/{ymd}.csv\"\n",
        "                futures[ex.submit(_download_day, url, local_path)] = (day, local_path)\n",
        "            for fut in as_completed(futures):\n",
        "                ok, url = fut.result()\n",
        "                if debug:\n",
        "                    if ok:\n",
        "                        print(f\"[load_grs_tracks] downloaded: {url}\")\n",
        "                    else:\n",
        "                        print(f\"[load_grs_tracks] missed / 404 or error: {url}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Parse all cached daily files concurrently (fast local I/O, C engine first)\n",
        "    def _parse_local_file_path(local_path):\n",
        "        \"\"\"\n",
        "        Parse a local CSV file path. Try fast C-engine with common delimiters first,\n",
        "        fall back to python engine auto-detect if necessary. Return DataFrame or None.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1) try comma with C engine\n",
        "            try:\n",
        "                df = pd.read_csv(local_path, sep=\",\", engine=\"c\", low_memory=False)\n",
        "                if df.shape[1] > 1:\n",
        "                    return df\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # 2) try tab with C engine\n",
        "            try:\n",
        "                df = pd.read_csv(local_path, sep=\"\\t\", engine=\"c\", low_memory=False)\n",
        "                if df.shape[1] > 1:\n",
        "                    return df\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # 3) fallback: python engine auto-detect (slower)\n",
        "            try:\n",
        "                df = pd.read_csv(local_path, sep=None, engine=\"python\")\n",
        "                return df\n",
        "            except Exception:\n",
        "                return None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # collect list of local paths that exist\n",
        "    local_paths = []\n",
        "    for day in dates:\n",
        "        ymd = day.strftime(\"%Y%m%d\")\n",
        "        local_path = os.path.join(save_dir, f\"{ymd}.csv\")\n",
        "        if os.path.exists(local_path) and os.path.getsize(local_path) > 50:\n",
        "            local_paths.append(local_path)\n",
        "        else:\n",
        "            if debug:\n",
        "                print(f\"[load_grs_tracks] no file for {ymd} @ path {local_path} skipping\")\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] parsing {len(local_paths)} cached files with max_workers={max_workers}\")\n",
        "\n",
        "    frames = []\n",
        "    # Use a ThreadPoolExecutor; pandas C engine releases the GIL so threads are effective here\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futures = {ex.submit(_parse_local_file_path, p): p for p in local_paths}\n",
        "        for fut in as_completed(futures):\n",
        "            p = futures[fut]\n",
        "            try:\n",
        "                df = fut.result()\n",
        "            except Exception as e:\n",
        "                df = None\n",
        "                if debug:\n",
        "                    print(f\"[load_grs_tracks] parse worker failed for {p}: {e}\")\n",
        "            if df is not None:\n",
        "                # same debug-style print line you used\n",
        "                if debug:\n",
        "                    print(f\"[load_grs_tracks] parsed {p} -> rows={len(df)}, cols={len(df.columns)}\")\n",
        "                frames.append(df)\n",
        "            else:\n",
        "                if debug:\n",
        "                    print(f\"[load_grs_tracks] parsed {p} -> returned None or failed\")\n",
        "\n",
        "    if not frames:\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] no track files parsed for year {year}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # CONCATENATE\n",
        "    grs = pd.concat(frames, ignore_index=True, sort=False)\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] concatenated {len(frames)} files -> {len(grs)} rows\")\n",
        "        # show columns for debugging\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] concatenated columns: {list(grs.columns)}\")\n",
        "\n",
        "    # IMPORTANT: canonicalize core column names after concat\n",
        "    # Strip whitespace from column names and collapse repeated spaces\n",
        "    new_cols = []\n",
        "    for c in grs.columns:\n",
        "        # make sure it's a string\n",
        "        s = str(c).strip()\n",
        "        new_cols.append(s)\n",
        "    grs.columns = new_cols\n",
        "\n",
        "    # heuristically find the lon/lat/time/storm columns (case-insensitive)\n",
        "    cols_lower = {c.lower(): c for c in grs.columns}\n",
        "\n",
        "    def _find_col(*keywords):\n",
        "        for lowname, orig in cols_lower.items():\n",
        "            if all(k.lower() in lowname for k in keywords):\n",
        "                return orig\n",
        "        return None\n",
        "\n",
        "    # find storm column (we only need it in order to create _orig_storm_number if missing)\n",
        "    storm_col_found = _find_col(\"storm\", \"number\") or _find_col(\"storm\") or next((c for c in grs.columns if \"storm\" in c.lower()), None)\n",
        "    time_col_found = _find_col(\"time\") or next((c for c in grs.columns if \"time\" in c.lower()), None)\n",
        "    lon_col_found = _find_col(\"long\") or next((c for c in grs.columns if \"lon\" in c.lower()), None)\n",
        "    lat_col_found = _find_col(\"lat\") or next((c for c in grs.columns if \"lat\" in c.lower()), None)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] detected post-concat columns -> storm: {storm_col_found}, time: {time_col_found}, lon: {lon_col_found}, lat: {lat_col_found}\")\n",
        "\n",
        "    if not (lon_col_found and lat_col_found and time_col_found):\n",
        "        # If we can't find core columns, throw a helpful error with available columns\n",
        "        raise KeyError(f\"Couldn't find longitude/latitude/time columns after concat. Available columns: {list(grs.columns)}\")\n",
        "\n",
        "    # rename canonical columns so downstream code can rely on them\n",
        "    rename_map = {}\n",
        "    rename_map[lon_col_found] = \"Longitude\"\n",
        "    rename_map[lat_col_found] = \"Latitude\"\n",
        "    rename_map[time_col_found] = \"Time\"\n",
        "    grs = grs.rename(columns=rename_map)\n",
        "\n",
        "    # Ensure _orig_storm_number exists (may be named differently in files)\n",
        "    if \"_orig_storm_number\" not in grs.columns:\n",
        "        if storm_col_found:\n",
        "            grs[\"_orig_storm_number\"] = pd.to_numeric(grs[storm_col_found], errors=\"coerce\").astype('Int64')\n",
        "        else:\n",
        "            # attempt any column with 'storm' substring\n",
        "            possible = next((c for c in grs.columns if \"storm\" in c.lower()), None)\n",
        "            if possible:\n",
        "                grs[\"_orig_storm_number\"] = pd.to_numeric(grs[possible], errors=\"coerce\").astype('Int64')\n",
        "\n",
        "    # Now the original dropna call is safe\n",
        "    grs = grs.dropna(subset=[\"Longitude\", \"Latitude\", \"_orig_storm_number\", \"Time\"]).reset_index(drop=True)\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] rows after dropping NaNs in core fields: {len(grs)}\")\n",
        "\n",
        "\n",
        "\n",
        "    # FAST nearest-site via chunked vectorized haversine (unchanged math; now time-aware)\n",
        "    # Prepare site arrays from dict-of-dicts + active date ranges\n",
        "    site_names, site_lats, site_lons, site_ranges = _prepare_site_arrays(radar_info)\n",
        "\n",
        "    if not site_names:\n",
        "        raise ValueError(\"radar_info is empty or lacks valid positions.\")\n",
        "\n",
        "    # prepare site arrays (radians, float32 to reduce mem)\n",
        "    site_lats_rad = np.deg2rad(site_lats).astype(np.float32)\n",
        "    site_lons_rad = np.deg2rad(site_lons).astype(np.float32)\n",
        "\n",
        "    N = len(grs)\n",
        "    nearest_sites_idx = np.full(N, -1, dtype=np.int32)  # -1 = no active site\n",
        "    nearest_dists = np.full(N, np.nan, dtype=np.float32)\n",
        "\n",
        "    lon_arr = grs[\"Longitude\"].to_numpy(dtype=np.float32)\n",
        "    lat_arr = grs[\"Latitude\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "    # Time as int64 ns for range checks\n",
        "    t_arr = pd.to_datetime(grs[\"Time\"], utc=True, errors=\"coerce\")\n",
        "    t_ns_arr = t_arr.view('int64').to_numpy()\n",
        "\n",
        "    R_earth_km = 6371.0  # km\n",
        "\n",
        "    def _compute_chunk(i0: int, i1: int, active_mask: np.ndarray):\n",
        "        \"\"\"\n",
        "        Compute distances [L,S] for rows i0:i1, then mask out inactive sites,\n",
        "        pick argmin per row, and return (idx, mindist).\n",
        "        \"\"\"\n",
        "        la = lat_arr[i0:i1].astype(np.float32)\n",
        "        lo = lon_arr[i0:i1].astype(np.float32)\n",
        "        la_rad = np.deg2rad(la).reshape(-1, 1)\n",
        "        lo_rad = np.deg2rad(lo).reshape(-1, 1)\n",
        "        s_lat = site_lats_rad.reshape(1, -1)\n",
        "        s_lon = site_lons_rad.reshape(1, -1)\n",
        "\n",
        "        dlat = la_rad - s_lat\n",
        "        dlon = lo_rad - s_lon\n",
        "        sin_dlat2 = np.sin(dlat * 0.5) ** 2\n",
        "        sin_dlon2 = np.sin(dlon * 0.5) ** 2\n",
        "        a = sin_dlat2 + np.cos(la_rad) * np.cos(s_lat) * sin_dlon2\n",
        "        a = np.clip(a, 0.0, 1.0)\n",
        "        c = 2.0 * np.arcsin(np.sqrt(a))\n",
        "        d = (R_earth_km * c).astype(np.float32)   # [L,S]\n",
        "\n",
        "        # mask out sites not active at the row's time\n",
        "        # inactive -> set to +inf so they can't be selected\n",
        "        # active_mask is [L,S], True=active\n",
        "        d[~active_mask] = np.inf\n",
        "\n",
        "        # pick nearest among active\n",
        "        idx = np.argmin(d, axis=1)\n",
        "        mind = d[np.arange(d.shape[0]), idx]\n",
        "\n",
        "        # rows with no active sites -> mind = inf, set idx=-1, dist=NaN\n",
        "        no_active = ~np.isfinite(mind)\n",
        "        if no_active.any():\n",
        "            idx = idx.astype(np.int32, copy=True)\n",
        "            mind = mind.astype(np.float32, copy=True)\n",
        "            idx[no_active] = -1\n",
        "            mind[no_active] = np.nan\n",
        "\n",
        "        return idx.astype(np.int32), mind.astype(np.float32)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] computing nearest active site in chunks of {chunk_size} rows\")\n",
        "\n",
        "    i0 = 0\n",
        "    while i0 < N:\n",
        "        i1 = min(N, i0 + chunk_size)\n",
        "        # build the [L,S] active-site mask for this chunkâ€™s timestamps\n",
        "        active_mask = _active_mask_for_chunk(t_ns_arr[i0:i1], site_ranges)\n",
        "\n",
        "        idxs, dists = _compute_chunk(i0, i1, active_mask)\n",
        "        nearest_sites_idx[i0:i1] = idxs\n",
        "        nearest_dists[i0:i1] = dists\n",
        "        if debug:\n",
        "            na = int(np.sum(np.isnan(dists)))\n",
        "            print(f\"[load_grs_tracks] processed rows {i0}:{i1} (rows w/ no active site: {na})\")\n",
        "        i0 = i1\n",
        "\n",
        "    # map indices to names; -1 -> None\n",
        "    nearest_sites = [site_names[i] if i >= 0 else None for i in nearest_sites_idx]\n",
        "    grs[\"radar_site\"] = nearest_sites\n",
        "    grs[\"distance_to_site\"] = nearest_dists.astype(float)\n",
        "\n",
        "    # drop rows beyond max_distance_km\n",
        "    before = len(grs)\n",
        "    grs = grs[grs[\"distance_to_site\"] <= max_distance_km].reset_index(drop=True)\n",
        "    if debug:\n",
        "        print(f\"[load_grs_tracks] dropped {before - len(grs)} rows with distance > {max_distance_km} km; remaining {len(grs)}\")\n",
        "    if grs.empty:\n",
        "        if debug:\n",
        "            print(\"[load_grs_tracks] no rows left after distance filtering\")\n",
        "        return grs\n",
        "\n",
        "    # VERY FAST vectorized split for multi-site storms\n",
        "    grs['_orig_storm_number'] = grs['_orig_storm_number'].astype(int)\n",
        "    site_counts = grs.groupby('_orig_storm_number')['radar_site'].nunique()\n",
        "    multi_mask_series = grs['_orig_storm_number'].map(site_counts) > 1\n",
        "\n",
        "    # For rows that need splitting (multi-site storms), factorize (orig, site) pairs and assign new ids\n",
        "    mask_multi = multi_mask_series.fillna(False).values  # boolean numpy array\n",
        "    if mask_multi.any():\n",
        "        # factorize the pair tuples for only those rows\n",
        "        pairs = list(zip(grs.loc[mask_multi, '_orig_storm_number'].values,\n",
        "                         grs.loc[mask_multi, 'radar_site'].values))\n",
        "        codes, uniques = pd.factorize(pairs)\n",
        "        max_orig = int(grs['_orig_storm_number'].max())\n",
        "        next_id = max_orig + 1\n",
        "        # map codes to new storm ids (vectorized)\n",
        "        new_ids_for_multi = next_id + codes\n",
        "        # create an array for storm_id\n",
        "        storm_id_arr = grs['_orig_storm_number'].values.copy().astype(int)\n",
        "        storm_id_arr[mask_multi] = new_ids_for_multi\n",
        "        grs['storm_id'] = storm_id_arr.astype(int)\n",
        "    else:\n",
        "        # no splits needed, preserve original numbers\n",
        "        grs['storm_id'] = grs['_orig_storm_number'].values.astype(int)\n",
        "\n",
        "    # drop storm groups with fewer than min_rows rows\n",
        "    counts = grs.groupby('storm_id').size()\n",
        "    keep_ids = counts[counts >= min_rows].index.tolist()\n",
        "    before2 = len(grs)\n",
        "    grs = grs[grs['storm_id'].isin(keep_ids)].reset_index(drop=True)\n",
        "    if debug:\n",
        "        dropped = before2 - len(grs)\n",
        "        print(f\"[load_grs_tracks] dropped {dropped} rows belonging to storm groups with < {min_rows} rows\")\n",
        "\n",
        "    # remove original storm-like columns and helper column\n",
        "    storm_like = [c for c in grs.columns if re.match(r'^\\s*storm(?:\\s*number)?\\s*$', c, flags=re.I)]\n",
        "    if storm_like:\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] dropping original storm-like columns: {storm_like}\")\n",
        "        grs = grs.drop(columns=storm_like, errors='ignore')\n",
        "    if '_orig_storm_number' in grs.columns:\n",
        "        grs = grs.drop(columns=['_orig_storm_number'], errors='ignore')\n",
        "\n",
        "\n",
        "    # snake_case all column names\n",
        "    def _snake_case(name: str) -> str:\n",
        "        s = str(name).strip()\n",
        "        s = re.sub(r'[^0-9a-zA-Z]+', '_', s)\n",
        "        s = re.sub(r'__+', '_', s)\n",
        "        s = s.strip('_').lower()\n",
        "        return s or name\n",
        "    grs.columns = [_snake_case(c) for c in grs.columns]\n",
        "\n",
        "    # final ordering: keep time/latitude/longitude/storm_id/radar_site/distance_to_site up front when present\n",
        "    front = []\n",
        "    for k in (\"time\", \"latitude\", \"longitude\", \"storm_id\", \"radar_site\", \"distance_to_site\"):\n",
        "        if k in grs.columns:\n",
        "            front.append(k)\n",
        "    rest = [c for c in grs.columns if c not in front]\n",
        "    grs = grs[front + rest]\n",
        "\n",
        "\n",
        "    ######################################################### FILTER DATAFRAME FOR DISCONTINUITIES #########################################################\n",
        "\n",
        "\n",
        "    # Continuity & splitting:\n",
        "    # 1) If NO cluster in a storm covers â‰¥50% of that storm's rows â†’ drop the entire storm.\n",
        "    # 2) Otherwise (there IS a majority cluster), keep every cluster with size â‰¥ min_rows:\n",
        "    #       - majority cluster keeps original storm_id\n",
        "    #       - each other kept cluster becomes a new storm_id\n",
        "    #    Clusters with size < min_rows are dropped.\n",
        "    if (\"storm_id\" in grs.columns) and (\"time\" in grs.columns):\n",
        "        t = pd.to_datetime(grs[\"time\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "        # Sort by (storm_id, time) to compute per-storm diffs and cluster ids\n",
        "        order = np.lexsort((t.view(np.int64), grs[\"storm_id\"].to_numpy()))\n",
        "        grs_sorted = grs.iloc[order].copy()\n",
        "        t_sorted = t.iloc[order]\n",
        "\n",
        "        dts = t_sorted.groupby(grs_sorted[\"storm_id\"]).diff()\n",
        "        gap = pd.to_timedelta(max_gap_hours, unit=\"h\")\n",
        "        new_cluster = dts.isna() | (dts > gap)\n",
        "        cluster_id = new_cluster.groupby(grs_sorted[\"storm_id\"]).cumsum().astype(np.int32)\n",
        "\n",
        "        # Cluster sizes and group sizes  (version-agnostic column naming)\n",
        "        cluster_sizes = (\n",
        "            grs_sorted\n",
        "            .groupby([\"storm_id\", cluster_id], sort=False)\n",
        "            .size()\n",
        "            .reset_index(name=\"cluster_size\")\n",
        "        )\n",
        "        # The second column is the one created from 'cluster_id' Series; rename by position.\n",
        "        # (Works regardless of whether pandas called it 'level_1', 0, or anything else.)\n",
        "        second_col = cluster_sizes.columns[1]\n",
        "        if second_col != \"cluster_id\":\n",
        "            cluster_sizes = cluster_sizes.rename(columns={second_col: \"cluster_id\"})\n",
        "\n",
        "        group_sizes = (\n",
        "            grs_sorted.groupby(\"storm_id\").size().rename(\"group_size\").reset_index()\n",
        "        )\n",
        "\n",
        "        # Largest cluster per storm (majority check) using normalized name\n",
        "        largest = (\n",
        "            cluster_sizes.sort_values([\"storm_id\", \"cluster_size\"], ascending=[True, False])\n",
        "            .groupby(\"storm_id\", sort=False)\n",
        "            .head(1)[[\"storm_id\", \"cluster_id\", \"cluster_size\"]]\n",
        "            .rename(columns={\"cluster_id\": \"best_cluster_id\", \"cluster_size\": \"best_size\"})\n",
        "        )\n",
        "\n",
        "        meta = (\n",
        "            cluster_sizes\n",
        "            .merge(group_sizes, on=\"storm_id\", how=\"left\")\n",
        "            .merge(largest, on=\"storm_id\", how=\"left\")\n",
        "        )\n",
        "        meta[\"best_ratio\"] = meta[\"best_size\"] / meta[\"group_size\"]\n",
        "\n",
        "        # Decide which storms survive (must have a majority cluster â‰¥ 50%)\n",
        "        storms_have_majority = (\n",
        "            meta[[\"storm_id\", \"best_ratio\"]].drop_duplicates().set_index(\"storm_id\")[\"best_ratio\"] >= 0.5\n",
        "        )\n",
        "\n",
        "        # Keep clusters only for storms that have a majority; among those, keep clusters with size â‰¥ min_rows\n",
        "        meta[\"keep_cluster\"] = storms_have_majority.reindex(meta[\"storm_id\"]).to_numpy() & (meta[\"cluster_size\"] >= min_rows)\n",
        "\n",
        "        # Build mapping (storm_id, cluster_id) -> new_storm_id for kept clusters\n",
        "        next_id = int(grs[\"storm_id\"].max()) + 1\n",
        "        mapping_rows = []\n",
        "        for sid, sub in meta.groupby(\"storm_id\", sort=False):\n",
        "            has_majority = bool(storms_have_majority.get(sid, False))\n",
        "            if not has_majority:\n",
        "                # drop the entire storm (no mapping rows created)\n",
        "                continue\n",
        "\n",
        "            # clusters that pass size threshold\n",
        "            kept = sub.loc[sub[\"keep_cluster\"]]\n",
        "            if kept.empty:\n",
        "                # even majority cluster was < min_rows â†’ drop this storm entirely\n",
        "                continue\n",
        "\n",
        "            # Identify majority cluster for this storm (one row in 'largest')\n",
        "            best_cid = int(sub[\"best_cluster_id\"].iloc[0])\n",
        "\n",
        "            # Majority cluster keeps original storm_id if it's kept; otherwise the storm is dropped\n",
        "            if (kept[\"cluster_id\"] == best_cid).any():\n",
        "                mapping_rows.append({\"storm_id\": sid, \"cluster_id\": best_cid, \"new_storm_id\": sid})\n",
        "            else:\n",
        "                # No kept majority cluster â†’ drop storm entirely\n",
        "                continue\n",
        "\n",
        "            # All other kept clusters â†’ assign fresh ids\n",
        "            others = kept[kept[\"cluster_id\"] != best_cid]\n",
        "            for _, r in others.iterrows():\n",
        "                mapping_rows.append({\"storm_id\": sid, \"cluster_id\": int(r[\"cluster_id\"]), \"new_storm_id\": next_id})\n",
        "                next_id += 1\n",
        "\n",
        "        if mapping_rows:\n",
        "            map_df = pd.DataFrame(mapping_rows)\n",
        "\n",
        "            # Attach (storm_id, cluster_id) for each row, then merge to new_storm_id\n",
        "            key = pd.DataFrame({\n",
        "                \"storm_id\": grs_sorted[\"storm_id\"].to_numpy(),\n",
        "                \"cluster_id\": cluster_id.to_numpy()\n",
        "            })\n",
        "            key = key.merge(map_df, on=[\"storm_id\", \"cluster_id\"], how=\"left\")\n",
        "\n",
        "            # Rows not mapped are dropped\n",
        "            keep_mask_sorted = key[\"new_storm_id\"].notna().to_numpy()\n",
        "            new_ids_sorted = key[\"new_storm_id\"].fillna(-1).astype(np.int64).to_numpy()\n",
        "\n",
        "            # Map to original order\n",
        "            keep_mask = np.empty_like(keep_mask_sorted, dtype=bool)\n",
        "            keep_mask[order] = keep_mask_sorted\n",
        "            new_ids = np.empty_like(new_ids_sorted)\n",
        "            new_ids[order] = new_ids_sorted\n",
        "\n",
        "            before_cont = len(grs)\n",
        "            grs = grs.loc[keep_mask].reset_index(drop=True)\n",
        "            grs[\"storm_id\"] = new_ids[keep_mask]\n",
        "\n",
        "            if debug:\n",
        "                dropped = before_cont - len(grs)\n",
        "                print(f\"[load_grs_tracks] continuity filter (max_gap_hours={max_gap_hours}): \"\n",
        "                      f\"dropped {dropped} rows; kept clusters with size â‰¥ {min_rows} \"\n",
        "                      f\"and split non-major clusters into new storm_ids; \"\n",
        "                      f\"dropped storms with no â‰¥50% majority cluster\")\n",
        "        else:\n",
        "            # No storms passed the rules â†’ empty result\n",
        "            if debug:\n",
        "                print(f\"[load_grs_tracks] continuity filter: no storms had a â‰¥50% majority cluster; dropping all\")\n",
        "            grs = grs.iloc[0:0].copy().reset_index(drop=True)\n",
        "    else:\n",
        "        if debug:\n",
        "            print(\"[load_grs_tracks] continuity filter skipped (missing 'storm_id' or 'time')\")\n",
        "\n",
        "\n",
        "    ######################################################################################################################################################\n",
        "\n",
        "\n",
        "    # save processed cache\n",
        "    try:\n",
        "        grs.to_parquet(processed_parquet, index=False)\n",
        "        if debug:\n",
        "            print(f\"[load_grs_tracks] wrote processed cache {processed_parquet}\")\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            grs.to_csv(processed_csv, index=False)\n",
        "            if debug:\n",
        "                print(f\"[load_grs_tracks] parquet save failed; saved CSV {processed_csv}: {e}\")\n",
        "        except Exception as ee:\n",
        "            if debug:\n",
        "                print(f\"[load_grs_tracks] failed to save processed cache: {ee}\")\n",
        "\n",
        "    return grs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EogqsHM1mZwd"
      },
      "source": [
        "#### **Filter Radar Scans**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwbuTG6of-ba"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pygrib\n",
        "import re\n",
        "import s3fs\n",
        "import gcsfs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pyart\n",
        "import tarfile\n",
        "import json\n",
        "import concurrent.futures, threading\n",
        "import gzip, numpy.ma as _ma\n",
        "\n",
        "\n",
        "from scipy.ndimage import label\n",
        "from io import BytesIO\n",
        "from urllib.request import urlretrieve\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# S3 access (anon)\n",
        "_s3fs = None\n",
        "_s3client = None\n",
        "\n",
        "\n",
        "\n",
        "############################################################################ HELPERS ############################################################################\n",
        "\n",
        "\n",
        "def _product_key(p: str) -> str:\n",
        "    # keep your sanitizer semantics\n",
        "    return re.sub(r'[^0-9A-Za-z]+', '_', p).strip('_').lower() or 'prod'\n",
        "\n",
        "def _product_name_map(allowed_fields):\n",
        "    # mapping from \"file-safe key\" -> \"pyart field name\"\n",
        "    return { _product_key(f): f for f in allowed_fields }\n",
        "\n",
        "def _save_gz_pickle(obj, path, debug=False):\n",
        "    try:\n",
        "        _ensure_dir(os.path.dirname(path))\n",
        "        with gzip.open(path, \"wb\") as f:\n",
        "            pickle.dump(obj, f, protocol=4)\n",
        "    except Exception as e:\n",
        "        if debug: print(f\"[find_radar_scans] skeleton write failed {path}: {e}\")\n",
        "\n",
        "def _load_gz_pickle(path, debug=False):\n",
        "    try:\n",
        "        with gzip.open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception as e:\n",
        "        if debug: print(f\"[find_radar_scans] skeleton read failed {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def _save_field_pack(base_path, field_dict, downcast=True, debug=False):\n",
        "    \"\"\"\n",
        "    Save a single field into two small files:\n",
        "      - {base_path}.npz  : compressed data + mask\n",
        "      - {base_path}.json : tiny metadata (everything except 'data')\n",
        "    \"\"\"\n",
        "    try:\n",
        "        _ensure_dir(os.path.dirname(base_path))\n",
        "        arr = field_dict[\"data\"]\n",
        "        data = np.asarray(arr, dtype=np.float32 if downcast else arr.dtype)\n",
        "        mask = np.asarray(_ma.getmaskarray(arr), dtype=np.uint8)\n",
        "        np.savez_compressed(base_path + \".npz\", data=data, mask=mask)\n",
        "        meta = {k: v for k, v in field_dict.items() if k != \"data\"}\n",
        "        with open(base_path + \".json\", \"w\") as f:\n",
        "            json.dump(meta, f)\n",
        "    except Exception as e:\n",
        "        if debug: print(f\"[find_radar_scans] field save failed {base_path}: {e}\")\n",
        "\n",
        "def _load_field_pack(base_path, debug=False):\n",
        "    \"\"\"\n",
        "    Load a single field pack and return a pyart-compatible field dict.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        z = np.load(base_path + \".npz\", allow_pickle=False)\n",
        "        with open(base_path + \".json\", \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "        data = _ma.MaskedArray(z[\"data\"], mask=z[\"mask\"].astype(bool))\n",
        "        meta[\"data\"] = data\n",
        "        return meta\n",
        "    except Exception as e:\n",
        "        if debug: print(f\"[find_radar_scans] field load failed {base_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def _make_radar_skeleton(r):\n",
        "    import pyart\n",
        "    import numpy as np\n",
        "\n",
        "    # minimal copies; no fields\n",
        "    time     = {'data': r.time['data'].copy()}\n",
        "    rng_dict = {'data': r.range['data'].copy()}   # source attr is 'range'\n",
        "    fields   = {}\n",
        "    metadata = dict(getattr(r, \"metadata\", {}) or {})\n",
        "\n",
        "    latitude   = dict(r.latitude)\n",
        "    longitude  = dict(r.longitude)\n",
        "    altitude   = dict(r.altitude)\n",
        "\n",
        "    sweep_number          = r.sweep_number.copy()\n",
        "    sweep_start_ray_index = r.sweep_start_ray_index.copy()\n",
        "    sweep_end_ray_index   = r.sweep_end_ray_index.copy()\n",
        "    azimuth   = {'data': r.azimuth['data'].copy()}\n",
        "    elevation = {'data': r.elevation['data'].copy()}\n",
        "\n",
        "    scan_type = getattr(r, \"scan_type\", \"ppi\")\n",
        "\n",
        "    # optional dict-like attrs (only include if present)\n",
        "    opt = {}\n",
        "    for name in (\"fixed_angle\", \"target_scan_rate\", \"rays_are_indexed\",\n",
        "                 \"ray_angle_res\", \"scan_rate\", \"antenna_transition\",\n",
        "                 \"altitude_agl\"):\n",
        "        val = getattr(r, name, None)\n",
        "        if val is not None:\n",
        "            opt[name] = dict(val)\n",
        "\n",
        "    # sweep_mode is required by many builds; ensure it exists\n",
        "    sm = getattr(r, \"sweep_mode\", None)\n",
        "    if sm is not None:\n",
        "        sweep_mode = dict(sm)\n",
        "    else:\n",
        "        # create a reasonable default\n",
        "        nsweeps = int(sweep_number[\"data\"].size)\n",
        "        default_mode = \"azimuth_surveillance\" if scan_type == \"ppi\" else \"rhi\"\n",
        "        sweep_mode = {\"data\": np.array([default_mode] * nsweeps)}\n",
        "\n",
        "    # keep bulky blocks out\n",
        "    instrument_parameters = None\n",
        "    radar_calibration = None\n",
        "\n",
        "    # Build via keywords; be compatible with both `_range` and `range`\n",
        "    try:\n",
        "        return pyart.core.Radar(\n",
        "            time=time, _range=rng_dict, fields=fields, metadata=metadata,\n",
        "            scan_type=scan_type,\n",
        "            latitude=latitude, longitude=longitude, altitude=altitude,\n",
        "            sweep_number=sweep_number, sweep_mode=sweep_mode,\n",
        "            fixed_angle=opt.get(\"fixed_angle\"),\n",
        "            sweep_start_ray_index=sweep_start_ray_index,\n",
        "            sweep_end_ray_index=sweep_end_ray_index,\n",
        "            azimuth=azimuth, elevation=elevation,\n",
        "            altitude_agl=opt.get(\"altitude_agl\"),\n",
        "            target_scan_rate=opt.get(\"target_scan_rate\"),\n",
        "            rays_are_indexed=opt.get(\"rays_are_indexed\"),\n",
        "            ray_angle_res=opt.get(\"ray_angle_res\"),\n",
        "            scan_rate=opt.get(\"scan_rate\"),\n",
        "            antenna_transition=opt.get(\"antenna_transition\"),\n",
        "            instrument_parameters=instrument_parameters,\n",
        "            radar_calibration=radar_calibration\n",
        "        )\n",
        "    except TypeError as e:\n",
        "        # Some older/newer Py-ART builds use 'range' instead of '_range'\n",
        "        if \"unexpected keyword argument '_range'\" in str(e):\n",
        "            return pyart.core.Radar(\n",
        "                time=time, range=rng_dict, fields=fields, metadata=metadata,\n",
        "                scan_type=scan_type,\n",
        "                latitude=latitude, longitude=longitude, altitude=altitude,\n",
        "                sweep_number=sweep_number, sweep_mode=sweep_mode,\n",
        "                fixed_angle=opt.get(\"fixed_angle\"),\n",
        "                sweep_start_ray_index=sweep_start_ray_index,\n",
        "                sweep_end_ray_index=sweep_end_ray_index,\n",
        "                azimuth=azimuth, elevation=elevation,\n",
        "                altitude_agl=opt.get(\"altitude_agl\"),\n",
        "                target_scan_rate=opt.get(\"target_scan_rate\"),\n",
        "                rays_are_indexed=opt.get(\"rays_are_indexed\"),\n",
        "                ray_angle_res=opt.get(\"ray_angle_res\"),\n",
        "                scan_rate=opt.get(\"scan_rate\"),\n",
        "                antenna_transition=opt.get(\"antenna_transition\"),\n",
        "                instrument_parameters=instrument_parameters,\n",
        "                radar_calibration=radar_calibration\n",
        "            )\n",
        "        raise\n",
        "\n",
        "\n",
        "def _slim_to_fields(r, allowed_fields):\n",
        "    \"\"\"\n",
        "    In-place: drop all fields not in allowed_fields. Downcast to float32 to shrink memory.\n",
        "    \"\"\"\n",
        "    keep = set(allowed_fields)\n",
        "    for k in list(r.fields.keys()):\n",
        "        if k not in keep:\n",
        "            del r.fields[k]\n",
        "        else:\n",
        "            arr = r.fields[k][\"data\"]\n",
        "            if arr.dtype != np.float32:\n",
        "                r.fields[k][\"data\"] = np.asarray(arr, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "############################################################ FIND RADAR SCANS FOR STORM_ID POSITIONS ###############################################################################\n",
        "\n",
        "\n",
        "\n",
        "def find_radar_scans(\n",
        "    storm_df: pd.DataFrame,\n",
        "    site_column: str = \"radar_site\",\n",
        "    time_column: str = \"time\",\n",
        "    level2_base: str = \"unidata-nexrad-level2\",   # NEW: AWS Level II bucket (name or s3://bucket)\n",
        "    cache_dir: str = \"Datasets/nexrad_datasets/level_two_raw\",  # keep local Drive/FS cache\n",
        "    product_filter: List = None,                  # e.g. [\"reflectivity\",\"velocity\",\"zdr\",\"rhohv\"]\n",
        "    time_tolerance_seconds: int = 29,             # +/- tolerance for matching volume start times\n",
        "    keep_in_memory: bool = True,                  # keep Py-ART Radar object(s) in the returned DF\n",
        "    debug: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Link GR-S storm rows to NEXRAD Level II volume files on AWS S3 (Unidata bucket).\n",
        "    For each storm timestamp (UTC), find the nearest Level II volume for the storm's radar\n",
        "    within +/- `time_tolerance_seconds`. Optionally restrict to specific Level II fields\n",
        "    using `product_filter` (e.g., [\"reflectivity\",\"velocity\",\"zdr\"]).\n",
        "\n",
        "    Returns a subset of `storm_df` with extra columns:\n",
        "      - If product_filter is provided: for each normalized field key `k`:\n",
        "          * `{k}_scan`                : Py-ART Radar object (or None if keep_in_memory=False)\n",
        "          * `{k}_cache_volume_path`   : local pickle path where the Radar object is cached\n",
        "          * `{k}_matched_volume_s3_key` : matched S3 object key (filename)\n",
        "      - Else (no product_filter): legacy single triple:\n",
        "          * 'radar_scan', 'cache_member_name', 'matched_member_name'\n",
        "\n",
        "    Notes:\n",
        "      - Uses AWS S3 bucket `unidata-nexrad-level2` by default; accepts either bare bucket\n",
        "        name or full \"s3://bucket\" in `level2_base`.\n",
        "      - No tar handling (complete migration to Level II single-file volumes).\n",
        "      - Preserves overall structure, caching, and debug verbosity from Level III version.\n",
        "    \"\"\"\n",
        "    # Utilities / Normalizers\n",
        "    def _strip_s3_prefix(b):\n",
        "        return b[5:] if isinstance(b, str) and b.startswith(\"s3://\") else b\n",
        "\n",
        "    def _s3_url(bucket: str, key: str) -> str:\n",
        "        return f\"s3://{_strip_s3_prefix(bucket).rstrip('/')}/{key.lstrip('/')}\"\n",
        "\n",
        "    # Canonicalize product names to Py-ART Level II field keys\n",
        "    # (Accept common aliases; fall back to lowercased alnum)\n",
        "    _LV2_ALIASES = {\n",
        "        \"reflectivity\": {\"reflectivity\", \"refl\", \"ref\", \"dz\", \"z\"},\n",
        "        \"velocity\": {\"velocity\", \"vel\", \"vr\"},\n",
        "        \"spectrum_width\": {\"spectrum_width\", \"sw\", \"width\"},\n",
        "        \"differential_reflectivity\": {\"differential_reflectivity\", \"zdr\"},\n",
        "        \"differential_phase\": {\"differential_phase\", \"phidp\", \"phi\", \"kdp?\"},\n",
        "        \"cross_correlation_ratio\": {\"cross_correlation_ratio\", \"rho\", \"rhohv\"},\n",
        "        \"normalized_coherent_power\": {\"normalized_coherent_power\", \"ncp\"},\n",
        "        \"clutter_filter_power_removed\": {\"clutter_filter_power_removed\", \"cfpr\", \"cpwr\"},\n",
        "    }\n",
        "\n",
        "    def _normalize_lv2_product_filter(pf: Optional[List[str]]) -> List[str]:\n",
        "        if not pf:\n",
        "            return []\n",
        "        norm = []\n",
        "        for raw in pf:\n",
        "            if raw is None:\n",
        "                continue\n",
        "            s = str(raw).strip().lower()\n",
        "            hit = None\n",
        "            for key, aliases in _LV2_ALIASES.items():\n",
        "                if s == key or s in aliases:\n",
        "                    hit = key\n",
        "                    break\n",
        "            if not hit:\n",
        "                # fallback: clean up to a \"likely\" field name; Py-ART may not have it but keep column schema\n",
        "                hit = re.sub(r\"[^0-9a-z]+\", \"_\", s).strip(\"_\")\n",
        "            if hit not in norm:\n",
        "                norm.append(hit)\n",
        "        return norm\n",
        "\n",
        "    try:\n",
        "        import s3fs  # best path: lets Py-ART read s3:// URIs directly\n",
        "        _s3fs = s3fs.S3FileSystem(anon=True)\n",
        "        if debug: print(\"[find_radar_scans] using s3fs (anonymous)\")\n",
        "    except Exception as e:\n",
        "        if debug: print(f\"[find_radar_scans] s3fs not available: {e} - falling back to boto3\")\n",
        "        try:\n",
        "            import boto3\n",
        "            from botocore import UNSIGNED\n",
        "            from botocore.client import Config\n",
        "            _s3client = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED), region_name=\"us-east-1\")\n",
        "            if debug: print(\"[find_radar_scans] using boto3 (unsigned) for S3 access\")\n",
        "        except Exception as e2:\n",
        "            if debug: print(f\"[find_radar_scans] ERROR: no S3 access libraries available: {e2}\")\n",
        "            # We'll continue; listing will fail and return empty.\n",
        "\n",
        "    def _s3_ls(bucket: str, prefix: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Return a list of 's3://bucket/key' paths under the prefix (no directories).\n",
        "        \"\"\"\n",
        "        full = _s3_url(bucket, prefix)\n",
        "        out = []\n",
        "        if _s3fs is not None:\n",
        "            try:\n",
        "                for p in _s3fs.ls(full, detail=False):\n",
        "                    # s3fs returns 's3://bucket/key' for objects\n",
        "                    # Filter out \"directories\" if any\n",
        "                    if p.endswith(\"/\"):\n",
        "                        continue\n",
        "                    out.append(p)\n",
        "            except Exception as e:\n",
        "                if debug: print(f\"[find_radar_scans] s3fs.ls({full}) failed: {e}\")\n",
        "        elif _s3client is not None:\n",
        "            try:\n",
        "                Bucket = _strip_s3_prefix(bucket)\n",
        "                Prefix = prefix.lstrip(\"/\")\n",
        "                paginator = _s3client.get_paginator(\"list_objects_v2\")\n",
        "                for page in paginator.paginate(Bucket=Bucket, Prefix=Prefix):\n",
        "                    for it in page.get(\"Contents\", []):\n",
        "                        out.append(_s3_url(Bucket, it[\"Key\"]))\n",
        "            except Exception as e:\n",
        "                if debug: print(f\"[find_radar_scans] boto3 list_objects_v2({bucket},{prefix}) failed: {e}\")\n",
        "        return out\n",
        "\n",
        "    def _s3_read_pyart(bucket: str, key: str):\n",
        "        \"\"\"\n",
        "        Read a Level II volume into a Py-ART Radar object.\n",
        "        Prefer s3fs path; fallback to boto3 BytesIO.\n",
        "        \"\"\"\n",
        "        import pyart\n",
        "        s3path = _s3_url(bucket, key)\n",
        "        try:\n",
        "            if _s3fs is not None:\n",
        "                # Py-ART 2.0+ reads s3:// URIs with s3fs installed\n",
        "                return pyart.io.read_nexrad_archive(s3path)\n",
        "            elif _s3client is not None:\n",
        "                resp = _s3client.get_object(Bucket=_strip_s3_prefix(bucket), Key=key.lstrip(\"/\"))\n",
        "                bio = BytesIO(resp[\"Body\"].read())\n",
        "                return pyart.io.read_nexrad_archive(bio)\n",
        "        except Exception as e:\n",
        "            if debug: print(f\"[find_radar_scans] pyart read failed for {s3path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Quick-empty checks\n",
        "    if storm_df is None or len(storm_df) == 0:\n",
        "        if debug: print(\"[find_radar_scans] empty storm_df, returning empty DataFrame\")\n",
        "        return pd.DataFrame(columns=list(storm_df.columns) + [\"radar_scan\", \"cache_member_name\"])\n",
        "\n",
        "    # Normalize time column\n",
        "    storm_df = storm_df.copy()\n",
        "    storm_df[time_column] = pd.to_datetime(storm_df[time_column], utc=True, errors=\"coerce\")\n",
        "    storm_df = storm_df.dropna(subset=[time_column])\n",
        "    if len(storm_df) == 0:\n",
        "        if debug: print(\"[find_radar_scans] all times coerce->NaT, returning empty\")\n",
        "        return pd.DataFrame(columns=list(storm_df.columns) + [\"radar_scan\", \"cache_member_name\"])\n",
        "\n",
        "    # Normalize requested Level II fields\n",
        "    allowed_fields = _normalize_lv2_product_filter(product_filter)  # [] = accept-all (legacy single column)\n",
        "    allowed_keys = [_product_key(p) for p in allowed_fields] if allowed_fields else []\n",
        "\n",
        "    # Expect single radar site per group\n",
        "    sites = storm_df[site_column].dropna().unique().tolist()\n",
        "    if len(sites) == 0:\n",
        "        raise ValueError(\"No radar site present in storm_df\")\n",
        "    if len(sites) > 1 and debug:\n",
        "        print(f\"[find_radar_scans] Warning: multiple radar sites found for group: {sites}. Using first: {sites[0]}\")\n",
        "    site = sites[0].upper().strip()\n",
        "\n",
        "    # Compute date range for S3 prefix listing\n",
        "    times = storm_df[time_column].sort_values()\n",
        "    min_dt = times.min()\n",
        "    max_dt = times.max()\n",
        "    start_date = min_dt.date()\n",
        "    end_date = max_dt.date()\n",
        "\n",
        "    # create caches\n",
        "    bucket = level2_base or \"unidata-nexrad-level2\"\n",
        "    bucket = _strip_s3_prefix(bucket)\n",
        "    pkl_cache_dir = os.path.join(cache_dir, site)\n",
        "    _ensure_dir(pkl_cache_dir)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[find_radar_scans] site={site}, date range {start_date} -> {end_date}\")\n",
        "        print(f\"[find_radar_scans] S3 bucket={bucket} (AWS Level II)\")\n",
        "        print(f\"[find_radar_scans] pkl_cache_dir={pkl_cache_dir}\")\n",
        "\n",
        "    # Build set of target timestamps (rounded to seconds)\n",
        "    target_times = list(times.dt.round(\"s\"))\n",
        "    date_to_times: Dict[date, List[pd.Timestamp]] = {}\n",
        "    for ts in target_times:\n",
        "        date_to_times.setdefault(ts.date(), []).append(ts)\n",
        "\n",
        "    # Results\n",
        "    linked_rows: List[dict] = []\n",
        "    tol = pd.Timedelta(seconds=time_tolerance_seconds)\n",
        "\n",
        "    # Iterate by date; list S3 once per day\n",
        "    for day, times_for_day in date_to_times.items():\n",
        "        y = f\"{day:%Y}\"; m = f\"{day:%m}\"; d = f\"{day:%d}\"\n",
        "        prefix = f\"{y}/{m}/{d}/{site}/\"\n",
        "\n",
        "        # List all Level II objects for this site/day\n",
        "        objects = _s3_ls(bucket, prefix)\n",
        "        if not objects:\n",
        "            if debug: print(f\"[find_radar_scans] no Level II objects for {site} on {day} under s3://{bucket}/{prefix}\")\n",
        "            continue\n",
        "        if debug:\n",
        "            print(f\"[find_radar_scans] {len(objects)} Level II object(s) for {site} on {day}\")\n",
        "\n",
        "        # Map: file_time -> object key (most days have multiple)\n",
        "        file_time_to_key: Dict[pd.Timestamp, str] = {}\n",
        "\n",
        "        # Example name: KHGX20220322_120125_V06  (no extension)\n",
        "        # We allow optional extension just in case.\n",
        "        name_re = re.compile(rf\"{re.escape(site)}(\\d{{8}})_(\\d{{6}})(?:_[A-Za-z0-9]+)?(?:\\.[A-Za-z0-9]+)?$\")\n",
        "\n",
        "        for s3path in objects:\n",
        "            fname = os.path.basename(s3path)\n",
        "            mo = name_re.search(fname)\n",
        "            if not mo:\n",
        "                continue\n",
        "            ymd = mo.group(1)\n",
        "            hms = mo.group(2)\n",
        "            timestr = f\"{ymd}{hms}\"\n",
        "            ftime = pd.to_datetime(timestr, format=\"%Y%m%d%H%M%S\", utc=True, errors=\"coerce\")\n",
        "            if pd.isna(ftime):\n",
        "                continue\n",
        "            # Store key (bucket-relative)\n",
        "            key = f\"{y}/{m}/{d}/{site}/{fname}\"\n",
        "            file_time_to_key[ftime] = key\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[find_radar_scans] parsed {len(file_time_to_key)} volume times for {site} on {day}\")\n",
        "\n",
        "        if not file_time_to_key:\n",
        "            continue\n",
        "\n",
        "        # For each storm time, find nearest volume within tolerance\n",
        "        matched_count_for_day = 0\n",
        "        volume_times_sorted = sorted(file_time_to_key.keys())\n",
        "\n",
        "        for storm_ts in times_for_day:\n",
        "            # Binary search for closest time\n",
        "            # (small N per day; linear scan also fine; keep readable)\n",
        "            best_time = None\n",
        "            best_dt_delta = None\n",
        "            for vts in volume_times_sorted:\n",
        "                delta = abs(storm_ts - vts)\n",
        "                if delta <= tol:\n",
        "                    if best_time is None or delta < best_dt_delta:\n",
        "                        best_time = vts\n",
        "                        best_dt_delta = delta\n",
        "                elif vts > storm_ts and (best_time is not None) and (vts - storm_ts) > best_dt_delta + tol:\n",
        "                    # simple early break once we're beyond a plausible better match\n",
        "                    break\n",
        "\n",
        "            if best_time is None:\n",
        "                if debug:\n",
        "                    print(f\"[find_radar_scans] no L2 match within +/-{time_tolerance_seconds}s for {storm_ts}\")\n",
        "                continue\n",
        "\n",
        "            key = file_time_to_key[best_time]\n",
        "            fname = os.path.basename(key)\n",
        "\n",
        "\n",
        "            # ---- Field-level caching paths (no duplicate {site}/{site} in path) ----\n",
        "            base_rel = os.path.join(day.strftime(\"%Y%m%d\"), fname)                  # e.g., 20170717/KMTX20170717_230028_V06\n",
        "            base_dir = os.path.join(pkl_cache_dir, base_rel)                        # Datasets/.../KMTX/20170717/...\n",
        "            skeleton_path = base_dir + \".skeleton.pkl.gz\"\n",
        "\n",
        "            # Unified path that we expose in the DataFrame:\n",
        "            df_cache_path = skeleton_path if allowed_fields else (base_dir + \".radar.pkl.gz\")\n",
        "\n",
        "            radar_obj = None\n",
        "\n",
        "            if allowed_fields:\n",
        "                keymap = _product_name_map(allowed_fields)                           # {'reflectivity': 'reflectivity', ...} via sanitized key mapping if needed\n",
        "                field_paths = { k: base_dir + f\".{k}\" for k in keymap.keys() }       # per-field base (we add .npz/.json)\n",
        "\n",
        "                # Check if we already have a full set in cache\n",
        "                have_skeleton = os.path.exists(skeleton_path)\n",
        "                have_all_fields = all(os.path.exists(p + \".npz\") and os.path.exists(p + \".json\") for p in field_paths.values())\n",
        "\n",
        "                if have_skeleton and have_all_fields:\n",
        "                    # Rebuild slim Radar from cache\n",
        "                    sk = _load_gz_pickle(skeleton_path, debug=debug)\n",
        "                    if sk is not None:\n",
        "                        radar_obj = sk\n",
        "                        for k, fldname in keymap.items():\n",
        "                            pack = _load_field_pack(field_paths[k], debug=debug)\n",
        "                            if pack is not None:\n",
        "                                radar_obj.fields[fldname] = pack\n",
        "                        if debug:\n",
        "                            print(f\"[find_radar_scans] cache HIT (skeleton + {list(keymap.keys())}) at {base_dir}\")\n",
        "\n",
        "            # If not fully cached, fetch from S3, slim, and write field-level caches\n",
        "            if radar_obj is None:\n",
        "                radar_obj = _s3_read_pyart(bucket, key)\n",
        "                if radar_obj is None:\n",
        "                    if debug: print(f\"[find_radar_scans] FAILED to read {key} from s3://{bucket}\")\n",
        "                    continue\n",
        "\n",
        "                if allowed_fields:\n",
        "                    # Keep only requested fields & convert to float32\n",
        "                    _slim_to_fields(radar_obj, allowed_fields)\n",
        "\n",
        "                    # Write skeleton + per-field packs\n",
        "                    _save_gz_pickle(_make_radar_skeleton(radar_obj), skeleton_path, debug=debug)\n",
        "                    if debug: print(f\"[find_radar_scans] skeleton WRITE: {skeleton_path}\")\n",
        "\n",
        "                    for fldname in allowed_fields:\n",
        "                        if fldname in radar_obj.fields:\n",
        "                            k = _product_key(fldname)             # file-safe key\n",
        "                            _save_field_pack(base_dir + f\".{k}\", radar_obj.fields[fldname], downcast=True, debug=debug)\n",
        "                    if debug: print(f\"[find_radar_scans] fields WRITE at base: {base_dir}\")\n",
        "                else:\n",
        "                    # Legacy (no product_filter): still shrink + compress a single file to avoid 400MB pickles\n",
        "                    # PATH: single gz-pickle instead of raw pickle\n",
        "                    single_path = df_cache_path\n",
        "                    _save_gz_pickle(radar_obj, single_path, debug=debug)\n",
        "                    if debug: print(f\"[find_radar_scans] cache WRITE (single gz): {single_path}\")\n",
        "\n",
        "\n",
        "            # Find matching storm_df rows with that exact normalized timestamp\n",
        "            matching_row_mask = (storm_df[time_column] == storm_ts)\n",
        "            matched_rows = storm_df[matching_row_mask]\n",
        "            if matched_rows.empty:\n",
        "                if debug:\n",
        "                    print(f\"[find_radar_scans] no exact storm_df rows matched post-normalization for time {storm_ts}\")\n",
        "                continue\n",
        "\n",
        "            # Attach columns (multi-field or legacy single)\n",
        "            if allowed_fields:\n",
        "                # build product-specific triples; reuse the *same* Radar object per field\n",
        "                prod_infos = {}\n",
        "                for fld in allowed_fields:\n",
        "                    k = _product_key(fld)\n",
        "                    prod_infos[k] = {\n",
        "                        \"scan\": radar_obj if keep_in_memory else None,\n",
        "                        \"pkl\": df_cache_path,\n",
        "                        \"name\": key,  # S3 key used as \"matched member name\"\n",
        "                    }\n",
        "\n",
        "                for _, row in matched_rows.iterrows():\n",
        "                    out = row.to_dict()\n",
        "                    for k, info in prod_infos.items():\n",
        "                        out[f\"{k}_scan\"] = info[\"scan\"]\n",
        "                        out[f\"{k}_cache_volume_path\"] = info[\"pkl\"]\n",
        "                        out[f\"{k}_matched_volume_s3_key\"] = info[\"name\"]\n",
        "                    linked_rows.append(out)\n",
        "                    matched_count_for_day += 1\n",
        "                    if debug:\n",
        "                        print(f\"[find_radar_scans] linked {storm_ts} -> {fname} fields={allowed_fields}\")\n",
        "            else:\n",
        "                # legacy single triple\n",
        "                for _, row in matched_rows.iterrows():\n",
        "                    out = row.to_dict()\n",
        "                    out[\"radar_scan\"] = radar_obj if keep_in_memory else None\n",
        "                    out[\"cache_member_name\"] = df_cache_path\n",
        "                    out[\"matched_member_name\"] = key\n",
        "                    linked_rows.append(out)\n",
        "                    matched_count_for_day += 1\n",
        "                    if debug:\n",
        "                        print(f\"[find_radar_scans] linked {storm_ts} -> {fname}\")\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[find_radar_scans] finished {site} {day} - matched {matched_count_for_day} row(s)\")\n",
        "\n",
        "    # Build final linked DataFrame\n",
        "    if len(linked_rows) == 0:\n",
        "        if debug:\n",
        "            print(\"[find_radar_scans] found no matches; returning empty DataFrame\")\n",
        "        # Keep consistent schema\n",
        "        base_cols = list(storm_df.columns)\n",
        "        if allowed_fields:\n",
        "            extra = []\n",
        "            for fld in allowed_fields:\n",
        "                k = _product_key(fld)\n",
        "                extra += [f\"{k}_scan\", f\"{k}_cache_volume_path\", f\"{k}_matched_volume_s3_key\"]\n",
        "            return pd.DataFrame(columns=base_cols + extra)\n",
        "        else:\n",
        "            return pd.DataFrame(columns=base_cols + [\"radar_scan\", \"cache_member_name\", \"matched_member_name\"])\n",
        "\n",
        "    linked_df = pd.DataFrame(linked_rows)\n",
        "\n",
        "    # Column ordering like before\n",
        "    if allowed_fields:\n",
        "        prod_keys = [_product_key(p) for p in allowed_fields]\n",
        "        extra_cols = []\n",
        "        for k in prod_keys:\n",
        "            extra_cols += [f\"{k}_scan\", f\"{k}_cache_volume_path\", f\"{k}_matched_volume_s3_key\"]\n",
        "        cols = list(storm_df.columns) + extra_cols\n",
        "    else:\n",
        "        cols = list(storm_df.columns) + [\"radar_scan\", \"cache_member_name\", \"matched_member_name\"]\n",
        "\n",
        "    linked_df = linked_df.reindex(columns=cols)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[find_radar_scans] returning linked_df with {len(linked_df)} rows for site {site}\")\n",
        "        print(f\"\\n linked_radar_df shape: {linked_df.shape} \\n\")\n",
        "        print(linked_df.head(50))\n",
        "\n",
        "    return linked_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa3VW1sWEA4q"
      },
      "source": [
        "#### **Build Bounding Box**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TddOwlmCEWJt"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import numpy.ma as _ma\n",
        "import matplotlib.pyplot as _plt\n",
        "import functools\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from time import perf_counter\n",
        "from collections import OrderedDict\n",
        "from pyproj import Transformer\n",
        "from scipy.ndimage import binary_closing, label\n",
        "from scipy.spatial import cKDTree\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "\n",
        "\n",
        "########################################################################### GRID CACHE #############################################################################\n",
        "\n",
        "\n",
        "# Module globals\n",
        "_GRID_PLAN_CACHE = OrderedDict()\n",
        "_GRID_PLAN_CACHE_MAX = 16\n",
        "\n",
        "def _ray_canonical_roll(az):\n",
        "    \"\"\"Return roll offset so that the first ray is the one with the smallest azimuth (0..360).\"\"\"\n",
        "    azm = np.mod(np.asarray(az, dtype=float), 360.0)\n",
        "    if azm.size == 0 or not np.isfinite(azm).any():\n",
        "        return 0\n",
        "    return int(np.nanargmin(azm))\n",
        "\n",
        "def _grid_cache_set(key, plan):\n",
        "    # move-to-end semantics; evict oldest when full\n",
        "    _GRID_PLAN_CACHE[key] = plan\n",
        "    _GRID_PLAN_CACHE.move_to_end(key)\n",
        "    if len(_GRID_PLAN_CACHE) > _GRID_PLAN_CACHE_MAX:\n",
        "        _GRID_PLAN_CACHE.popitem(last=False)  # evict LRU\n",
        "\n",
        "\n",
        "def _grid_plan_key(comp_scan, grid_res_m, pad_m, class_field):\n",
        "    sweep = int(getattr(comp_scan, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "\n",
        "    # radar center\n",
        "    try:\n",
        "        rlat = float(comp_scan.latitude['data']) if np.isscalar(comp_scan.latitude['data']) else float(comp_scan.latitude['data'][0])\n",
        "        rlon = float(comp_scan.longitude['data']) if np.isscalar(comp_scan.longitude['data']) else float(comp_scan.longitude['data'][0])\n",
        "    except Exception:\n",
        "        rlat = float(np.nanmean(comp_scan.gate_latitude['data']))\n",
        "        rlon = float(np.nanmean(comp_scan.gate_longitude['data']))\n",
        "    rlon = ((rlon + 180.0) % 360.0) - 180.0\n",
        "    rlat_r = round(rlat, 4); rlon_r = round(rlon, 4)\n",
        "\n",
        "    start = comp_scan.sweep_start_ray_index['data'][sweep]\n",
        "    stop  = comp_scan.sweep_end_ray_index['data'][sweep]\n",
        "    n_rays  = int(stop - start)\n",
        "    n_gates = int(comp_scan.fields[class_field]['data'].shape[1])\n",
        "\n",
        "    # Range fingerprint (start and spacing rounded)\n",
        "    try:\n",
        "        rng = np.asarray(comp_scan.range['data'][:n_gates], dtype=float)\n",
        "        r0 = float(rng[0]) if rng.size else 0.0\n",
        "        dr = float(np.nanmean(np.diff(rng))) if rng.size > 1 else 0.0\n",
        "    except Exception:\n",
        "        r0, dr = 0.0, 0.0\n",
        "    r0_r = round(r0, 1); dr_r = round(dr, 1)\n",
        "\n",
        "    # NOTE: intentionally no azimuth hash here (too jittery scan-to-scan)\n",
        "    return (rlat_r, rlon_r, sweep, n_rays, n_gates, float(grid_res_m), float(pad_m), r0_r, dr_r)\n",
        "\n",
        "\n",
        "def _get_or_build_grid_plan(comp_scan, class_field, grid_res_m, pad_m, debug=False):\n",
        "    key = _grid_plan_key(comp_scan, grid_res_m, pad_m, class_field)\n",
        "    if key in _GRID_PLAN_CACHE:\n",
        "        plan = _GRID_PLAN_CACHE[key]\n",
        "        _GRID_PLAN_CACHE.move_to_end(key)\n",
        "        if debug:\n",
        "            print(f\"[_grid_plan] cache HIT â†’ grid={plan['ny']}x{plan['nx']}\")\n",
        "        return plan\n",
        "\n",
        "    if debug:\n",
        "        print(\"[_grid_plan] cache MISS â†’ building planâ€¦\")\n",
        "\n",
        "    sweep = key[2]\n",
        "    start = comp_scan.sweep_start_ray_index['data'][sweep]\n",
        "    stop  = comp_scan.sweep_end_ray_index['data'][sweep]\n",
        "\n",
        "    # Canonicalize ray order (roll so the smallest azimuth ray is first)\n",
        "    az = np.asarray(comp_scan.azimuth['data'][start:stop], dtype=float)\n",
        "    roll = _ray_canonical_roll(az)\n",
        "\n",
        "    comp_scan.init_gate_longitude_latitude()\n",
        "    lats_full = comp_scan.gate_latitude['data'][start:stop, :]\n",
        "    lons_full = comp_scan.gate_longitude['data'][start:stop, :]\n",
        "\n",
        "    # Roll ray axis to canonical order\n",
        "    lats = np.roll(lats_full, -roll, axis=0)\n",
        "    lons = np.roll(lons_full, -roll, axis=0)\n",
        "\n",
        "    # radar center (from key rounding)\n",
        "    radar_lat, radar_lon = key[0], key[1]\n",
        "\n",
        "    t_geog2xy, t_xy2geog = _cached_aeqd_transformers(round(radar_lat, 6), round(radar_lon, 6))\n",
        "\n",
        "    # valid mask in canonical order\n",
        "    valid_geo_2d = np.isfinite(lons) & np.isfinite(lats)\n",
        "    if not np.any(valid_geo_2d):\n",
        "        raise RuntimeError(\"[_get_or_build_grid_plan] no valid geo gates found\")\n",
        "\n",
        "    # project valid gates once (lon normalized around radar_lon for stability)\n",
        "    lon_valid = _normalize_lons_to_center(lons[valid_geo_2d], radar_lon)\n",
        "    lat_valid = lats[valid_geo_2d]\n",
        "    xx_flat, yy_flat = t_geog2xy.transform(lon_valid, lat_valid)\n",
        "\n",
        "    # grid extents + arrays\n",
        "    xmin, xmax = xx_flat.min() - pad_m, xx_flat.max() + pad_m\n",
        "    ymin, ymax = yy_flat.min() - pad_m, yy_flat.max() + pad_m\n",
        "    nx = int(np.ceil((xmax - xmin) / grid_res_m)) + 1\n",
        "    ny = int(np.ceil((ymax - ymin) / grid_res_m)) + 1\n",
        "    grid_x = np.linspace(xmin, xmax, nx)\n",
        "    grid_y = np.linspace(ymin, ymax, ny)\n",
        "\n",
        "    # KDTree over valid gates\n",
        "    pts = np.column_stack((xx_flat, yy_flat))\n",
        "    tree = cKDTree(pts)\n",
        "\n",
        "    # Prepare per-valid-point (ray, gate) indices in canonical order\n",
        "    n_rays = lats.shape[0]; n_gates = lats.shape[1]\n",
        "    ray_idx_2d = np.broadcast_to(np.arange(n_rays)[:, None], (n_rays, n_gates))\n",
        "    gate_idx_2d = np.broadcast_to(np.arange(n_gates)[None, :], (n_rays, n_gates))\n",
        "    valid_ray_idx = ray_idx_2d[valid_geo_2d].astype(np.int32, copy=False)\n",
        "    valid_gate_idx = gate_idx_2d[valid_geo_2d].astype(np.int32, copy=False)\n",
        "\n",
        "    # Query nearest for every grid node in stripes\n",
        "    grid_ray = np.empty(ny * nx, dtype=np.int32)\n",
        "    grid_gate = np.empty(ny * nx, dtype=np.int32)\n",
        "    block_rows = 512 if ny > 512 else ny\n",
        "    write_ptr = 0\n",
        "    for y0 in range(0, ny, block_rows):\n",
        "        y1 = min(y0 + block_rows, ny)\n",
        "        Xb = np.tile(grid_x, y1 - y0)\n",
        "        Yb = np.repeat(grid_y[y0:y1], nx)\n",
        "        _, idx = tree.query(np.column_stack((Xb, Yb)), k=1, workers=-1)\n",
        "        grid_ray[write_ptr:write_ptr + idx.size]  = valid_ray_idx[idx]\n",
        "        grid_gate[write_ptr:write_ptr + idx.size] = valid_gate_idx[idx]\n",
        "        write_ptr += idx.size\n",
        "\n",
        "    plan = {\n",
        "        't_geog2xy': t_geog2xy,\n",
        "        't_xy2geog': t_xy2geog,\n",
        "        'grid_x': grid_x, 'grid_y': grid_y,\n",
        "        'nx': nx, 'ny': ny,\n",
        "        # NEW: order-invariant mapping\n",
        "        'grid_ray': grid_ray,          # (ny*nx,) int32 â†’ ray index in canonical order\n",
        "        'grid_gate': grid_gate,        # (ny*nx,) int32 â†’ gate index\n",
        "        'ray_order_rule': 'min_az_first',  # for clarity/debug\n",
        "        'xmin': xmin, 'xmax': xmax, 'ymin': ymin, 'ymax': ymax,\n",
        "        'n_rays': n_rays, 'n_gates': n_gates,\n",
        "    }\n",
        "    _GRID_PLAN_CACHE[key] = plan\n",
        "    if debug:\n",
        "        print(f\"[_grid_plan] built new plan: grid={ny}x{nx}, valid_gates={pts.shape[0]}, roll={roll}, cache_size={len(_GRID_PLAN_CACHE)}\")\n",
        "    return plan\n",
        "\n",
        "\n",
        "\n",
        "###################################################################### PSEUDO-COMPOSITE #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "def _make_reflectivity_pseudocomposite(\n",
        "    radar: \"pyart.core.Radar\",\n",
        "    field_name: str = \"reflectivity\",\n",
        "    out_field: str = \"reflectivity\",      # keep same so downstream class_field='reflectivity' just works\n",
        "    max_tilts: int = 3,                   # unused here but kept for signature stability\n",
        "    chunk_size: int = 2048,               # rays per chunk to bound memory; None = no chunking\n",
        "    debug: bool = False,\n",
        "    plot_dir: str | None = None,\n",
        "    plot_stub: str | None = None\n",
        ") -> \"pyart.core.Radar\":\n",
        "    \"\"\"\n",
        "    Build a pseudo-composite reflectivity from a subset of tilts:\n",
        "      - choose all tilts to build true composite,\n",
        "      - per-gate max across the chosen tilts,\n",
        "      - returns a *new* Radar object with a single field `out_field`.\n",
        "    \"\"\"\n",
        "    from time import perf_counter\n",
        "\n",
        "    _t0 = perf_counter()\n",
        "\n",
        "    if field_name not in radar.fields:\n",
        "        raise KeyError(f\"[pseudo-comp] field '{field_name}' not found in radar.fields\")\n",
        "\n",
        "    nsw = int(radar.nsweeps)\n",
        "    sw_start = radar.sweep_start_ray_index['data'].astype(int)\n",
        "    sw_end   = radar.sweep_end_ray_index['data'].astype(int)\n",
        "\n",
        "    # fixed_angle is the canonical way to order tilts\n",
        "    _t_fixed0 = perf_counter()\n",
        "    try:\n",
        "        fixed = np.asarray(radar.fixed_angle['data']).astype(float)\n",
        "        if fixed.size != nsw:\n",
        "            fixed = np.array([\n",
        "                np.nanmean(radar.elevation['data'][sw_start[i]:sw_end[i]])\n",
        "                for i in range(nsw)\n",
        "            ], dtype=float)\n",
        "    except Exception:\n",
        "        fixed = np.array([\n",
        "            np.nanmean(radar.elevation['data'][sw_start[i]:sw_end[i]])\n",
        "            for i in range(nsw)\n",
        "        ], dtype=float)\n",
        "    _t_fixed1 = perf_counter()\n",
        "\n",
        "    # --- DEBUG: per-sweep timing / size summary ---------------------------------\n",
        "    _t_summ0 = perf_counter()\n",
        "    tsec = np.full(nsw, np.nan, dtype=float)\n",
        "    try:\n",
        "        t_all = np.asarray(radar.time['data'], dtype=float)\n",
        "        for i in range(nsw):\n",
        "            s, e = int(sw_start[i]), int(sw_end[i])\n",
        "            tsec[i] = float(np.nanmedian(t_all[s:e])) if e > s else np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if debug:\n",
        "        print(\"[_make_reflectivity_pseudocomposite] sweep summary (idx  elev(deg)  rays  gates  median_t[s]):\")\n",
        "        for i in range(nsw):\n",
        "            s, e = int(sw_start[i]), int(sw_end[i])\n",
        "            rays = int(max(0, e - s))\n",
        "            try:\n",
        "                gates = int(radar.fields[field_name]['data'].shape[1])\n",
        "            except Exception:\n",
        "                gates = -1\n",
        "            print(f\"    {i:02d}   {float(fixed[i]):5.2f}    {rays:5d}   {gates:5d}   {tsec[i]:10.1f}\")\n",
        "    _t_summ1 = perf_counter()\n",
        "\n",
        "    # use ALL available tilts, sorted by fixed angle (lowest â†’ highest)\n",
        "    _t_choose0 = perf_counter()\n",
        "    order = np.argsort(fixed)\n",
        "    if order.size == 0:\n",
        "        raise RuntimeError(\"[pseudo-comp] no sweeps available\")\n",
        "    chosen = order  # â† all sweeps\n",
        "    _t_choose1 = perf_counter()\n",
        "\n",
        "    if debug:\n",
        "        fa = [float(fixed[i]) for i in chosen]\n",
        "        ts = [float(tsec[i]) for i in chosen]\n",
        "        print(f\"[_make_reflectivity_pseudocomposite] chosen sweeps={chosen.tolist()} \"\n",
        "              f\"fixed_angles={fa} median_t[s]={ts}\")\n",
        "        '''\n",
        "        #################################################### COMMENT PLOTTING BLOCK OUT ON PRODUCTION RUNS ###############################################################\n",
        "\n",
        "        # Plot each contributing sweep (as-is in native file) for visual parity\n",
        "        _t_dbgplotA0 = perf_counter()\n",
        "        try:\n",
        "            from pyart.graph import RadarDisplay\n",
        "            disp = RadarDisplay(radar)\n",
        "            n = int(len(chosen))\n",
        "            fig, axes = _plt.subplots(1, n, figsize=(6*n, 5), squeeze=False)\n",
        "            for j, sw in enumerate(chosen):\n",
        "                ax = axes[0, j]\n",
        "                title = f\"{field_name} sweep {int(sw)} ({float(fixed[sw]):.1f}Â°)\"\n",
        "                try:\n",
        "                    disp.plot(field_name, sweep=int(sw), ax=ax, title=title)\n",
        "                    ax.set_aspect('equal', adjustable='box')\n",
        "                except Exception as e:\n",
        "                    ax.text(0.5, 0.5, f\"Plot failed: {e}\", ha='center', va='center')\n",
        "                    ax.set_title(title + \" [failed]\")\n",
        "            _plt.tight_layout();\n",
        "            stub = f\"{(plot_stub or 'pseudo')}_contrib_sweeps\"\n",
        "            _save_plot(fig=fig, plot_dir=plot_dir, func_name=\"_make_reflectivity_pseudocomposite\", stub=stub, debug=debug,)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[_make_reflectivity_pseudocomposite] debug plotting failed: {e}\")\n",
        "        _t_dbgplotA1 = perf_counter()\n",
        "\n",
        "        ##############################################################################################################################################################\n",
        "        '''\n",
        "    # --- geometry / host sweep selection ----------------------------------------\n",
        "    _t_host0 = perf_counter()\n",
        "    rays_per_sweep, gates_per_sweep = [], []\n",
        "    for i in chosen:\n",
        "        s, e = int(sw_start[i]), int(sw_end[i])\n",
        "        rays_per_sweep.append(int(max(0, e - s)))\n",
        "        gates_per_sweep.append(int(radar.fields[field_name]['data'].shape[1]))\n",
        "\n",
        "    host_idx_in_chosen = int(np.argmax(rays_per_sweep))\n",
        "    host_sweep = int(chosen[host_idx_in_chosen])\n",
        "\n",
        "    ngates_common = int(min(gates_per_sweep))\n",
        "    if ngates_common <= 0:\n",
        "        raise RuntimeError(\"[pseudo-comp] no common gate count across chosen sweeps\")\n",
        "    _t_host1 = perf_counter()\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[_make_reflectivity_pseudocomposite] host_sweep={host_sweep} \"\n",
        "              f\"(elev={float(fixed[host_sweep]):.2f}Â°), rays_host={rays_per_sweep[host_idx_in_chosen]}, \"\n",
        "              f\"ngates_common={ngates_common}\")\n",
        "\n",
        "    base_ma = radar.fields[field_name]['data']\n",
        "    if not isinstance(base_ma, _ma.MaskedArray):\n",
        "        base_ma = _ma.MaskedArray(base_ma, mask=np.zeros_like(base_ma, dtype=bool))\n",
        "\n",
        "    _t_hostaz0 = perf_counter()\n",
        "    hs, he = int(sw_start[host_sweep]), int(sw_end[host_sweep])\n",
        "    rays_host = int(max(0, he - hs))\n",
        "    host_az = np.array(radar.azimuth['data'][hs:he], dtype=float)\n",
        "    host_u = np.unwrap(np.deg2rad(host_az))  # radians, ~ span â‰ˆ 2Ï€\n",
        "    if debug:\n",
        "        dhost = np.diff(host_u)\n",
        "        nonmono = int(np.sum(dhost < 0))\n",
        "        print(f\"[_make_reflectivity_pseudocomposite] host az unwrap: rays={host_u.size}, \"\n",
        "              f\"span(deg)={np.degrees(host_u[-1]-host_u[0]):.2f}, non-monotonic diffs={nonmono}\")\n",
        "    _t_hostaz1 = perf_counter()\n",
        "\n",
        "    # --- accumulation & (optional) per-sweep reindexed debug stacks -------------\n",
        "    out_mask = np.ones((rays_host, ngates_common), dtype=bool)\n",
        "    acc_data = None\n",
        "    acc_valid_any = None\n",
        "\n",
        "    # Optional: store reindexed fields for \"winner\" map (guard memory)\n",
        "    store_reindexed = bool(debug) and (rays_host * ngates_common <= 4_000_000)  # ~16MB/sweep (float32)\n",
        "    reindexed_stack = [] if store_reindexed else None\n",
        "\n",
        "    _t_accum0 = perf_counter()\n",
        "    per_sweep_map_ms = []\n",
        "    per_sweep_accum_ms = []\n",
        "    for i in chosen:\n",
        "        s, e = int(sw_start[i]), int(sw_end[i])\n",
        "\n",
        "        # ---------- Mapping (source sweep â†’ host rays) ----------\n",
        "        _t_map0 = perf_counter()\n",
        "\n",
        "        # Source sweep azimuths (unwrapped absolute angles)\n",
        "        src_az = np.array(radar.azimuth['data'][s:e], dtype=float)\n",
        "        src_u  = np.unwrap(np.deg2rad(src_az))  # radians, length â‰ˆ 2Ï€ but offset vs host\n",
        "\n",
        "        # Align only by integer 2Ï€ cycles so src & host live on same unwrap \"cycle\"\n",
        "        two_pi = 2.0 * np.pi\n",
        "        k = int(np.round((host_u.mean() - src_u.mean()) / two_pi))\n",
        "        src_u_aligned = src_u + k * two_pi\n",
        "\n",
        "        # Sort once; keep mapping to original ray indices\n",
        "        order_idx = np.argsort(src_u_aligned)\n",
        "        src_sorted = src_u_aligned[order_idx]\n",
        "        src_sorted = np.maximum.accumulate(src_sorted)  # guard tiny non-monotonic dips\n",
        "        idx_sorted = order_idx  # original ray index at each sorted position\n",
        "        N = src_sorted.size\n",
        "\n",
        "        # ---------- PERIODIC EXTENSION (prevents endpoint smearing) ----------\n",
        "        src_ext = np.concatenate([src_sorted - two_pi, src_sorted, src_sorted + two_pi])\n",
        "        idx_ext = np.concatenate([idx_sorted,       idx_sorted,       idx_sorted      ])\n",
        "        M = src_ext.size  # = 3N\n",
        "\n",
        "        # Nearest-neighbor on the extended axis (no clamping to endpoints)\n",
        "        j = np.searchsorted(src_ext, host_u)  # insertion index\n",
        "        j0 = np.clip(j - 1, 0, M - 1)\n",
        "        j1 = np.clip(j,     0, M - 1)\n",
        "        d0 = np.abs(host_u - src_ext[j0])\n",
        "        d1 = np.abs(host_u - src_ext[j1])\n",
        "        pick_ext = np.where(d0 <= d1, j0, j1)     # nearest neighbor in the extended axis\n",
        "        src_idx  = idx_ext[pick_ext]              # map back to ORIGINAL ray indices (0..N_src-1)\n",
        "\n",
        "        # ---------- OPTIONAL: GAP GUARD (donâ€™t bridge real azimuth holes) ----------\n",
        "        gap_deg = 6.0  # tune as needed; 4â€“8Â° works well\n",
        "        gap_rad = np.deg2rad(gap_deg)\n",
        "\n",
        "        prev_ext = np.clip(pick_ext - 1, 0, M - 1)\n",
        "        next_ext = np.clip(pick_ext + 1, 0, M - 1)\n",
        "        left_span  = np.abs(src_ext[pick_ext] - src_ext[prev_ext])\n",
        "        right_span = np.abs(src_ext[next_ext] - src_ext[pick_ext])\n",
        "        local_gap  = np.maximum(left_span, right_span)  # conservative local spacing\n",
        "        bad_bridge = local_gap > gap_rad                # too sparse => likely a real gap\n",
        "\n",
        "        if debug:\n",
        "            # How bad was it before the fix?\n",
        "            rr = src_idx.copy()\n",
        "            rr[bad_bridge] = -1\n",
        "            max_run = 0\n",
        "            run = 0\n",
        "            prev = None\n",
        "            for v in rr:\n",
        "                if v >= 0 and v == prev:\n",
        "                    run += 1\n",
        "                else:\n",
        "                    max_run = max(max_run, run)\n",
        "                    run = 1\n",
        "                prev = v\n",
        "            max_run = max(max_run, run)\n",
        "            frac_bad = float(np.mean(bad_bridge)) if bad_bridge.size else 0.0\n",
        "            print(f\"[_make_reflectivity_pseudocomposite] map sweep {i:02d}â†’host {host_sweep:02d}: \"\n",
        "                  f\"k={k:+d}, max_identical_src_run={max_run}, gap_mask_frac={frac_bad:.3f}\")\n",
        "\n",
        "        _t_map1 = perf_counter()\n",
        "\n",
        "        # ---------- Accumulation (max over sweeps) ----------\n",
        "        _t_acc0 = perf_counter()\n",
        "\n",
        "        src_field = base_ma[s:e, :ngates_common]\n",
        "\n",
        "        # For bad_bridge positions, weâ€™ll force invalid by clearing 'valid' later.\n",
        "        src_idx_clip = np.clip(src_idx, 0, src_field.shape[0]-1)\n",
        "\n",
        "        # Accumulator for winner map (if enabled)\n",
        "        sweep_acc = None\n",
        "        if store_reindexed:\n",
        "            sweep_acc = np.full((rays_host, ngates_common), -np.inf, dtype=np.float32)\n",
        "\n",
        "        gate_chunk = ngates_common if (chunk_size is None or chunk_size <= 0) else int(max(1, min(chunk_size, ngates_common)))\n",
        "        for g0 in range(0, ngates_common, gate_chunk):\n",
        "            g1 = min(g0 + gate_chunk, ngates_common)\n",
        "\n",
        "            ch     = src_field[:, g0:g1]           # (rays_src, gates_chunk)\n",
        "            ch_re  = ch[src_idx_clip, :]           # (rays_host, gates_chunk)\n",
        "            ch_dat = ch_re.filled(np.nan).astype(np.float32, copy=False)\n",
        "            ch_msk = _ma.getmaskarray(ch_re)\n",
        "\n",
        "            # Valid data from the source AND not crossing a detected gap\n",
        "            valid_map = (~ch_msk) & np.isfinite(ch_dat) & (~bad_bridge[:, None])\n",
        "\n",
        "            if acc_data is None:\n",
        "                acc_data      = np.full((rays_host, ngates_common), -np.inf, dtype=np.float32)\n",
        "                acc_valid_any = np.zeros((rays_host, ngates_common), dtype=bool)\n",
        "\n",
        "            work = np.where(valid_map, ch_dat, -np.inf)\n",
        "            acc_data[:, g0:g1]      = np.maximum(acc_data[:, g0:g1], work)\n",
        "            acc_valid_any[:, g0:g1] |= valid_map\n",
        "\n",
        "            if store_reindexed:\n",
        "                sweep_acc[:, g0:g1] = work  # -inf where invalid\n",
        "\n",
        "        if store_reindexed:\n",
        "            reindexed_stack.append(sweep_acc)\n",
        "\n",
        "        _t_acc1 = perf_counter()\n",
        "\n",
        "        per_sweep_map_ms.append((_t_map1 - _t_map0) * 1000.0)\n",
        "        per_sweep_accum_ms.append((_t_acc1 - _t_acc0) * 1000.0)\n",
        "\n",
        "    _t_accum1 = perf_counter()\n",
        "\n",
        "    # finalize masked result on HOST grid\n",
        "    _t_finalize0 = perf_counter()\n",
        "    out_data = acc_data\n",
        "    out_mask = ~acc_valid_any\n",
        "    out_ma = _ma.MaskedArray(out_data, mask=out_mask)\n",
        "    _t_finalize1 = perf_counter()\n",
        "    '''\n",
        "    ######################################################## COMMENT PLOTTING BLOCK OUT ON PRODUCTION RUNS ########################################################\n",
        "\n",
        "    # --- DEBUG: winner map (which sweep contributed the max per gate) -----------\n",
        "    _t_winner0 = perf_counter()\n",
        "    if debug and store_reindexed and len(reindexed_stack) >= 1:\n",
        "        stack = np.stack(reindexed_stack, axis=0)  # (ns, rays_host, ngates_common)\n",
        "        winner = np.argmax(stack, axis=0)          # (rays_host, ngates_common)\n",
        "        maxval = np.max(stack, axis=0)\n",
        "        # gates with no valid contributor: mark as -1\n",
        "        winner = np.where(np.isfinite(maxval) & (maxval > -1e20), winner, -1)\n",
        "\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
        "            im = ax.imshow(winner.T, origin='lower', aspect='auto', interpolation='nearest')\n",
        "            ax.set_title(\"Winner map (which sweep provides max) [index in chosen]\")\n",
        "            ax.set_xlabel(\"ray (host az index)\"); ax.set_ylabel(\"gate\")\n",
        "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "            plt.tight_layout();\n",
        "\n",
        "            # Save to local directory\n",
        "            stub = f\"{(plot_stub or 'pseudo')}_winner_map\"\n",
        "            _save_plot(fig, plot_dir, \"_make_reflectivity_pseudocomposite\", stub, debug)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[_make_reflectivity_pseudocomposite] winner-map plotting failed: {e}\")\n",
        "    _t_winner1 = perf_counter()\n",
        "\n",
        "    #############################################################################################################################################################\n",
        "    '''\n",
        "    # skeleton radar with single host sweep\n",
        "    _t_skel0 = perf_counter()\n",
        "    skel = radar.deepcopy() if hasattr(radar, \"deepcopy\") else pickle.loads(pickle.dumps(radar, -1))\n",
        "\n",
        "    # Slice per-ray series down to the host sweep\n",
        "    for key in (\"time\", \"azimuth\", \"elevation\", \"sweep_number\"):\n",
        "        if key in skel.__dict__ and \"data\" in skel.__dict__[key]:\n",
        "            arr = skel.__dict__[key][\"data\"]\n",
        "            try:\n",
        "                skel.__dict__[key][\"data\"] = arr[hs:he]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    try:\n",
        "        skel.fixed_angle[\"data\"] = np.array([float(radar.fixed_angle[\"data\"][host_sweep])], dtype=float)\n",
        "    except Exception:\n",
        "        skel.fixed_angle[\"data\"] = np.array([float(np.nanmean(radar.elevation[\"data\"][hs:he]))], dtype=float)\n",
        "\n",
        "    try:\n",
        "        skel.range[\"data\"] = np.array(radar.range[\"data\"][:ngates_common], copy=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    skel.nsweeps = 1\n",
        "    skel.sweep_start_ray_index[\"data\"] = np.array([0], dtype=int)\n",
        "    skel.sweep_end_ray_index[\"data\"]   = np.array([rays_host], dtype=int)\n",
        "\n",
        "    for gate_key in (\"gate_longitude\", \"gate_latitude\", \"gate_altitude\"):\n",
        "        if hasattr(skel, gate_key):\n",
        "            try:\n",
        "                delattr(skel, gate_key)\n",
        "            except Exception:\n",
        "                pass\n",
        "    _t_skel1 = perf_counter()\n",
        "\n",
        "    _t_gate0 = perf_counter()\n",
        "    skel.init_gate_longitude_latitude()\n",
        "    if not hasattr(skel, 'gate_altitude'):\n",
        "        skel.init_gate_altitude()\n",
        "    _t_gate1 = perf_counter()\n",
        "\n",
        "    _t_fields0 = perf_counter()\n",
        "    skel.fields = {\n",
        "        out_field: {\n",
        "            \"data\": out_ma.astype(np.float32, copy=False),\n",
        "            \"long_name\": \"composite reflectivity (host sweep only)\",\n",
        "            \"standard_name\": out_field,\n",
        "            \"units\": radar.fields[field_name].get(\"units\", \"dBZ\")\n",
        "        }\n",
        "    }\n",
        "    skel.metadata = dict(getattr(skel, \"metadata\", {}) or {})\n",
        "    skel.metadata[\"pseudo_host_sweep\"] = 0\n",
        "    _t_fields1 = perf_counter()\n",
        "    '''\n",
        "    ######################################################## COMMENT PLOTTING BLOCK OUT ON PRODUCTION RUNS ########################################################\n",
        "\n",
        "    # --- DEBUG: final product sanity plots --------------------------------------\n",
        "    _t_dbgplotB0 = perf_counter()\n",
        "    if debug:\n",
        "        try:\n",
        "            from pyart.graph import RadarDisplay\n",
        "            import matplotlib.pyplot as plt\n",
        "            disp2 = RadarDisplay(skel)\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "            axA, axB = axes\n",
        "\n",
        "            # Final pseudo-comp in km coords\n",
        "            disp2.plot(out_field, sweep=0, ax=axA, title=\"Pseudo-composite (host grid)\")\n",
        "            axA.set_aspect('equal', adjustable='box')\n",
        "\n",
        "            # Threshold scatter of the final host product in lon/lat\n",
        "            skel.init_gate_longitude_latitude()\n",
        "            glat = skel.gate_latitude['data'][0: rays_host, :ngates_common]\n",
        "            glon = skel.gate_longitude['data'][0: rays_host, :ngates_common]\n",
        "            data = skel.fields[out_field]['data']\n",
        "            data_arr = data.filled(-9999.0) if isinstance(data, _ma.MaskedArray) else data\n",
        "\n",
        "            thr = 20.0  # or pass through from caller if you prefer\n",
        "            mask = (data_arr >= thr) & np.isfinite(glat) & np.isfinite(glon)\n",
        "            idx = np.flatnonzero(mask.ravel())\n",
        "            if idx.size > 100000:\n",
        "                idx = idx[:: int(idx.size // 100000) + 1]\n",
        "\n",
        "            axB.scatter(glon.ravel()[idx], glat.ravel()[idx], s=2, marker='.', alpha=0.6, zorder=5)\n",
        "            axB.set_title(f\"Final host gates â‰¥ {thr} dBZ (lon/lat)\")\n",
        "            axB.set_xlabel(\"Longitude\"); axB.set_ylabel(\"Latitude\")\n",
        "            axB.set_aspect('equal', adjustable='box')\n",
        "\n",
        "            plt.tight_layout();\n",
        "            stub = f\"{(plot_stub or 'pseudo')}_final_host\"\n",
        "            _save_plot(fig, plot_dir, \"_make_reflectivity_pseudocomposite\", stub, debug)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[_make_reflectivity_pseudocomposite] final plotting failed: {e}\")\n",
        "    _t_dbgplotB1 = perf_counter()\n",
        "\n",
        "    ################################################################################################################################################################\n",
        "    '''\n",
        "\n",
        "    # --------- PERF SUMMARY ------------------------------------------------------\n",
        "    if debug:\n",
        "        total_ms = (perf_counter() - _t0) * 1000.0\n",
        "        print(f\"[_make_reflectivity_pseudocomposite] PERF:\")\n",
        "        print(f\"    fixed-angle compute:         {(_t_fixed1  - _t_fixed0 )*1000:.1f} ms\")\n",
        "        print(f\"    sweep summary (timing):      {(_t_summ1   - _t_summ0  )*1000:.1f} ms\")\n",
        "        print(f\"    choose sweeps:               {(_t_choose1 - _t_choose0)*1000:.1f} ms\")\n",
        "        print(f\"    host selection + dims:       {(_t_host1   - _t_host0  )*1000:.1f} ms\")\n",
        "        print(f\"    host azimuth unwrap:         {(_t_hostaz1 - _t_hostaz0)*1000:.1f} ms\")\n",
        "        print(f\"    accumulation loop (total):   {(_t_accum1  - _t_accum0 )*1000:.1f} ms\")\n",
        "        if per_sweep_map_ms:\n",
        "            print(f\"        per-sweep mapping avg:   {np.mean(per_sweep_map_ms):.1f} ms  (n={len(per_sweep_map_ms)})\")\n",
        "            print(f\"        per-sweep mapping sum:   {np.sum(per_sweep_map_ms):.1f} ms\")\n",
        "        if per_sweep_accum_ms:\n",
        "            print(f\"        per-sweep accumulate avg:{np.mean(per_sweep_accum_ms):.1f} ms  (n={len(per_sweep_accum_ms)})\")\n",
        "            print(f\"        per-sweep accumulate sum:{np.sum(per_sweep_accum_ms):.1f} ms\")\n",
        "        print(f\"    finalize masked result:      {(_t_finalize1- _t_finalize0)*1000:.1f} ms\")\n",
        "        print(f\"    winner-map (debug):          {((_t_winner1 - _t_winner0 )*1000 if '_t_winner1' in locals() else 0):.1f} ms\")\n",
        "        print(f\"    skeleton build/slice:        {(_t_skel1   - _t_skel0  )*1000:.1f} ms\")\n",
        "        print(f\"    gate lon/lat/alt init:       {(_t_gate1   - _t_gate0  )*1000:.1f} ms\")\n",
        "        print(f\"    set fields/metadata:         {(_t_fields1 - _t_fields0)*1000:.1f} ms\")\n",
        "        print(f\"    debug plots (chosen sweeps): {((_t_dbgplotA1 - _t_dbgplotA0)*1000 if '_t_dbgplotA1' in locals() else 0):.1f} ms\")\n",
        "        print(f\"    debug plots (final host):    {((_t_dbgplotB1 - _t_dbgplotB0)*1000 if '_t_dbgplotB1' in locals() else 0):.1f} ms\")\n",
        "        print(f\"    TOTAL:                       {total_ms:.1f} ms\")\n",
        "\n",
        "    return skel\n",
        "\n",
        "\n",
        "\n",
        "########################################################################### BBOX-LOGIC #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "# Helper: weighted dimensions\n",
        "def _compute_weighted_dimensions(widths_m: np.ndarray, heights_m: np.ndarray, nbins=20, debug=False):\n",
        "    \"\"\"\n",
        "    Compute weighted average of dimensions (width, height in meters).\n",
        "    Weight is higher for rows whose width AND height are nearer the mode (histogram mode),\n",
        "    with robustness via MAD.\n",
        "    Returns (avg_width_m, avg_height_m, weights_array).\n",
        "    \"\"\"\n",
        "    widths = np.asarray(widths_m, dtype=float)\n",
        "    heights = np.asarray(heights_m, dtype=float)\n",
        "    assert widths.shape == heights.shape\n",
        "\n",
        "    # histogram-mode helper\n",
        "    def hist_mode(vals, nbins_local):\n",
        "        vals = vals[np.isfinite(vals)]\n",
        "        if vals.size == 0:\n",
        "            return np.nan\n",
        "        counts, edges = np.histogram(vals, bins=nbins_local)\n",
        "        idx = int(np.argmax(counts))\n",
        "        return 0.5 * (edges[idx] + edges[idx + 1])\n",
        "\n",
        "    mode_w = hist_mode(widths, nbins)\n",
        "    mode_h = hist_mode(heights, nbins)\n",
        "    if debug:\n",
        "        print(f\"[_compute_weighted_dimensions] mode_w={mode_w:.1f}, mode_h={mode_h:.1f}\")\n",
        "\n",
        "    # MAD scales (robust)\n",
        "    def mad_scale(vals):\n",
        "        v = vals[np.isfinite(vals)]\n",
        "        if v.size == 0:\n",
        "            return 1.0\n",
        "        med = np.median(v)\n",
        "        mad = np.median(np.abs(v - med))\n",
        "        return max(mad, 1.0)  # avoid near-zero scale\n",
        "\n",
        "    scale_w = mad_scale(widths)\n",
        "    scale_h = mad_scale(heights)\n",
        "\n",
        "    # deviation scores -> convert into weight scores (higher for smaller dev)\n",
        "    dev_w = np.abs(widths - mode_w)\n",
        "    dev_h = np.abs(heights - mode_h)\n",
        "\n",
        "    w_score = 1.0 / (1.0 + (dev_w / (scale_w + 1e-12)))\n",
        "    h_score = 1.0 / (1.0 + (dev_h / (scale_h + 1e-12)))\n",
        "\n",
        "    combined = w_score * h_score\n",
        "    # fallback to equal weights if something weird happens\n",
        "    if np.sum(combined) <= 0:\n",
        "        combined = np.ones_like(combined)\n",
        "\n",
        "    weights = combined / float(np.sum(combined))\n",
        "    avg_w = float(np.sum(weights * widths))\n",
        "    avg_h = float(np.sum(weights * heights))\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[_compute_weighted_dimensions] avg_w={avg_w:.1f} m, avg_h={avg_h:.1f} m\")\n",
        "    return avg_w, avg_h, weights\n",
        "\n",
        "\n",
        "def _bbox_center(minlat, maxlat, minlon, maxlon):\n",
        "    # robust center even if the box crosses the dateline\n",
        "    width = maxlon - minlon\n",
        "    if width < 0: width += 360.0\n",
        "    clon = minlon + 0.5 * width\n",
        "    clon = ((clon + 180.0) % 360.0) - 180.0\n",
        "    clat = 0.5 * (minlat + maxlat)\n",
        "    return clat, clon\n",
        "\n",
        "\n",
        "# Helper: fit averaged box to a scan\n",
        "def _fit_box_to_scan_max_overlap(comp_scan,\n",
        "                                 width_m, height_m,\n",
        "                                 class_field='reflectivity',\n",
        "                                 threshold=20,\n",
        "                                 search_km=20.0, step_km=2.5,\n",
        "                                 center_hint=None,      # <-- used now\n",
        "                                 debug=False):\n",
        "    \"\"\"\n",
        "    Grid-search candidate box centers around a chosen center (by default the\n",
        "    blob centroid; if center_hint is given, use that), and return\n",
        "    (minlat,maxlat,minlon,maxlon) of the best-scoring box (max fraction of\n",
        "    dbz>=threshold gates inside the box).\n",
        "    \"\"\"\n",
        "    # Build gate-level blob mask and gate coords\n",
        "    sweep = int(getattr(comp_scan, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "    start = comp_scan.sweep_start_ray_index['data'][sweep]\n",
        "    stop  = comp_scan.sweep_end_ray_index['data'][sweep]\n",
        "    dbz_field = comp_scan.fields[class_field]['data'][start:stop, :]\n",
        "    if isinstance(dbz_field, np.ma.MaskedArray):\n",
        "        dbz_arr = dbz_field.filled(np.nan)\n",
        "    else:\n",
        "        dbz_arr = dbz_field\n",
        "\n",
        "    comp_scan.init_gate_longitude_latitude()\n",
        "    gate_lats = comp_scan.gate_latitude['data'][start:stop, :]\n",
        "    gate_lons = comp_scan.gate_longitude['data'][start:stop, :]\n",
        "\n",
        "    # normalize gate longitudes around radar lon\n",
        "    try:\n",
        "        radar_lon = float(comp_scan.longitude['data']) if np.isscalar(comp_scan.longitude['data']) else float(comp_scan.longitude['data'][0])\n",
        "    except Exception:\n",
        "        radar_lon = None\n",
        "    if radar_lon is not None:\n",
        "        radar_lon_norm = ((radar_lon + 180.0) % 360.0) - 180.0\n",
        "        gate_lons = _normalize_lons_to_center(gate_lons, radar_lon_norm)\n",
        "    else:\n",
        "        radar_lon_norm = None\n",
        "\n",
        "    blob_mask = (dbz_arr >= threshold) & np.isfinite(dbz_arr)\n",
        "    total_blob = float(blob_mask.sum())\n",
        "    # if no blob data, fallback to scan centroid area near radar\n",
        "    if total_blob <= 0:\n",
        "        try:\n",
        "            rlat = float(comp_scan.latitude['data']) if np.isscalar(comp_scan.latitude['data']) else float(comp_scan.latitude['data'][0])\n",
        "            rlon = float(comp_scan.longitude['data']) if np.isscalar(comp_scan.longitude['data']) else float(comp_scan.longitude['data'][0])\n",
        "        except Exception:\n",
        "            rlat = float(np.nanmean(gate_lats[np.isfinite(gate_lats)]))\n",
        "            rlon = float(np.nanmean(gate_lons[np.isfinite(gate_lons)]))\n",
        "        return rlat - 0.5, rlat + 0.5, rlon - 0.5, rlon + 0.5\n",
        "\n",
        "    # centroid (in lon/lat) of ALL gates >= threshold (baseline)\n",
        "    valid = blob_mask & np.isfinite(gate_lats) & np.isfinite(gate_lons)\n",
        "    if valid.sum() == 0:\n",
        "        centroid_lat = float(np.nanmean(gate_lats[np.isfinite(gate_lats)]))\n",
        "        centroid_lon = float(np.nanmean(gate_lons[np.isfinite(gate_lons)]))\n",
        "    else:\n",
        "        centroid_lat = float(np.nanmean(gate_lats[valid]))\n",
        "        centroid_lon = float(np.nanmean(gate_lons[valid]))\n",
        "\n",
        "    # choose center: override with center_hint if provided\n",
        "    used_lat, used_lon = centroid_lat, centroid_lon\n",
        "    if center_hint is not None:\n",
        "        try:\n",
        "            hl, ho = float(center_hint[0]), float(center_hint[1])\n",
        "            # normalize hint lon consistently with gate_lons\n",
        "            if radar_lon_norm is not None:\n",
        "                ho = _normalize_lons_to_center(ho, radar_lon_norm)\n",
        "            else:\n",
        "                ho = ((ho + 180.0) % 360.0) - 180.0\n",
        "            used_lat, used_lon = hl, ho\n",
        "            if debug:\n",
        "                print(f\"[_fit_box_to_scan_max_overlap] using center_hint=({used_lat:.4f},{used_lon:.4f}) \"\n",
        "                      f\"(raw centroid was {centroid_lat:.4f},{centroid_lon:.4f})\")\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[_fit_box_to_scan_max_overlap] center_hint ignored due to error: {e}\")\n",
        "\n",
        "    # prepare projection to metric coordinates (centered at radar)\n",
        "    if debug: _t_grid0 = perf_counter()\n",
        "    info = _compute_metric_grid_and_labels(\n",
        "        comp_scan, class_field=class_field, threshold=threshold,\n",
        "        pad_m=0.0, grid_res_m=1000.0, debug=False\n",
        "    )\n",
        "    if debug:\n",
        "        _t_grid1 = perf_counter()\n",
        "        ny = int(info['grid_y'].size); nx = int(info['grid_x'].size)\n",
        "        print(f\"[_find_blob_for_point] grid+labeling: {(_t_grid1 - _t_grid0):.3f}s  (ny={ny}, nx={nx}, pixels={ny*nx})\")\n",
        "\n",
        "    t_geog2xy = info['t_geog2xy']\n",
        "    t_xy2geog = info['t_xy2geog']\n",
        "\n",
        "    # search around the chosen center\n",
        "    cx_m, cy_m = t_geog2xy.transform(used_lon, used_lat)\n",
        "\n",
        "    search_m = float(search_km) * 1000.0\n",
        "    step_m = float(step_km) * 1000.0\n",
        "    offsets = np.arange(-search_m, search_m + 1e-9, step_m)\n",
        "\n",
        "    # flatten gate coords & blob mask for fast checks\n",
        "    gate_lons_flat = gate_lons.ravel()\n",
        "    gate_lats_flat = gate_lats.ravel()\n",
        "    blob_flat = blob_mask.ravel()\n",
        "    total_blob_pts = float(blob_flat.sum())\n",
        "\n",
        "    best_score = -1.0\n",
        "    best_box = None\n",
        "\n",
        "    for dx in offsets:\n",
        "        for dy in offsets:\n",
        "            center_x = cx_m + dx\n",
        "            center_y = cy_m + dy\n",
        "            minx = center_x - (width_m / 2.0)\n",
        "            maxx = center_x + (width_m / 2.0)\n",
        "            miny = center_y - (height_m / 2.0)\n",
        "            maxy = center_y + (height_m / 2.0)\n",
        "            # convert corners back to lon/lat for gate arithmetic\n",
        "            lon_min, lat_min = t_xy2geog.transform(minx, miny)\n",
        "            lon_max, lat_max = t_xy2geog.transform(maxx, maxy)\n",
        "            if radar_lon_norm is not None:\n",
        "                lon_min = _normalize_lons_to_center(lon_min, radar_lon_norm)\n",
        "                lon_max = _normalize_lons_to_center(lon_max, radar_lon_norm)\n",
        "            else:\n",
        "                lon_min = ((lon_min + 180.0) % 360.0) - 180.0\n",
        "                lon_max = ((lon_max + 180.0) % 360.0) - 180.0\n",
        "\n",
        "            # gate inclusion test (degrees, approximate)\n",
        "            inside = ((gate_lats_flat >= min(lat_min, lat_max)) &\n",
        "                      (gate_lats_flat <= max(lat_min, lat_max)) &\n",
        "                      (gate_lons_flat >= min(lon_min, lon_max)) &\n",
        "                      (gate_lons_flat <= max(lon_min, lon_max)))\n",
        "            if inside.sum() == 0:\n",
        "                score = 0.0\n",
        "            else:\n",
        "                score = float((blob_flat & inside).sum()) / (total_blob_pts + 1e-12)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_box = (min(lat_min, lat_max),\n",
        "                            max(lat_min, lat_max),\n",
        "                            min(lon_min, lon_max),\n",
        "                            max(lon_min, lon_max))\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[_fit_box_to_scan_max_overlap] best_score={best_score:.3f}\")\n",
        "        if best_box is not None:\n",
        "            bminlat, bmaxlat, bminlon, bmaxlon = best_box\n",
        "            print(f\"[_fit_box_to_scan_max_overlap] best_box \"\n",
        "                  f\"lat [{bminlat:.4f}, {bmaxlat:.4f}] \"\n",
        "                  f\"lon [{bminlon:.4f}, {bmaxlon:.4f}] \"\n",
        "                  f\"(center_used {used_lat:.4f},{used_lon:.4f})\")\n",
        "\n",
        "    return best_box  # minlat, maxlat, minlon, maxlon\n",
        "\n",
        "\n",
        "# small helper: normalize longitudes to be continuous around a center lon\n",
        "def _normalize_lons_to_center(lons, center_lon):\n",
        "    \"\"\"\n",
        "    Normalize lon(s) so they are in the range [center_lon-180, center_lon+180),\n",
        "    preserving continuity around center_lon (works with scalars or numpy arrays).\n",
        "    \"\"\"\n",
        "    # broadcast to numpy array\n",
        "    lon_arr = np.array(lons)\n",
        "    # compute centered difference in (-180,180]\n",
        "    diff = ((lon_arr - center_lon + 180.0) % 360.0) - 180.0\n",
        "    return center_lon + diff\n",
        "\n",
        "# For usage in _compute_metric_grid_and_labels\n",
        "@functools.lru_cache(maxsize=64)\n",
        "def _cached_aeqd_transformers(lat0_round6, lon0_round6):\n",
        "    lat0 = float(lat0_round6)\n",
        "    lon0 = float(lon0_round6)\n",
        "    proj_str = f\"+proj=aeqd +lat_0={lat0} +lon_0={lon0} +units=m +datum=WGS84\"\n",
        "    t_geog2xy = Transformer.from_crs(\"EPSG:4326\", proj_str, always_xy=True)\n",
        "    t_xy2geog = Transformer.from_crs(proj_str, \"EPSG:4326\", always_xy=True)\n",
        "    return t_geog2xy, t_xy2geog\n",
        "\n",
        "\n",
        "def _compute_metric_grid_and_labels(comp_scan,\n",
        "                                    class_field='reflectivity',\n",
        "                                    threshold=20,\n",
        "                                    pad_m=5000.0,\n",
        "                                    grid_res_m=1000.0,\n",
        "                                    debug=False):\n",
        "    _t0 = perf_counter()\n",
        "\n",
        "    # locate sweep & read field for thresholding\n",
        "    sweep = int(getattr(comp_scan, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "    start = comp_scan.sweep_start_ray_index['data'][sweep]\n",
        "    stop  = comp_scan.sweep_end_ray_index['data'][sweep]\n",
        "\n",
        "    _t_fields0 = perf_counter()\n",
        "    dbz_cls = comp_scan.fields[class_field]['data'][start:stop, :]\n",
        "    _t_fields1 = perf_counter()\n",
        "\n",
        "    _t_geo0 = perf_counter()\n",
        "    comp_scan.init_gate_longitude_latitude()\n",
        "    lats_full = comp_scan.gate_latitude['data'][start:stop, :]\n",
        "    lons_full = comp_scan.gate_longitude['data'][start:stop, :]\n",
        "\n",
        "    # Canonical roll to match plan ordering\n",
        "    az = np.asarray(comp_scan.azimuth['data'][start:stop], dtype=float)\n",
        "    roll = _ray_canonical_roll(az)\n",
        "    lats = np.roll(lats_full, -roll, axis=0)\n",
        "    lons = np.roll(lons_full, -roll, axis=0)\n",
        "    _t_geo1 = perf_counter()\n",
        "\n",
        "    # Fill masked -> ndarray if needed\n",
        "    _t_fill0 = perf_counter()\n",
        "    if isinstance(dbz_cls, np.ma.MaskedArray):\n",
        "        if debug: print(\"[_compute_metric_grid_and_labels] dbz_cls is MaskedArray -> filled\")\n",
        "        dbz_cls = dbz_cls.filled(0)\n",
        "    # keep data rolled to canonical order too\n",
        "    dbz_cls = np.roll(dbz_cls, -roll, axis=0)\n",
        "    _t_fill1 = perf_counter()\n",
        "\n",
        "    # threshold mask (in canonical order)\n",
        "    _t_thr0 = perf_counter()\n",
        "    mask2d = (dbz_cls >= threshold)\n",
        "    _t_thr1 = perf_counter()\n",
        "\n",
        "    # respect invalid geo gates (canonical order)\n",
        "    _t_bad0 = perf_counter()\n",
        "    bad_geo = ~np.isfinite(lats) | ~np.isfinite(lons)\n",
        "    if np.any(bad_geo):\n",
        "        mask2d = np.where(bad_geo, False, mask2d)\n",
        "        if debug: print(f\"[_compute_metric_grid_and_labels] masked out {int(bad_geo.sum())} invalid geo gates\")\n",
        "    valid_geo = ~bad_geo\n",
        "    _t_bad1 = perf_counter()\n",
        "\n",
        "    # plan (order-invariant); pre-check cache hit status for debug\n",
        "    cache_hit = None\n",
        "    if debug:\n",
        "        try:\n",
        "            _key_probe = _grid_plan_key(comp_scan, grid_res_m, pad_m, class_field)\n",
        "            cache_hit = _key_probe in _GRID_PLAN_CACHE\n",
        "        except Exception:\n",
        "            cache_hit = None\n",
        "\n",
        "    _t_plan0 = perf_counter()\n",
        "    plan = _get_or_build_grid_plan(comp_scan, class_field, grid_res_m, pad_m, debug=debug)\n",
        "    ny, nx = plan['ny'], plan['nx']\n",
        "    _t_plan1 = perf_counter()\n",
        "\n",
        "    # map mask â†’ grid\n",
        "    _t_flat0 = perf_counter()\n",
        "    # (kept for compatibility with old logs even if not used in new path)\n",
        "    # valid_geo_n etc. still computed from 'valid_geo'\n",
        "    _t_flat1 = perf_counter()\n",
        "\n",
        "    _t_map0 = perf_counter()\n",
        "    if ('grid_ray' in plan) and ('grid_gate' in plan):\n",
        "        # order-invariant pair indexing path\n",
        "        grid_vals = mask2d[plan['grid_ray'], plan['grid_gate']].reshape(ny, nx)\n",
        "    else:\n",
        "        # legacy fallback path\n",
        "        mask_flat = mask2d.ravel()[valid_geo.ravel()].astype(np.uint8, copy=False)\n",
        "        grid_vals = mask_flat[plan['idx_grid']].reshape(ny, nx)\n",
        "    _t_map1 = perf_counter()\n",
        "\n",
        "    # morphology + labeling\n",
        "    _t_morph0 = perf_counter()\n",
        "    struct_xy = np.ones((3, 3), dtype=bool)\n",
        "    grid_mask = binary_closing(grid_vals >= 1, structure=struct_xy)\n",
        "    _t_morph1 = perf_counter()\n",
        "\n",
        "    _t_label0 = perf_counter()\n",
        "    labeled_grid, n_blob = label(grid_mask)\n",
        "    _t_label1 = perf_counter()\n",
        "\n",
        "    if debug:\n",
        "        rays = int(stop - start)\n",
        "        gates = int(dbz_cls.shape[1]) if hasattr(dbz_cls, \"shape\") and len(dbz_cls.shape) >= 2 else -1\n",
        "        valid_geo_n = int(valid_geo.sum()) if hasattr(valid_geo, \"sum\") else -1\n",
        "        cache_note = (\"hit\" if cache_hit else \"miss\") if cache_hit is not None else \"n/a\"\n",
        "        print(f\"[_compute_metric_grid_and_labels] dims: sweep={sweep} rays={rays} gates={gates}  \"\n",
        "              f\"grid={ny}x{nx} ({ny*nx} px)  valid_geo={valid_geo_n}  plan_cache={cache_note}\")\n",
        "        print(f\"    read fields:          {(_t_fields1 - _t_fields0)*1000:.1f} ms\")\n",
        "        print(f\"    init/get lon/lat:     {(_t_geo1    - _t_geo0   )*1000:.1f} ms\")\n",
        "        print(f\"    fill masked -> array: {(_t_fill1   - _t_fill0  )*1000:.1f} ms\")\n",
        "        print(f\"    threshold compare:    {(_t_thr1    - _t_thr0   )*1000:.1f} ms\")\n",
        "        print(f\"    bad_geo apply:        {(_t_bad1    - _t_bad0   )*1000:.1f} ms\")\n",
        "        print(f\"    get/build plan:       {(_t_plan1   - _t_plan0  )*1000:.1f} ms\")\n",
        "        print(f\"    flatten/select:       {(_t_flat1   - _t_flat0  )*1000:.1f} ms\")\n",
        "        # label kept for backward-compatibility even though we now use (ray,gate) pairs\n",
        "        print(f\"    mapâ†’grid (idx_grid):  {(_t_map1    - _t_map0   )*1000:.1f} ms\")\n",
        "        print(f\"    binary_closing:       {(_t_morph1  - _t_morph0 )*1000:.1f} ms\")\n",
        "        print(f\"    labeling:             {(_t_label1  - _t_label0 )*1000:.1f} ms\")\n",
        "        print(f\"    TOTAL:                {(perf_counter() - _t0   )*1000:.1f} ms\")\n",
        "        print(f\"[_compute_metric_grid_and_labels] labeled blobs on grid: {n_blob}\")\n",
        "\n",
        "    return {\n",
        "        'labeled_grid': labeled_grid,\n",
        "        'grid_x': plan['grid_x'],\n",
        "        'grid_y': plan['grid_y'],\n",
        "        't_xy2geog': plan['t_xy2geog'],\n",
        "        't_geog2xy': plan['t_geog2xy'],\n",
        "        'xmin': plan['xmin'], 'xmax': plan['xmax'], 'ymin': plan['ymin'], 'ymax': plan['ymax'],\n",
        "        'grid_mask': grid_mask\n",
        "    }\n",
        "\n",
        "\n",
        "def _find_blob_for_point(comp_scan,\n",
        "                         center_lat, center_lon,\n",
        "                         class_field='reflectivity',\n",
        "                         threshold=20,\n",
        "                         min_size=2500,\n",
        "                         pad_m=5000.0,\n",
        "                         grid_res_m=1000.0,\n",
        "                         include_nearby_km=10.0,\n",
        "                         debug=False,\n",
        "                         plot_dir: str | None = None,\n",
        "                         plot_stub: str | None = None):\n",
        "    \"\"\"\n",
        "    (fast) Find the labeled blob containing (center_lat, center_lon) and return\n",
        "    bbox with recursive merging of nearby blobs. Returns (minlat,maxlat,minlon,maxlon, centers_list).\n",
        "    \"\"\"\n",
        "    from scipy import ndimage as ndi  # local import to keep module deps light\n",
        "\n",
        "    # timing debug\n",
        "    if debug:\n",
        "        from time import perf_counter\n",
        "        _t0 = perf_counter()\n",
        "\n",
        "    # normalize center lon to canonical range [-180,180)\n",
        "    center_lon = ((center_lon + 180.0) % 360.0) - 180.0\n",
        "\n",
        "    # grid & labeling (unchanged helper)\n",
        "    info = _compute_metric_grid_and_labels(\n",
        "        comp_scan, class_field=class_field, threshold=threshold,\n",
        "        pad_m=pad_m, grid_res_m=grid_res_m, debug=debug\n",
        "    )\n",
        "    labeled_grid = info['labeled_grid']      # (ny, nx), labels 0..N\n",
        "    grid_x = info['grid_x']                  # metric x coords (nx,)\n",
        "    grid_y = info['grid_y']                  # metric y coords (ny,)\n",
        "    t_xy2geog = info['t_xy2geog']\n",
        "    t_geog2xy = info['t_geog2xy']\n",
        "    grid_mask = info['grid_mask']            # boolean mask of valid > threshold\n",
        "\n",
        "    # center in metric coords (meters)\n",
        "    cx_m, cy_m = t_geog2xy.transform(center_lon, center_lat)\n",
        "\n",
        "    # grid cell containing the center (fast, vectorized edges)\n",
        "    xi = np.searchsorted(grid_x, cx_m) - 1\n",
        "    yi = np.searchsorted(grid_y, cy_m) - 1\n",
        "    xi = max(0, min(xi, grid_x.size - 1))\n",
        "    yi = max(0, min(yi, grid_y.size - 1))\n",
        "\n",
        "    # ----- FAST region statistics -----\n",
        "    # counts per label (O(N))\n",
        "    lab_flat = labeled_grid.ravel()\n",
        "    if debug: _t_stats0 = perf_counter()\n",
        "    counts = np.bincount(lab_flat)  # index 0 is background\n",
        "    # keep labels with enough pixels\n",
        "    valid_labels = np.nonzero(counts >= max(1, int(min_size)))[0]\n",
        "    valid_labels = valid_labels[valid_labels != 0]  # drop background\n",
        "\n",
        "    if valid_labels.size == 0:\n",
        "        if debug:\n",
        "            print(\"[_find_blob_for_point] no blobs passed the min_size filter\")\n",
        "            _t_end = perf_counter()\n",
        "            print(f\"[_find_blob_for_point] TOTAL: {(_t_end - _t0):.3f}s (early exit: no blobs)\")\n",
        "        return None, None, None, None, []\n",
        "\n",
        "    # quick bbox per label via slices (O(N))\n",
        "    obj_slices = ndi.find_objects(labeled_grid, max_label=int(labeled_grid.max()))\n",
        "    # center-of-mass for all valid labels in one go (fast C-impl)\n",
        "    # (use grid_mask as input so COM is over \"inside\" pixels only)\n",
        "    coms = ndi.center_of_mass(grid_mask.astype(np.float32), labeled_grid, index=valid_labels)\n",
        "    coms = np.asarray(coms, dtype=float)  # shape (k, 2) -> (y, x) in index space\n",
        "\n",
        "    # convert COM (index space) -> metric coords\n",
        "    # Using linear mapping from index to coord: x = grid_x[idx], y = grid_y[idx]\n",
        "    # center_of_mass returns fractional index positions; map by interpolation on the arrays\n",
        "    x_idx = coms[:, 1]\n",
        "    y_idx = coms[:, 0]\n",
        "    # Faster than np.interp per row: use uniform spacing if grid is uniform; otherwise fall back to interp.\n",
        "    # grid_x/y are produced via linspace -> uniform\n",
        "    dx = grid_x[1] - grid_x[0]\n",
        "    dy = grid_y[1] - grid_y[0]\n",
        "    cx = grid_x[0] + x_idx * dx\n",
        "    cy = grid_y[0] + y_idx * dy\n",
        "\n",
        "    # Build per-label bboxes in metric coords using the object slices\n",
        "    # find_objects returns slices indexed by label-1\n",
        "    minx = np.empty(valid_labels.size, dtype=float)\n",
        "    maxx = np.empty(valid_labels.size, dtype=float)\n",
        "    miny = np.empty(valid_labels.size, dtype=float)\n",
        "    maxy = np.empty(valid_labels.size, dtype=float)\n",
        "\n",
        "    # vectorizable using slice arrays\n",
        "    # Each slice is (slice_y, slice_x) with .start/.stop indices\n",
        "    sl_y = np.fromiter((obj_slices[lab-1][0].start for lab in valid_labels), dtype=np.int64, count=valid_labels.size)\n",
        "    el_y = np.fromiter((obj_slices[lab-1][0].stop  for lab in valid_labels), dtype=np.int64, count=valid_labels.size)\n",
        "    sl_x = np.fromiter((obj_slices[lab-1][1].start for lab in valid_labels), dtype=np.int64, count=valid_labels.size)\n",
        "    el_x = np.fromiter((obj_slices[lab-1][1].stop  for lab in valid_labels), dtype=np.int64, count=valid_labels.size)\n",
        "\n",
        "    # Convert index bounds to metric coords (stop is exclusive; use stop-1)\n",
        "    minx[:] = grid_x[sl_x]\n",
        "    maxx[:] = grid_x[np.maximum(el_x - 1, 0)]\n",
        "    miny[:] = grid_y[sl_y]\n",
        "    maxy[:] = grid_y[np.maximum(el_y - 1, 0)]\n",
        "\n",
        "    # transform all centroids -> lon/lat in one call (vectorized)\n",
        "    clon, clat = t_xy2geog.transform(cx, cy)\n",
        "    # normalize longitudes\n",
        "    clon = ((np.asarray(clon) + 180.0) % 360.0) - 180.0\n",
        "\n",
        "    # Build centers list (same schema as before)\n",
        "    centers = [\n",
        "        {\n",
        "            'label': int(lab),\n",
        "            'clat': float(clat[i]),\n",
        "            'clon': float(clon[i]),\n",
        "            'cx':   float(cx[i]),\n",
        "            'cy':   float(cy[i]),\n",
        "            'size': int(counts[lab]),\n",
        "            'minx': float(minx[i]), 'maxx': float(maxx[i]),\n",
        "            'miny': float(miny[i]), 'maxy': float(maxy[i]),\n",
        "        }\n",
        "        for i, lab in enumerate(valid_labels)\n",
        "    ]\n",
        "    if debug:\n",
        "        _t_stats1 = perf_counter()\n",
        "        print(f\"[_find_blob_for_point] region stats: {(_t_stats1 - _t_stats0):.3f}s  \"\n",
        "              f\"(labels={valid_labels.size}, total_cells={labeled_grid.size})\")\n",
        "\n",
        "    # ----- choose initial label -----\n",
        "    label_at_center = int(labeled_grid[yi, xi]) if (0 <= yi < labeled_grid.shape[0] and 0 <= xi < labeled_grid.shape[1]) else 0\n",
        "    if label_at_center != 0 and (label_at_center in valid_labels):\n",
        "        chosen_label = int(label_at_center)\n",
        "        if debug:\n",
        "            print(f\"[_find_blob_for_point] center cell belongs to label {chosen_label}\")\n",
        "    else:\n",
        "        # vectorized nearest centroid in metric space\n",
        "        dxv = cx - cx_m\n",
        "        dyv = cy - cy_m\n",
        "        nearest_idx = int(np.argmin(dxv*dxv + dyv*dyv))\n",
        "        chosen_label = int(valid_labels[nearest_idx])\n",
        "        if debug:\n",
        "            best_dist = float(np.hypot(dxv[nearest_idx], dyv[nearest_idx]))\n",
        "            print(f\"[_find_blob_for_point] chosen nearest label {chosen_label} (dist {best_dist:.1f} m)\")\n",
        "    if debug:\n",
        "        _t_choose = perf_counter()\n",
        "        print(f\"[_find_blob_for_point] choose-initial-label: {(_t_choose - _t_stats1):.3f}s (label={chosen_label})\")\n",
        "\n",
        "    # ----- iterative merging (same logic; operates on arrays) -----\n",
        "    expand_m = float(include_nearby_km) * 1000.0\n",
        "    included = {chosen_label}\n",
        "    if debug:\n",
        "        print(f\"[_find_blob_for_point] starting merge with label {chosen_label}, expand_km={include_nearby_km}\")\n",
        "\n",
        "    # index map from label -> row index in our arrays\n",
        "    lab_to_idx = {int(lab): int(i) for i, lab in enumerate(valid_labels)}\n",
        "\n",
        "    def union_bbox(indices):\n",
        "        # vectorized union over selected indices\n",
        "        return (minx[indices].min(), maxx[indices].max(),\n",
        "                miny[indices].min(), maxy[indices].max())\n",
        "\n",
        "    iteration = 0\n",
        "    if debug: _t_merge0 = perf_counter()\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        idxs = np.fromiter((lab_to_idx[l] for l in included), dtype=int)\n",
        "        uminx, umaxx, uminy, umaxy = union_bbox(idxs)\n",
        "\n",
        "        # expanded union bbox\n",
        "        exp_minx = uminx - expand_m\n",
        "        exp_maxx = umaxx + expand_m\n",
        "        exp_miny = uminy - expand_m\n",
        "        exp_maxy = umaxy + expand_m\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[_find_blob_for_point] iter {iteration}: included={sorted(included)}, \"\n",
        "                  f\"union_bbox_m=[{uminx:.1f},{umaxx:.1f}] x [{uminy:.1f},{umaxy:.1f}], \"\n",
        "                  f\"expanded by {expand_m:.1f} m\")\n",
        "\n",
        "        # Compute bbox-to-bbox distance to all *not yet included* candidates (vectorized)\n",
        "        remaining = [l for l in valid_labels if l not in included]\n",
        "        if not remaining:\n",
        "            break\n",
        "        r_idx = np.array([lab_to_idx[int(l)] for l in remaining], dtype=int)\n",
        "\n",
        "        # dx: horizontal gap (0 if overlapping)\n",
        "        dx_left  = np.maximum(0.0, minx[r_idx] - exp_maxx)\n",
        "        dx_right = np.maximum(0.0, exp_minx - maxx[r_idx])\n",
        "        dx_gap = np.where(dx_left > 0, dx_left, dx_right)\n",
        "\n",
        "        # dy: vertical gap (0 if overlapping)\n",
        "        dy_low   = np.maximum(0.0, miny[r_idx] - exp_maxy)\n",
        "        dy_high  = np.maximum(0.0, exp_miny - maxy[r_idx])\n",
        "        dy_gap = np.where(dy_low > 0, dy_low, dy_high)\n",
        "\n",
        "        dist = np.hypot(dx_gap, dy_gap)\n",
        "        add_mask = (dist <= expand_m)  # overlap or within expand distance\n",
        "        if not np.any(add_mask):\n",
        "            if debug:\n",
        "                print(f\"[_find_blob_for_point] iter {iteration}: no new blobs added; merging finished\")\n",
        "            break\n",
        "\n",
        "        for l in np.asarray(remaining)[add_mask]:\n",
        "            included.add(int(l))\n",
        "            if debug:\n",
        "                print(f\"[_find_blob_for_point] iter {iteration}: added label {int(l)} (bbox-dist {float(dist[add_mask][0]):.1f} m)\")\n",
        "    if debug:\n",
        "        _t_merge1 = perf_counter()\n",
        "        print(f\"[_find_blob_for_point] merge nearby: {(_t_merge1 - _t_merge0):.3f}s  \"\n",
        "              f\"(iters={iteration}, included={len(included)})\")\n",
        "\n",
        "    # final union bbox (metric coords)\n",
        "    idxs = np.fromiter((lab_to_idx[l] for l in included), dtype=int)\n",
        "    final_minx = float(minx[idxs].min())\n",
        "    final_maxx = float(maxx[idxs].max())\n",
        "    final_miny = float(miny[idxs].min())\n",
        "    final_maxy = float(maxy[idxs].max())\n",
        "\n",
        "    if debug: _t_back0 = perf_counter()\n",
        "\n",
        "    # convert final metric bbox -> lon/lat (single vectorized call)\n",
        "    lon_pair, lat_pair = t_xy2geog.transform(\n",
        "        np.array([final_minx, final_maxx], dtype=float),\n",
        "        np.array([final_miny, final_maxy], dtype=float)\n",
        "    )\n",
        "    minlon = ((float(lon_pair[0]) + 180.0) % 360.0) - 180.0\n",
        "    maxlon = ((float(lon_pair[1]) + 180.0) % 360.0) - 180.0\n",
        "    minlat = float(lat_pair[0])\n",
        "    maxlat = float(lat_pair[1])\n",
        "    if debug:\n",
        "        _t_back1 = perf_counter()\n",
        "        print(f\"[_find_blob_for_point] bbox metricâ†’geo transform: {(_t_back1 - _t_back0)*1000:.1f} ms\")\n",
        "        print(f\"[_find_blob_for_point] final merged bbox lat [{minlat:.4f}, {maxlat:.4f}] lon [{minlon:.4f}, {maxlon:.4f}]\")\n",
        "        print(f\"[_find_blob_for_point] merged labels: {sorted(list(included))}\")\n",
        "\n",
        "        '''\n",
        "        ################################################### COMMENT PLOTTING BLOCK OUT ON PRODUCTION RUNS #######################################################\n",
        "        #                                                             (VERY SLOW OTHERWISE)\n",
        "\n",
        "\n",
        "        # plotting block unchanged except we already know the host sweep\n",
        "        try:\n",
        "            from pyart.graph import RadarDisplay\n",
        "            import matplotlib.pyplot as plt\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "            ax0, ax1 = axes\n",
        "            try:\n",
        "                display = RadarDisplay(comp_scan)\n",
        "                sweep = int(getattr(comp_scan, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "                display.plot(class_field, sweep=sweep, ax=ax0, title=f\"Reflectivity (sweep {sweep})\")\n",
        "            except Exception as e:\n",
        "                if debug:\n",
        "                    print(f\"[_find_blob_for_point] RadarDisplay plotting failed: {e}\")\n",
        "                ax0.text(0.5, 0.5, \"RadarDisplay failed\", ha='center')\n",
        "                ax0.set_title(\"Reflectivity (not available)\")\n",
        "\n",
        "            GX, GY = np.meshgrid(grid_x, grid_y)\n",
        "            lon_grid, lat_grid = t_xy2geog.transform(GX, GY)\n",
        "            try:\n",
        "                radar_lon = float(comp_scan.longitude['data']) if np.isscalar(comp_scan.longitude['data']) else float(comp_scan.longitude['data'][0])\n",
        "            except Exception:\n",
        "                radar_lon = np.nanmedian(comp_scan.gate_longitude['data'])\n",
        "\n",
        "            # Use the *same* normalization scheme as everywhere else:\n",
        "            lon_grid = _normalize_lons_to_center(lon_grid, radar_lon)\n",
        "            ax1.pcolormesh(lon_grid, lat_grid, grid_mask.astype(int),\n",
        "                           shading='auto', cmap='viridis', alpha=0.5)\n",
        "\n",
        "            # plot every candidate bbox (light gray)\n",
        "            # vectorized draw: still loop to draw rectangles\n",
        "            for i, lab in enumerate(valid_labels):\n",
        "                lon_min_c, lat_min_c = t_xy2geog.transform(minx[i], miny[i])\n",
        "                lon_max_c, lat_max_c = t_xy2geog.transform(maxx[i], maxy[i])\n",
        "                lon_min_c = ((lon_min_c + 180.0) % 360.0) - 180.0\n",
        "                lon_max_c = ((lon_max_c + 180.0) % 360.0) - 180.0\n",
        "                w = lon_max_c - lon_min_c\n",
        "                if w < 0: w += 360.0\n",
        "                h = lat_max_c - lat_min_c\n",
        "                rect = Rectangle((lon_min_c, lat_min_c), w, h,\n",
        "                                 fill=False, linewidth=1.0, linestyle='-', edgecolor='lightgray', alpha=0.9, zorder=2)\n",
        "                ax1.add_patch(rect)\n",
        "                ax1.text(centers[i]['clon'], centers[i]['clat'], str(int(lab)),\n",
        "                         fontsize=8, ha='center', va='center', zorder=11)\n",
        "\n",
        "            # highlight included bboxes + final bbox\n",
        "            for l in included:\n",
        "                i = lab_to_idx[int(l)]\n",
        "                lon_min_c, lat_min_c = t_xy2geog.transform(minx[i], miny[i])\n",
        "                lon_max_c, lat_max_c = t_xy2geog.transform(maxx[i], maxy[i])\n",
        "                lon_min_c = ((lon_min_c + 180.0) % 360.0) - 180.0\n",
        "                lon_max_c = ((lon_max_c + 180.0) % 360.0) - 180.0\n",
        "                w = lon_max_c - lon_min_c\n",
        "                if w < 0: w += 360.0\n",
        "                h = lat_max_c - lat_min_c\n",
        "                rect = Rectangle((lon_min_c, lat_min_c), w, h,\n",
        "                                 fill=True, linewidth=1.5, linestyle='-',\n",
        "                                 edgecolor='blue', facecolor='blue', alpha=0.15, zorder=5)\n",
        "                ax1.add_patch(rect)\n",
        "\n",
        "            # final bbox\n",
        "            width = maxlon - minlon\n",
        "            if width < 0: width += 360.0\n",
        "            height = maxlat - minlat\n",
        "            rect_final = Rectangle((minlon, minlat), width, height,\n",
        "                                   fill=False, linewidth=3.0, linestyle='--', edgecolor='red', zorder=12)\n",
        "            ax1.add_patch(rect_final)\n",
        "\n",
        "            # track center\n",
        "            plot_center_lon = ((center_lon + 180.0) % 360.0) - 180.0\n",
        "            ax1.scatter(plot_center_lon, center_lat, s=120, marker='X', edgecolor='k',\n",
        "                        facecolor='yellow', zorder=15, label='track center')\n",
        "\n",
        "            ax1.set_xlabel(\"Longitude\"); ax1.set_ylabel(\"Latitude\")\n",
        "            ax1.set_title(\"Gridded mask + candidate cell bboxes (gray), included (blue), final bbox (red)\")\n",
        "            ax1.set_aspect('equal', adjustable='box')\n",
        "            ax1.legend(loc='upper right', fontsize='small')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save to directory\n",
        "            stub = plot_stub or \"find_blob\"\n",
        "            _save_plot(fig=fig,\n",
        "                       plot_dir=plot_dir,\n",
        "                       func_name=\"_find_blob_for_point\",\n",
        "                       stub=stub,\n",
        "                       debug=debug,\n",
        "                       )\n",
        "\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[_find_blob_for_point] debug plotting (reflectivity + mask) failed: {e}\")\n",
        "\n",
        "\n",
        "        ################################################################### END BLOCK ###################################################################\n",
        "        '''\n",
        "\n",
        "    if debug:\n",
        "        _t_total = perf_counter()\n",
        "        print(f\"[_find_blob_for_point] TOTAL: {(_t_total - _t0):.3f}s\")\n",
        "    return minlat, maxlat, minlon, maxlon, centers, info\n",
        "\n",
        "\n",
        "\n",
        "def _fast_metric_inside_mask(comp, minlat, maxlat, minlon, maxlon, buffer_km):\n",
        "    # 1) local projection centered at radar\n",
        "    try:\n",
        "        rlat = float(comp.latitude['data']) if np.isscalar(comp.latitude['data']) else float(comp.latitude['data'][0])\n",
        "        rlon = float(comp.longitude['data']) if np.isscalar(comp.longitude['data']) else float(comp.longitude['data'][0])\n",
        "    except Exception:\n",
        "        # worst case: fall back to Py-ARTâ€™s georef once (still way rarer than per-crop)\n",
        "        comp.init_gate_longitude_latitude()\n",
        "        rlat = float(np.nanmean(comp.gate_latitude['data']))\n",
        "        rlon = float(np.nanmean(comp.gate_longitude['data']))\n",
        "    from math import radians, sin, cos, isfinite\n",
        "    t_g2x, _ = _cached_aeqd_transformers(round(rlat, 6), round(((rlon + 180) % 360) - 180, 6))\n",
        "\n",
        "    # 2) bbox -> metric (use all 4 corners for safety, buffer already folded into min/max)\n",
        "    xs, ys = t_g2x.transform(\n",
        "        [minlon, minlon, maxlon, maxlon],\n",
        "        [minlat, maxlat, minlat, maxlat]\n",
        "    )\n",
        "    minx = float(np.min(xs)); maxx = float(np.max(xs))\n",
        "    miny = float(np.min(ys)); maxy = float(np.max(ys))\n",
        "\n",
        "    # 3) geometry vectors\n",
        "    sweep = int(getattr(comp, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "    s = int(comp.sweep_start_ray_index['data'][sweep])\n",
        "    e = int(comp.sweep_end_ray_index['data'][sweep])\n",
        "\n",
        "    az = np.asarray(comp.azimuth['data'][s:e], dtype=float)\n",
        "    el = np.asarray(comp.elevation['data'][s:e], dtype=float)\n",
        "    rng = np.asarray(comp.range['data'], dtype=float)\n",
        "\n",
        "    # ground-range per gate (outer product) via cos(elev)\n",
        "    elc = np.cos(np.deg2rad(el)).astype(np.float32, copy=False)   # (rays,)\n",
        "    rg  = (elc[:, None] * rng[None, :]).astype(np.float32, copy=False)  # (rays, gates)\n",
        "\n",
        "    # 4) per-ray range bounds from rectangle\n",
        "    a  = np.deg2rad(az).astype(np.float32, copy=False)\n",
        "    sa = np.sin(a); ca = np.cos(a)\n",
        "    eps = 1e-6\n",
        "\n",
        "    def bounds_1d(vmin, vmax, denom):\n",
        "        # returns (lo, hi) per ray for r given vmin <= r*denom <= vmax\n",
        "        lo = np.full(denom.shape, -np.inf, dtype=np.float32)\n",
        "        hi = np.full(denom.shape,  np.inf, dtype=np.float32)\n",
        "        mask = np.abs(denom) >= eps\n",
        "        q1 = (vmin / denom[mask]).astype(np.float32, copy=False)\n",
        "        q2 = (vmax / denom[mask]).astype(np.float32, copy=False)\n",
        "        lo[mask] = np.minimum(q1, q2)\n",
        "        hi[mask] = np.maximum(q1, q2)\n",
        "        # if |denom|<eps, line is x=0 or y=0; if 0 in [vmin,vmax] â†’ no constraint; else empty\n",
        "        pass_mask = (~mask) & (vmin <= 0.0) & (0.0 <= vmax)\n",
        "        # else keep lo=-inf, hi=+inf (no solution will be filtered later by intersection)\n",
        "        # For the impossible case (0 not in [vmin,vmax]) we leave lo=-inf,hi=+inf\n",
        "        return lo, hi\n",
        "\n",
        "    rx_lo, rx_hi = bounds_1d(minx, maxx, sa)\n",
        "    ry_lo, ry_hi = bounds_1d(miny, maxy, ca)\n",
        "\n",
        "    # intersect and clamp to r>=0\n",
        "    r_lo = np.maximum(0.0, np.maximum(rx_lo, ry_lo))   # (rays,)\n",
        "    r_hi = np.minimum(     rx_hi, ry_hi)               # (rays,)\n",
        "\n",
        "    # 5) final inside mask\n",
        "    inside_mask = (rg >= r_lo[:, None]) & (rg <= r_hi[:, None])\n",
        "    return inside_mask\n",
        "\n",
        "\n",
        "def _crop_comp_scan_to_bbox(comp_scan, minlat, maxlat, minlon, maxlon,\n",
        "                            buffer_km=5.0, debug=False, inplace=True,\n",
        "                            drop_gate_coords=True, prefer_nan_fill=True):\n",
        "    \"\"\"\n",
        "    In-place crop via masking (or NaN-fill) outside bbox+buffer, optimized for speed & bandwidth:\n",
        "      - builds the inside mask in *metric space* (no gate lon/lat fetch),\n",
        "      - early exit if the mask keeps everything,\n",
        "      - applies masks per-sweep slice if fields span all sweeps,\n",
        "      - reuse a single shared mask across fields,\n",
        "      - avoid array-wide dtype casts/writes for non-floating fields.\n",
        "    \"\"\"\n",
        "    _t0 = perf_counter()\n",
        "\n",
        "    # Copy vs in-place\n",
        "    _t_copy0 = perf_counter()\n",
        "    comp = comp_scan if inplace else copy.deepcopy(comp_scan)\n",
        "    _t_copy1 = perf_counter()\n",
        "\n",
        "    # Bbox + buffer (deg)\n",
        "    _t_bbox0 = perf_counter()\n",
        "    deg_per_lat_km = 1.0 / 110.574\n",
        "    midlat = (minlat + maxlat) * 0.5\n",
        "    deg_per_lon_km = 1.0 / (111.320 * np.cos(np.deg2rad(midlat))) if np.isfinite(midlat) else (1.0 / 111.320)\n",
        "    lat_min = minlat - buffer_km * deg_per_lat_km\n",
        "    lat_max = maxlat + buffer_km * deg_per_lat_km\n",
        "    lon_min = minlon - buffer_km * deg_per_lon_km\n",
        "    lon_max = maxlon + buffer_km * deg_per_lon_km\n",
        "    _t_bbox1 = perf_counter()\n",
        "\n",
        "    # Build inside mask in metric space (no gate lon/lat)\n",
        "    _t_mask0 = perf_counter()\n",
        "    inside_mask = _fast_metric_inside_mask(comp, lat_min, lat_max, lon_min, lon_max, buffer_km)\n",
        "    _t_mask1 = perf_counter()\n",
        "\n",
        "    # Compute sweep slice used by the mask\n",
        "    sweep = int(getattr(comp, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "    s = int(comp.sweep_start_ray_index['data'][sweep])\n",
        "    e = int(comp.sweep_end_ray_index['data'][sweep])\n",
        "\n",
        "    # Fast no-op escape\n",
        "    _t_noop0 = perf_counter()\n",
        "    if isinstance(inside_mask, np.ndarray) and inside_mask.dtype == bool and inside_mask.all():\n",
        "        if debug:\n",
        "            try:\n",
        "                rays = e - s\n",
        "            except Exception:\n",
        "                rays = inside_mask.shape[0] if inside_mask.ndim == 2 else -1\n",
        "            try:\n",
        "                gates = int(np.asarray(comp.range['data']).size)\n",
        "            except Exception:\n",
        "                gates = inside_mask.shape[1] if inside_mask.ndim == 2 else -1\n",
        "            print(f\"[_crop_comp_scan_to_bbox] dims: rays={rays} gates={gates} fields={len(getattr(comp, 'fields', {}))} \"\n",
        "                  f\"inplace={bool(inplace)} prefer_nan_fill={bool(prefer_nan_fill)} drop_gate_coords={bool(drop_gate_coords)}\")\n",
        "            print(f\"    copy/deepcopy:        {(_t_copy1 - _t_copy0)*1000:.1f} ms\")\n",
        "            print(f\"    bbox math:            {(_t_bbox1 - _t_bbox0)*1000:.1f} ms\")\n",
        "            print(f\"    compute metric mask:  {(_t_mask1 - _t_mask0)*1000:.1f} ms\")\n",
        "            print(f\"    no-op check:          {(perf_counter() - _t_noop0)*1000:.1f} ms\")\n",
        "            print(f\"    TOTAL:                {(perf_counter() - _t0   )*1000:.1f} ms\")\n",
        "            print(\"[_crop_comp_scan_to_bbox] bbox covers full radar -> no-op (metric mask all True)\")\n",
        "        return comp\n",
        "    _t_noop1 = perf_counter()\n",
        "\n",
        "    # Turn insideâ†’outside (shared mask)\n",
        "    _t_inv0 = perf_counter()\n",
        "    np.logical_not(inside_mask, out=inside_mask)\n",
        "    mask_outside = inside_mask\n",
        "    _t_inv1 = perf_counter()\n",
        "\n",
        "    gate_shape = mask_outside.shape  # = (e-s, ngates)\n",
        "    shared_bcast = {}\n",
        "\n",
        "    # Per-field timing accumulators\n",
        "    _t_fields0 = perf_counter()\n",
        "    n_fields = 0; n_masked = 0; n_float = 0; n_intbool = 0\n",
        "    t_masked_merge = 0.0; t_float_fill = 0.0; t_int_wrap = 0.0; t_bcast_build = 0.0\n",
        "\n",
        "    # Helper: apply mask to a MaskedArray that spans *all* rays (slice [s:e,:])\n",
        "    def _merge_mask_slice(arr_ma, mask_slice):\n",
        "        # Prepare a full-size mask (only slice gets OR-ed with mask_slice)\n",
        "        if (arr_ma.mask is np.ma.nomask) or (arr_ma.mask is False):\n",
        "            full = np.zeros(arr_ma.shape, dtype=bool)\n",
        "        elif isinstance(arr_ma.mask, np.ndarray) and arr_ma.mask.shape == arr_ma.shape:\n",
        "            full = arr_ma.mask\n",
        "        else:\n",
        "            try:\n",
        "                full = np.array(arr_ma.mask, dtype=bool, copy=True)\n",
        "            except Exception:\n",
        "                full = np.zeros(arr_ma.shape, dtype=bool)\n",
        "            if full.shape != arr_ma.shape:\n",
        "                full = np.zeros(arr_ma.shape, dtype=bool)\n",
        "        np.logical_or(full[s:e, :], mask_slice, out=full[s:e, :])\n",
        "        arr_ma.mask = full\n",
        "\n",
        "    for fname, fdict in comp.fields.items():\n",
        "        arr = fdict.get(\"data\", None)\n",
        "        if arr is None:\n",
        "            continue\n",
        "        n_fields += 1\n",
        "        _t_one0 = perf_counter()\n",
        "\n",
        "        # -------- MaskedArray fields --------\n",
        "        if isinstance(arr, np.ma.MaskedArray):\n",
        "            n_masked += 1\n",
        "            if arr.ndim == 2 and arr.shape == gate_shape:\n",
        "                # exact per-sweep array\n",
        "                if (arr.mask is np.ma.nomask) or (arr.mask is False):\n",
        "                    arr.mask = mask_outside\n",
        "                else:\n",
        "                    if isinstance(arr.mask, np.ndarray) and arr.mask.shape == gate_shape:\n",
        "                        np.logical_or(arr.mask, mask_outside, out=arr.mask)\n",
        "                    else:\n",
        "                        real = mask_outside.copy(order='C')\n",
        "                        np.logical_or(real, arr.mask, out=real)\n",
        "                        arr.mask = real\n",
        "                try:\n",
        "                    if arr.dtype != np.float32:\n",
        "                        arr._data = arr._data.astype(np.float32, copy=False)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                fdict[\"data\"] = arr\n",
        "\n",
        "            elif arr.ndim == 2 and arr.shape[1] == gate_shape[1] and e <= arr.shape[0]:\n",
        "                # full-radar array: apply only to [s:e, :]\n",
        "                _merge_mask_slice(arr, mask_outside)\n",
        "                try:\n",
        "                    if arr.dtype != np.float32:\n",
        "                        arr._data = arr._data.astype(np.float32, copy=False)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                fdict[\"data\"] = arr\n",
        "\n",
        "            else:\n",
        "                # other shapes -> try broadcast (may be rare)\n",
        "                if arr.shape not in shared_bcast:\n",
        "                    _tb0 = perf_counter()\n",
        "                    bm = np.broadcast_to(mask_outside, arr.shape).copy(order='C')\n",
        "                    _tb1 = perf_counter()\n",
        "                    shared_bcast[arr.shape] = bm\n",
        "                    t_bcast_build += (_tb1 - _tb0)\n",
        "                bm = shared_bcast[arr.shape]\n",
        "                if (arr.mask is np.ma.nomask) or (arr.mask is False):\n",
        "                    arr.mask = bm\n",
        "                else:\n",
        "                    if isinstance(arr.mask, np.ndarray) and arr.mask.shape == arr.shape:\n",
        "                        np.logical_or(arr.mask, bm, out=arr.mask)\n",
        "                    else:\n",
        "                        real = bm.copy(order='C')\n",
        "                        np.logical_or(real, arr.mask, out=real)\n",
        "                        arr.mask = real\n",
        "                try:\n",
        "                    if arr.dtype != np.float32:\n",
        "                        arr._data = arr._data.astype(np.float32, copy=False)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                fdict[\"data\"] = arr\n",
        "\n",
        "            t_masked_merge += (perf_counter() - _t_one0)\n",
        "            continue\n",
        "\n",
        "        # -------- Plain ndarray fields --------\n",
        "        if prefer_nan_fill and np.issubdtype(arr.dtype, np.floating):\n",
        "            n_float += 1\n",
        "            _tf0 = perf_counter()\n",
        "            if arr.dtype != np.float32:\n",
        "                arr = arr.astype(np.float32, copy=False)\n",
        "\n",
        "            if arr.ndim == 2 and arr.shape == gate_shape:\n",
        "                # per-sweep\n",
        "                np.copyto(arr, np.nan, where=mask_outside)\n",
        "            elif arr.ndim == 2 and arr.shape[1] == gate_shape[1] and e <= arr.shape[0]:\n",
        "                # full-radar: slice\n",
        "                np.copyto(arr[s:e, :], np.nan, where=mask_outside)\n",
        "            else:\n",
        "                # fallback broadcast (rare)\n",
        "                bm = shared_bcast.get(arr.shape)\n",
        "                if bm is None:\n",
        "                    _tb0 = perf_counter()\n",
        "                    bm = np.broadcast_to(mask_outside, arr.shape).copy(order='C')\n",
        "                    _tb1 = perf_counter()\n",
        "                    shared_bcast[arr.shape] = bm\n",
        "                    t_bcast_build += (_tb1 - _tb0)\n",
        "                np.copyto(arr, np.nan, where=bm)\n",
        "\n",
        "            fdict[\"data\"] = arr\n",
        "            t_float_fill += (perf_counter() - _tf0)\n",
        "        else:\n",
        "            n_intbool += 1\n",
        "            _ti0 = perf_counter()\n",
        "            if arr.ndim == 2 and arr.shape == gate_shape:\n",
        "                fdict[\"data\"] = _ma.MaskedArray(arr, mask=mask_outside, copy=False)\n",
        "            elif arr.ndim == 2 and arr.shape[1] == gate_shape[1] and e <= arr.shape[0]:\n",
        "                # full array -> build a full-size mask and fill only [s:e, :]\n",
        "                bm_full = np.zeros(arr.shape, dtype=bool)\n",
        "                bm_full[s:e, :] = mask_outside\n",
        "                fdict[\"data\"] = _ma.MaskedArray(arr, mask=bm_full, copy=False)\n",
        "            else:\n",
        "                bm = shared_bcast.get(arr.shape)\n",
        "                if bm is None:\n",
        "                    _tb0 = perf_counter()\n",
        "                    bm = np.broadcast_to(mask_outside, arr.shape).copy(order='C')\n",
        "                    _tb1 = perf_counter()\n",
        "                    shared_bcast[arr.shape] = bm\n",
        "                    t_bcast_build += (_tb1 - _tb0)\n",
        "                fdict[\"data\"] = _ma.MaskedArray(arr, mask=bm, copy=False)\n",
        "            t_int_wrap += (perf_counter() - _ti0)\n",
        "\n",
        "    _t_fields1 = perf_counter()\n",
        "\n",
        "    # Optional: drop large cached geo arrays if desired\n",
        "    _t_drop0 = perf_counter()\n",
        "    if drop_gate_coords:\n",
        "        for gate_key in (\"gate_longitude\", \"gate_latitude\", \"gate_altitude\"):\n",
        "            try: delattr(comp, gate_key)\n",
        "            except Exception: pass\n",
        "    _t_drop1 = perf_counter()\n",
        "\n",
        "    # Clear locals\n",
        "    _t_clean0 = perf_counter()\n",
        "    try: del mask_outside, inside_mask\n",
        "    except Exception: pass\n",
        "    _t_clean1 = perf_counter()\n",
        "\n",
        "    if debug:\n",
        "        rays = e - s\n",
        "        try:\n",
        "            gates = int(np.asarray(comp.range['data']).size)\n",
        "        except Exception:\n",
        "            gates = gate_shape[1] if isinstance(gate_shape, tuple) and len(gate_shape) >= 2 else -1\n",
        "\n",
        "        print(f\"[_crop_comp_scan_to_bbox] dims: rays={rays} gates={gates} fields={n_fields} \"\n",
        "              f\"inplace={bool(inplace)} prefer_nan_fill={bool(prefer_nan_fill)} drop_gate_coords={bool(drop_gate_coords)}\")\n",
        "        print(f\"    copy/deepcopy:        {(_t_copy1  - _t_copy0 )*1000:.1f} ms\")\n",
        "        print(f\"    bbox math:            {(_t_bbox1  - _t_bbox0 )*1000:.1f} ms\")\n",
        "        print(f\"    compute metric mask:  {(_t_mask1  - _t_mask0 )*1000:.1f} ms\")\n",
        "        print(f\"    no-op check:          {(_t_noop1  - _t_noop0 )*1000:.1f} ms\")\n",
        "        print(f\"    invert to outside:    {(_t_inv1   - _t_inv0  )*1000:.1f} ms\")\n",
        "        print(f\"    per-field apply:      {(_t_fields1- _t_fields0)*1000:.1f} ms\")\n",
        "        print(f\"        masked merge (n={n_masked}):   {t_masked_merge*1000:.1f} ms\")\n",
        "        print(f\"        float NaN fill (n={n_float}):  {t_float_fill*1000:.1f} ms\")\n",
        "        print(f\"        int/bool wrap (n={n_intbool}): {t_int_wrap*1000:.1f} ms\")\n",
        "        print(f\"        broadcast builds:               {t_bcast_build*1000:.1f} ms\")\n",
        "        print(f\"    drop gate coords:     {(_t_drop1  - _t_drop0 )*1000:.1f} ms\")\n",
        "        print(f\"    cleanup locals:       {(_t_clean1 - _t_clean0)*1000:.1f} ms\")\n",
        "        print(f\"    TOTAL:                {(perf_counter() - _t0   )*1000:.1f} ms\")\n",
        "        print(f\"[_crop_comp_scan_to_bbox] cropped to {minlat:.4f}-{maxlat:.4f}, {minlon:.4f}-{maxlon:.4f} (buffer_km={buffer_km})\")\n",
        "\n",
        "    return comp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################################### MAIN PIPELINE ######################################################################\n",
        "\n",
        "\n",
        "\n",
        "def build_bboxes_for_linked_df(linked_df: pd.DataFrame,\n",
        "                               class_field='reflectivity',\n",
        "                               threshold=20,\n",
        "                               min_size=500,\n",
        "                               pad_km=5.0,\n",
        "                               grid_res_m=1000.0,\n",
        "                               buffer_km=5.0,\n",
        "                               include_nearby_km=8.0,\n",
        "                               debug=False,\n",
        "                               debug_plot_dir: str | None = None,\n",
        "                               debug_plot_limit: int = 2) -> pd.DataFrame:\n",
        "\n",
        "    \"\"\"\n",
        "    High-level handler (minimal multi-product changes):\n",
        "      - Detect product keys in linked_df by columns that end with '_scan' (e.g. 'dhr_scan', 'dpa_scan').\n",
        "      - Choose a reference product key for bbox computation: prefer 'dhr' if present, otherwise the first product key,\n",
        "        falling back to legacy 'radar_scan' if present.\n",
        "      - Use the reference composite to run the exact same bounding logic as before.\n",
        "      - Crop EVERY product composite present in the row (for each <key>_scan) to the stationary bbox (loading from\n",
        "        <key>_cache_volume_path if needed) and store the cropped composite back in <key>_scan in the output rows.\n",
        "    \"\"\"\n",
        "    # set up debug plot dir if requested\n",
        "    if debug:\n",
        "        if debug_plot_dir is None:\n",
        "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            debug_plot_dir = os.path.join(\"Logs\", \"plots\", f\"bbox_{ts}_pid{os.getpid()}\")\n",
        "        os.makedirs(debug_plot_dir, exist_ok=True)\n",
        "        if debug:\n",
        "            print(f\"[build_bboxes] debug_plot_dir={debug_plot_dir}\")\n",
        "\n",
        "    # detect product keys (columns like '<key>_scan')\n",
        "    product_keys = sorted({c[:-5] for c in linked_df.columns if c.endswith('_scan')})\n",
        "    # legacy fallback: if no product-specific scan columns, but legacy 'radar_scan' exists, treat it as 'radar' key\n",
        "    legacy_mode = False\n",
        "    if not product_keys:\n",
        "        if 'radar_scan' in linked_df.columns:\n",
        "            product_keys = ['radar']\n",
        "            legacy_mode = True\n",
        "        else:\n",
        "            raise KeyError(\"linked_df must contain at least one '<key>_scan' column (e.g. 'dhr_scan') or 'radar_scan'\")\n",
        "\n",
        "    # choose reference key: prefer dhr (reflectivity) if present\n",
        "    reference_key = 'dhr' if 'dhr' in product_keys else product_keys[0]\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[build_bboxes] product_keys={product_keys}, reference_key={reference_key}, legacy_mode={legacy_mode}\")\n",
        "\n",
        "    collected = []\n",
        "    pseudocomps_by_idx = {}\n",
        "\n",
        "    for idx, row in linked_df.iterrows():\n",
        "        try:\n",
        "            # Load the reference composite (try in-memory first, then cache)\n",
        "            ref_scan_col = (f\"{reference_key}_scan\" if not legacy_mode else 'radar_scan')\n",
        "            ref_cache_col = (f\"{reference_key}_cache_volume_path\" if not legacy_mode else 'cache_member_name')\n",
        "\n",
        "            comp_scan = row.get(ref_scan_col, None)\n",
        "\n",
        "            # Move on to the next row if comp_scan fails, for whatever reason\n",
        "            if comp_scan is None:\n",
        "                if debug:\n",
        "                    print(f\"[build_bboxes] idx {idx}: no reference scan ({ref_scan_col}) -> skipping\")\n",
        "                continue\n",
        "\n",
        "            # ### NEW: build a pseudo-composite reflectivity for Level II (or pass-through for Level III)\n",
        "            try:\n",
        "                pseudo = _make_reflectivity_pseudocomposite(\n",
        "                    comp_scan,\n",
        "                    field_name=class_field,\n",
        "                    out_field=class_field,     # keep same so downstream code uses class_field unchanged\n",
        "                    max_tilts=3,\n",
        "                    chunk_size=2048,\n",
        "                    debug=debug,\n",
        "                    plot_dir=(debug_plot_dir if debug else None),\n",
        "                    plot_stub=(f\"row_{idx}_pseudo\")\n",
        "                )\n",
        "                pseudocomps_by_idx[idx] = pseudo\n",
        "                comp_scan = pseudo      # use pseudo for bbox finding\n",
        "            except Exception as e:\n",
        "                if debug:\n",
        "                    print(f\"[build_bboxes] idx {idx}: pseudo-composite failed ({e}); falling back to given scan\")\n",
        "                pseudocomps_by_idx[idx] = None\n",
        "                comp_scan = comp_scan   # fallback (e.g., Level III single-tilt/composite)\n",
        "\n",
        "            # obtain center lat/lon\n",
        "            center_lat = float(row['latitude'])\n",
        "            center_lon = float(row['longitude'])\n",
        "            center_lon = ((center_lon + 180.0) % 360.0) - 180.0\n",
        "\n",
        "            if debug:\n",
        "                print(f\"[build_bboxes] processing idx {idx} center=({center_lat:.4f},{center_lon:.4f}) using ref='{reference_key}'\")\n",
        "\n",
        "            # find blob bbox using the reference composite (identical call as before)\n",
        "            minlat, maxlat, minlon, maxlon, centers, info = _find_blob_for_point(\n",
        "                comp_scan,\n",
        "                center_lat, center_lon,\n",
        "                class_field=class_field,\n",
        "                threshold=threshold,\n",
        "                min_size=min_size,\n",
        "                pad_m=pad_km * 1000.0,\n",
        "                grid_res_m=grid_res_m,\n",
        "                include_nearby_km=include_nearby_km,\n",
        "                debug=debug,\n",
        "                plot_dir=(debug_plot_dir if debug else None),\n",
        "                plot_stub=(f\"row_{idx}_find_blob\")\n",
        "            )\n",
        "\n",
        "\n",
        "            if minlat is None:\n",
        "                if debug:\n",
        "                    print(f\"[build_bboxes] idx {idx}: _find_blob_for_point returned None -> skipping\")\n",
        "                continue\n",
        "\n",
        "            # NEW: per-scan bbox print\n",
        "            if debug:\n",
        "                print(f\"[build_bboxes] idx {idx}: per-scan bbox \"\n",
        "                      f\"lat [{minlat:.4f}, {maxlat:.4f}] lon [{minlon:.4f}, {maxlon:.4f}]\")\n",
        "\n",
        "            t_geog2xy = info['t_geog2xy']\n",
        "\n",
        "            # convert lon/lat bbox to metric bbox\n",
        "            minx, miny = t_geog2xy.transform(minlon, minlat)\n",
        "            maxx, maxy = t_geog2xy.transform(maxlon, maxlat)\n",
        "\n",
        "            metric_minx, metric_maxx = (min(minx, maxx), max(minx, maxx))\n",
        "            metric_miny, metric_maxy = (min(miny, maxy), max(miny, maxy))\n",
        "\n",
        "            collected.append({\n",
        "                'idx': idx, 'row': row, 'comp_ref': comp_scan,\n",
        "                'minlat': minlat, 'maxlat': maxlat, 'minlon': minlon, 'maxlon': maxlon,\n",
        "                'minx': metric_minx, 'maxx': metric_maxx, 'miny': metric_miny, 'maxy': metric_maxy,\n",
        "                'centers': centers\n",
        "            })\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[build_bboxes] error idx {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if len(collected) == 0:\n",
        "        if debug:\n",
        "            print(\"[build_bboxes] no valid rows after scanning\")\n",
        "        cols = list(linked_df.columns) + ['min_lat', 'max_lat', 'min_lon', 'max_lon']\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    # compute widths/heights in meters (from the metric boxes we computed)\n",
        "    widths = np.array([c['maxx'] - c['minx'] for c in collected], dtype=float)\n",
        "    heights = np.array([c['maxy'] - c['miny'] for c in collected], dtype=float)\n",
        "\n",
        "    avg_width_m, avg_height_m, weights = _compute_weighted_dimensions(widths, heights, debug=debug)\n",
        "\n",
        "\n",
        "    # fit averaged box to first and last reference scans\n",
        "    search_km = 20.0\n",
        "    step_km = max(1.0, (grid_res_m / 1000.0) / 2.0)\n",
        "\n",
        "    first_box = (collected[0]['minlat'], collected[0]['maxlat'],\n",
        "                collected[0]['minlon'], collected[0]['maxlon'])\n",
        "    last_box  = (collected[-1]['minlat'], collected[-1]['maxlat'],\n",
        "                collected[-1]['minlon'], collected[-1]['maxlon'])\n",
        "\n",
        "    first_center_hint = _bbox_center(*first_box)\n",
        "    last_center_hint  = _bbox_center(*last_box)\n",
        "\n",
        "    first  = collected[0]['comp_ref']\n",
        "    last   = collected[-1]['comp_ref']\n",
        "\n",
        "\n",
        "    first_fit = _fit_box_to_scan_max_overlap(first, avg_width_m, avg_height_m,\n",
        "                                             class_field=class_field, threshold=threshold,\n",
        "                                             search_km=search_km, step_km=step_km, center_hint=first_center_hint, debug=debug)\n",
        "    last_fit  = _fit_box_to_scan_max_overlap(last, avg_width_m, avg_height_m,\n",
        "                                             class_field=class_field, threshold=threshold,\n",
        "                                             search_km=search_km, step_km=step_km, center_hint=last_center_hint, debug=debug)\n",
        "\n",
        "    # first/last fit prints\n",
        "    if debug:\n",
        "        if first_fit is not None and last_fit is not None:\n",
        "            fminlat, fmaxlat, fminlon, fmaxlon = first_fit\n",
        "            lminlat, lmaxlat, lminlon, lmaxlon = last_fit\n",
        "            print(f\"[build_bboxes] first_fit lat [{fminlat:.4f}, {fmaxlat:.4f}] \"\n",
        "                  f\"lon [{fminlon:.4f}, {fmaxlon:.4f}]\")\n",
        "            print(f\"[build_bboxes] last_fit  lat [{lminlat:.4f}, {lmaxlat:.4f}] \"\n",
        "                  f\"lon [{lminlon:.4f}, {lmaxlon:.4f}]\")\n",
        "        else:\n",
        "            print(\"[build_bboxes] first/last fit is None (will fallback to union of per-scan bboxes)\")\n",
        "\n",
        "\n",
        "    # fallback -> union of per-row lon/lat extents if fit fails\n",
        "    if (first_fit is None) or (last_fit is None):\n",
        "        if debug:\n",
        "            print(\"[build_bboxes] fit failed for first/last; falling back to union of per-row lat/lon extents\")\n",
        "        all_minlat = min(c['minlat'] for c in collected)\n",
        "        all_maxlat = max(c['maxlat'] for c in collected)\n",
        "        all_minlon = min(c['minlon'] for c in collected)\n",
        "        all_maxlon = max(c['maxlon'] for c in collected)\n",
        "        final_minlat, final_maxlat, final_minlon, final_maxlon = all_minlat, all_maxlat, all_minlon, all_maxlon\n",
        "    else:\n",
        "        fminlat, fmaxlat, fminlon, fmaxlon = first_fit\n",
        "        lminlat, lmaxlat, lminlon, lmaxlon = last_fit\n",
        "        final_minlat = min(fminlat, lminlat)\n",
        "        final_maxlat = max(fmaxlat, lmaxlat)\n",
        "        final_minlon = min(fminlon, lminlon)\n",
        "        final_maxlon = max(fmaxlon, lmaxlon)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[build_bboxes] stationary bbox lat [{final_minlat:.4f}, {final_maxlat:.4f}] lon [{final_minlon:.4f}, {final_maxlon:.4f}]\")\n",
        "\n",
        "    # crop every scan (for every product key) to the stationary bbox (apply buffer_km) and collect output rows\n",
        "    out_rows = []\n",
        "    for c in collected:\n",
        "        try:\n",
        "            out = c['row'].to_dict()\n",
        "            out['min_lat'] = final_minlat\n",
        "            out['max_lat'] = final_maxlat\n",
        "            out['min_lon'] = final_minlon\n",
        "            out['max_lon'] = final_maxlon\n",
        "\n",
        "            # For each product key present in the original linked_df, crop its comp (loading from cache if necessary)\n",
        "            for key in product_keys:\n",
        "                scan_col = (f\"{key}_scan\" if not (legacy_mode and key == 'radar') else 'radar_scan')\n",
        "                cache_col = (f\"{key}_cache_volume_path\" if not (legacy_mode and key == 'radar') else 'cache_member_name')\n",
        "\n",
        "                comp = out.get(scan_col, None)\n",
        "                if comp is None:\n",
        "                    # attempt to load from cache\n",
        "                    pkl_path = out.get(cache_col, None)\n",
        "                    if pkl_path:\n",
        "                        try:\n",
        "                            comp = _load_composite_pickle(pkl_path, debug=debug)\n",
        "                            if debug:\n",
        "                                print(f\"[build_bboxes] idx {c['idx']}: loaded comp for key={key} from cache {pkl_path}\")\n",
        "                        except Exception:\n",
        "                            comp = None\n",
        "\n",
        "                if comp is None:\n",
        "                    # no composite available for this product on this row -> keep column but set to None\n",
        "                    out[scan_col] = None\n",
        "                else:\n",
        "                    try:\n",
        "                        cropped = _crop_comp_scan_to_bbox(comp, final_minlat, final_maxlat, final_minlon, final_maxlon,\n",
        "                                                         buffer_km=buffer_km, debug=debug, inplace=True)\n",
        "                        out[scan_col] = cropped\n",
        "                    except Exception as e:\n",
        "                        if debug:\n",
        "                            print(f\"[build_bboxes] cropping fail for idx {c['idx']}, key={key}: {e}\")\n",
        "                        out[scan_col] = None\n",
        "\n",
        "            # ### NEW: also crop and attach the pseudo-composite (built per-row from reference scan)\n",
        "            pseudo_comp = pseudocomps_by_idx.get(c['idx'], None)\n",
        "            if pseudo_comp is not None:\n",
        "                try:\n",
        "                    pseudo_cropped = _crop_comp_scan_to_bbox(\n",
        "                        pseudo_comp, final_minlat, final_maxlat, final_minlon, final_maxlon,\n",
        "                        buffer_km=buffer_km, debug=debug, inplace=True\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    if debug:\n",
        "                        print(f\"[build_bboxes] pseudo-crop fail idx {c['idx']}: {e}\")\n",
        "                    pseudo_cropped = None\n",
        "            else:\n",
        "                pseudo_cropped = None\n",
        "            out['reflectivity_composite_scan'] = pseudo_cropped  # <- NEW COLUMN\n",
        "            out_rows.append(out)\n",
        "\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[build_bboxes] cropping fail idx {c['idx']}: {e}\")\n",
        "\n",
        "    if len(out_rows) == 0:\n",
        "        if debug:\n",
        "            print(\"[build_bboxes] no rows after final cropping\")\n",
        "        cols = list(linked_df.columns) + ['min_lat', 'max_lat', 'min_lon', 'max_lon']\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    out_df = pd.DataFrame(out_rows)\n",
        "\n",
        "    # final debug plotting: save up to N samples\n",
        "    if debug and debug_plot_dir:\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            from pyart.graph import RadarDisplay\n",
        "\n",
        "            sample_is = list(range(len(collected)))\n",
        "            if len(sample_is) > debug_plot_limit:\n",
        "                # keep first, mid, last and a few evenly spaced in-between\n",
        "                first, last = 0, len(collected)-1\n",
        "                mids = np.linspace(1, last-1, num=min(debug_plot_limit-2, max(0, len(collected)-2)), dtype=int)\n",
        "                sample_is = [first] + sorted(set(mids.tolist())) + [last]\n",
        "\n",
        "            for si in sample_is:\n",
        "                p = collected[si]\n",
        "                comp_scan = p['comp_ref']\n",
        "\n",
        "                fig, axes = _plt.subplots(1, 2, figsize=(14, 6))\n",
        "                ax0, ax1 = axes\n",
        "                try:\n",
        "                    display = RadarDisplay(comp_scan)\n",
        "                    sweep = int(getattr(comp_scan, \"metadata\", {}).get(\"pseudo_host_sweep\", 0))\n",
        "                    display.plot(class_field, sweep=sweep, ax=ax0, title=f\"Reflectivity (sample {si})\")\n",
        "                except Exception:\n",
        "                    ax0.text(0.5, 0.5, \"RadarDisplay failed\", ha='center')\n",
        "                    ax0.set_title(\"Reflectivity (not available)\")\n",
        "\n",
        "                info = _compute_metric_grid_and_labels(comp_scan, class_field=class_field,\n",
        "                                                      threshold=threshold, pad_m=pad_km*1000.0,\n",
        "                                                      grid_res_m=grid_res_m, debug=False)\n",
        "                GX, GY = np.meshgrid(info['grid_x'], info['grid_y'])\n",
        "                lon_grid, lat_grid = info['t_xy2geog'].transform(GX, GY)\n",
        "                lon_grid = ((lon_grid + 180.0) % 360.0) - 180.0\n",
        "\n",
        "                ax1.pcolormesh(lon_grid, lat_grid, info['grid_mask'].astype(int),\n",
        "                              shading='auto', cmap='viridis', alpha=0.5)\n",
        "\n",
        "                # per-row boxes (light gray)\n",
        "                for q in collected:\n",
        "                    lon_min_c, lat_min_c = q['minlon'], q['minlat']\n",
        "                    lon_max_c, lat_max_c = q['maxlon'], q['maxlat']\n",
        "                    w = lon_max_c - lon_min_c\n",
        "                    if w < 0: w += 360.0\n",
        "                    h = lat_max_c - lat_min_c\n",
        "                    rect = Rectangle((lon_min_c, lat_min_c), w, h,\n",
        "                                    fill=False, linewidth=1.0, linestyle='-',\n",
        "                                    edgecolor='lightgray', alpha=0.8, zorder=2)\n",
        "                    ax1.add_patch(rect)\n",
        "\n",
        "                # stationary bbox (red dashed)\n",
        "                width = final_maxlon - final_minlon\n",
        "                if width < 0: width += 360.0\n",
        "                height = final_maxlat - final_minlat\n",
        "                rect_final = Rectangle((final_minlon, final_minlat), width, height,\n",
        "                                      fill=False, linewidth=3.0, linestyle='--',\n",
        "                                      edgecolor='red', zorder=12)\n",
        "                ax1.add_patch(rect_final)\n",
        "\n",
        "                ax1.set_xlabel(\"Longitude\"); ax1.set_ylabel(\"Latitude\")\n",
        "                ax1.set_title(f\"Stationary bbox overlay (sample {si})\")\n",
        "                ax1.set_aspect('equal', adjustable='box')\n",
        "                _plt.tight_layout()\n",
        "\n",
        "                # SAVE into subdir named after this function\n",
        "                stub = f\"summary_sample_{si}\"\n",
        "                _save_plot(fig, debug_plot_dir, \"build_bboxes_for_linked_df\", stub, debug)\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[build_bboxes] final debug plotting failed: {e}\")\n",
        "\n",
        "    # return DataFrame with bounding box columns\n",
        "    cols = list(linked_df.columns) + ['min_lat', 'max_lat', 'min_lon', 'max_lon', 'reflectivity_composite_scan']\n",
        "    cols = [c for c in cols if c in out_df.columns]\n",
        "    out_df = out_df[cols]\n",
        "    if debug:\n",
        "        print(f\"[build_bboxes] returning out_df shape: {out_df.shape}\")\n",
        "        print(out_df.head(50))\n",
        "\n",
        "    return out_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BKWlFuipS7r"
      },
      "source": [
        "### **Pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s2OSBFLxe5g"
      },
      "source": [
        "#### **Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrGV_P0YxiLR"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import io\n",
        "\n",
        "\n",
        "##########################################################################################################################################################\n",
        "\n",
        "\n",
        "class _QueueStream(io.TextIOBase):\n",
        "    def __init__(self, q, kind, sid):\n",
        "        self.q = q\n",
        "        self.kind = kind  # \"log\" | \"err\" | \"warn\"\n",
        "        self.sid = sid\n",
        "        self._buf = []\n",
        "\n",
        "    def write(self, s):\n",
        "        if not s:\n",
        "            return 0\n",
        "        if not isinstance(s, str):\n",
        "            s = str(s)\n",
        "        self._buf.append(s)\n",
        "        if \"\\n\" in s:\n",
        "            text = \"\".join(self._buf)\n",
        "            self._buf.clear()\n",
        "            for line in text.splitlines():\n",
        "                try:\n",
        "                    self.q.put_nowait((self.kind, f\"[child sid={self.sid}] {line}\"))\n",
        "                except Exception:\n",
        "                    # drop if queue is full to avoid deadlock\n",
        "                    pass\n",
        "        return len(s)\n",
        "\n",
        "    def flush(self):\n",
        "        if self._buf:\n",
        "            text = \"\".join(self._buf)\n",
        "            self._buf.clear()\n",
        "            try:\n",
        "                self.q.put_nowait((self.kind, f\"[child sid={self.sid}] {text}\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    def isatty(self): return False\n",
        "    @property\n",
        "    def encoding(self): return \"utf-8\"\n",
        "\n",
        "\n",
        "##########################################################################################################################################################\n",
        "\n",
        "\n",
        "def build_saved_storm_index(base_dir: str, debug: bool = False) -> dict[tuple[int, int], list[str]]:\n",
        "    \"\"\"\n",
        "    Fast one-pass index: {(year, sid) -> [storm_dir_paths]} for storms that already\n",
        "    have at least one .h5 in base_dir/{year}/{site}/storm_{sid}/.\n",
        "\n",
        "    Notes:\n",
        "      - Robust to extra/non-numeric year folders and non-storm dirs.\n",
        "      - Aggregates across sites (multiple paths per (year, sid)).\n",
        "    \"\"\"\n",
        "    import os, re\n",
        "\n",
        "    saved: dict[tuple[int, int], list[str]] = {}\n",
        "    try:\n",
        "        if not os.path.isdir(base_dir):\n",
        "            if debug:\n",
        "                print(f\"[build_saved_storm_index] base_dir '{base_dir}' does not exist yet.\")\n",
        "            return saved\n",
        "\n",
        "        with os.scandir(base_dir) as years:\n",
        "            for y in years:\n",
        "                if not y.is_dir():\n",
        "                    continue\n",
        "                # Parse year folder name (must start with digits)\n",
        "                ym = re.match(r\"^\\s*(\\d{4})\\b\", y.name)\n",
        "                if not ym:\n",
        "                    continue\n",
        "                try:\n",
        "                    year = int(ym.group(1))\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                with os.scandir(y.path) as sites:\n",
        "                    for s in sites:\n",
        "                        if not s.is_dir():\n",
        "                            continue\n",
        "                        with os.scandir(s.path) as storms:\n",
        "                            for st in storms:\n",
        "                                if not st.is_dir():\n",
        "                                    continue\n",
        "                                name = st.name\n",
        "                                if not name.startswith(\"storm_\"):\n",
        "                                    continue\n",
        "                                m = re.match(r\"^storm_(\\d+)\", name)\n",
        "                                if not m:\n",
        "                                    continue\n",
        "                                sid = int(m.group(1))\n",
        "\n",
        "                                # Consider it \"present\" only if there's at least one .h5 inside\n",
        "                                has_h5 = False\n",
        "                                try:\n",
        "                                    with os.scandir(st.path) as files:\n",
        "                                        for f in files:\n",
        "                                            if f.is_file() and f.name.endswith(\".h5\"):\n",
        "                                                has_h5 = True\n",
        "                                                break\n",
        "                                except Exception:\n",
        "                                    pass\n",
        "                                if not has_h5:\n",
        "                                    continue\n",
        "\n",
        "                                saved.setdefault((year, sid), []).append(st.path)\n",
        "\n",
        "        if debug:\n",
        "            n_keys = len(saved)\n",
        "            n_dirs = sum(len(v) for v in saved.values())\n",
        "            print(f\"[build_saved_storm_index] indexed {n_keys} (year,sid) keys across {n_dirs} folder(s).\")\n",
        "        return saved\n",
        "\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"[build_saved_storm_index] error while scanning '{base_dir}': {e}\")\n",
        "        return saved\n",
        "\n",
        "\n",
        "def should_skip_sid(year: int, sid: int, existing_index: dict[tuple[int, int], list[str]] | None,\n",
        "                    rewrite: bool, debug: bool = False) -> bool:\n",
        "    \"\"\"\n",
        "    Return True if we should skip processing this (year, sid):\n",
        "      - skip if rewrite=False and (year, sid) is already in existing_index\n",
        "      - process if rewrite=True (always)\n",
        "    \"\"\"\n",
        "    if rewrite:\n",
        "        if debug:\n",
        "            print(f\"[should_skip_sid] rewrite=True â†’ will process ({year}, SID {sid}) regardless of existing files.\")\n",
        "        return False\n",
        "\n",
        "    present = (existing_index is not None) and ((year, sid) in existing_index)\n",
        "    if debug:\n",
        "        if present:\n",
        "            paths = existing_index.get((year, sid), [])\n",
        "            path_hint = paths[0] if paths else \"<unknown>\"\n",
        "            print(f\"[should_skip_sid] ({year}, SID {sid}) already present at {path_hint} â†’ skipping.\")\n",
        "        else:\n",
        "            print(f\"[should_skip_sid] ({year}, SID {sid}) not found in index â†’ will process.\")\n",
        "    return present\n",
        "\n",
        "\n",
        "##########################################################################################################################################################################\n",
        "\n",
        "\n",
        "# Add date ranges to account for sites becoming inactive, etc.\n",
        "#                    start/end:    YYYY-MM-DD\n",
        "#                       |-- if no end date, assume active\n",
        "#                    position:     (lat, lon)\n",
        "radar_info = {\n",
        "    'KABR': {\n",
        "        'position': {'lat': 45.455833, 'lon': -98.413333},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KABX': {\n",
        "        'position': {'lat': 35.149722, 'lon': -106.82388},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KAKQ': {\n",
        "        'position': {'lat': 36.98405, 'lon': -77.007361},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KAMA': {\n",
        "        'position': {'lat': 35.233333, 'lon': -101.70927},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KAMX': {\n",
        "        'position': {'lat': 25.611083, 'lon': -80.412667},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KAPX': {\n",
        "        'position': {'lat': 44.90635, 'lon': -84.719533},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KARX': {\n",
        "        'position': {'lat': 43.822778, 'lon': -91.191111},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KATX': {\n",
        "        'position': {'lat': 48.194611, 'lon': -122.49569},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBBX': {\n",
        "        'position': {'lat': 39.495639, 'lon': -121.63161},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBGM': {\n",
        "        'position': {'lat': 42.199694, 'lon': -75.984722},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBHX': {\n",
        "        'position': {'lat': 40.498583, 'lon': -124.29216},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBIS': {\n",
        "        'position': {'lat': 46.770833, 'lon': -100.76055},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBLX': {\n",
        "        'position': {'lat': 45.853778, 'lon': -108.6068},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBMX': {\n",
        "        'position': {'lat': 33.172417, 'lon': -86.770167},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBOX': {\n",
        "        'position': {'lat': 41.955778, 'lon': -71.136861},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBRO': {\n",
        "        'position': {'lat': 25.916, 'lon': -97.418967},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBUF': {\n",
        "        'position': {'lat': 42.948789, 'lon': -78.736781},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KBYX': {\n",
        "        'position': {'lat': 24.5975, 'lon': -81.703167},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCAE': {\n",
        "        'position': {'lat': 33.948722, 'lon': -81.118278},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCBW': {\n",
        "        'position': {'lat': 46.03925, 'lon': -67.806431},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCBX': {\n",
        "        'position': {'lat': 43.490217, 'lon': -116.23603},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCCX': {\n",
        "        'position': {'lat': 40.923167, 'lon': -78.003722},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCLE': {\n",
        "        'position': {'lat': 41.413217, 'lon': -81.859867},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCLX': {\n",
        "        'position': {'lat': 32.655528, 'lon': -81.042194},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCRI': {\n",
        "        'position': {'lat': 35.238333, 'lon': -97.46},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCRP': {\n",
        "        'position': {'lat': 27.784017, 'lon': -97.51125},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCXX': {\n",
        "        'position': {'lat': 44.511, 'lon': -73.166431},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KCYS': {\n",
        "        'position': {'lat': 41.151919, 'lon': -104.80603},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDAX': {\n",
        "        'position': {'lat': 38.501111, 'lon': -121.67783},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDDC': {\n",
        "        'position': {'lat': 37.760833, 'lon': -99.968889},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDFX': {\n",
        "        'position': {'lat': 29.273139, 'lon': -100.28033},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDGX': {\n",
        "        'position': {'lat': 32.279944, 'lon': -89.984444},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDIX': {\n",
        "        'position': {'lat': 39.947089, 'lon': -74.410731},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDLH': {\n",
        "        'position': {'lat': 46.836944, 'lon': -92.209722},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDMX': {\n",
        "        'position': {'lat': 41.7312, 'lon': -93.722869},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDOX': {\n",
        "        'position': {'lat': 38.825767, 'lon': -75.440117},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDTX': {\n",
        "        'position': {'lat': 42.7, 'lon': -83.471667},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDVN': {\n",
        "        'position': {'lat': 41.611667, 'lon': -90.580833},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KDYX': {\n",
        "        'position': {'lat': 32.5385, 'lon': -99.254333},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEAX': {\n",
        "        'position': {'lat': 38.81025, 'lon': -94.264472},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEMX': {\n",
        "        'position': {'lat': 31.89365, 'lon': -110.63025},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KENX': {\n",
        "        'position': {'lat': 42.586556, 'lon': -74.064083},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEOX': {\n",
        "        'position': {'lat': 31.460556, 'lon': -85.459389},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEPZ': {\n",
        "        'position': {'lat': 31.873056, 'lon': -106.698},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KESX': {\n",
        "        'position': {'lat': 35.70135, 'lon': -114.89165},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEVX': {\n",
        "        'position': {'lat': 30.565033, 'lon': -85.921667},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEWX': {\n",
        "        'position': {'lat': 29.704056, 'lon': -98.028611},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KEYX': {\n",
        "        'position': {'lat': 35.09785, 'lon': -117.56075},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFCX': {\n",
        "        'position': {'lat': 37.0244, 'lon': -80.273969},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFDR': {\n",
        "        'position': {'lat': 34.362194, 'lon': -98.976667},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFDX': {\n",
        "        'position': {'lat': 34.634167, 'lon': -103.61888},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFFC': {\n",
        "        'position': {'lat': 33.36355, 'lon': -84.56595},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFSD': {\n",
        "        'position': {'lat': 43.587778, 'lon': -96.729444},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFSX': {\n",
        "        'position': {'lat': 34.574333, 'lon': -111.19844},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFTG': {\n",
        "        'position': {'lat': 39.786639, 'lon': -104.5458},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KFWS': {\n",
        "        'position': {'lat': 32.573, 'lon': -97.30315},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGGW': {\n",
        "        'position': {'lat': 48.206361, 'lon': -106.62469},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGJX': {\n",
        "        'position': {'lat': 39.062169, 'lon': -108.21376},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGLD': {\n",
        "        'position': {'lat': 39.366944, 'lon': -101.70027},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGRB': {\n",
        "        'position': {'lat': 44.498633, 'lon': -88.111111},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGRK': {\n",
        "        'position': {'lat': 30.721833, 'lon': -97.382944},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGRR': {\n",
        "        'position': {'lat': 42.893889, 'lon': -85.544889},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGSP': {\n",
        "        'position': {'lat': 34.883306, 'lon': -82.219833},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGWX': {\n",
        "        'position': {'lat': 33.896917, 'lon': -88.329194},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KGYX': {\n",
        "        'position': {'lat': 43.891306, 'lon': -70.256361},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KHDC': {\n",
        "        'position': {'lat': 30.5193, 'lon': -90.4074},\n",
        "        'active_ranges': [{'start': '2024-03-23', 'end': None}],\n",
        "    },\n",
        "    'KHDX': {\n",
        "        'position': {'lat': 33.077, 'lon': -106.12003},\n",
        "        'active_ranges': [{'start': '1995-08-24', 'end': None}],\n",
        "    },\n",
        "    'KHGX': {\n",
        "        'position': {'lat': 29.4719, 'lon': -95.078733},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KHNX': {\n",
        "        'position': {'lat': 36.314181, 'lon': -119.63213},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KHPX': {\n",
        "        'position': {'lat': 36.736972, 'lon': -87.285583},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KHTX': {\n",
        "        'position': {'lat': 34.930556, 'lon': -86.083611},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KICT': {\n",
        "        'position': {'lat': 37.654444, 'lon': -97.443056},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KICX': {\n",
        "        'position': {'lat': 37.59105, 'lon': -112.86218},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KILN': {\n",
        "        'position': {'lat': 39.420483, 'lon': -83.82145},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KILX': {\n",
        "        'position': {'lat': 40.1505, 'lon': -89.336792},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KIND': {\n",
        "        'position': {'lat': 39.7075, 'lon': -86.280278},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KINX': {\n",
        "        'position': {'lat': 36.175131, 'lon': -95.564161},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KIWA': {\n",
        "        'position': {'lat': 33.289233, 'lon': -111.66991},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KIWX': {\n",
        "        'position': {'lat': 41.358611, 'lon': -85.7},\n",
        "        'active_ranges': [{'start': '1997-08-30', 'end': None}],\n",
        "    },\n",
        "    'KJAX': {\n",
        "        'position': {'lat': 30.484633, 'lon': -81.7019},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KJGX': {\n",
        "        'position': {'lat': 32.675683, 'lon': -83.350833},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KJKL': {\n",
        "        'position': {'lat': 37.590833, 'lon': -83.313056},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLBB': {\n",
        "        'position': {'lat': 33.654139, 'lon': -101.81416},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLCH': {\n",
        "        'position': {'lat': 30.125306, 'lon': -93.215889},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLGX': {\n",
        "        'position': {'lat': 47.116944, 'lon': -124.10666},\n",
        "        'active_ranges': [{'start': '2011-10-01', 'end': None}],\n",
        "    },\n",
        "    'KLIX': {\n",
        "        'position': {'lat': 30.336667, 'lon': -89.825417},\n",
        "        'active_ranges': [{'start': None, 'end': '2024-03-28'}],\n",
        "    },\n",
        "    'KLNX': {\n",
        "        'position': {'lat': 41.957944, 'lon': -100.57622},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLOT': {\n",
        "        'position': {'lat': 41.604444, 'lon': -88.084444},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLRX': {\n",
        "        'position': {'lat': 40.73955, 'lon': -116.8027},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLSX': {\n",
        "        'position': {'lat': 38.698611, 'lon': -90.682778},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLTX': {\n",
        "        'position': {'lat': 33.98915, 'lon': -78.429108},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLVX': {\n",
        "        'position': {'lat': 37.975278, 'lon': -85.943889},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KLWX': {\n",
        "        'position': {'lat': 38.976111, 'lon': -77.4875},\n",
        "        'active_ranges': [{'start': '1992-06-12', 'end': None}],\n",
        "    },\n",
        "    'KLZK': {\n",
        "        'position': {'lat': 34.8365, 'lon': -92.262194},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMAF': {\n",
        "        'position': {'lat': 31.943461, 'lon': -102.18925},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMAX': {\n",
        "        'position': {'lat': 42.081169, 'lon': -122.71736},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMBX': {\n",
        "        'position': {'lat': 48.393056, 'lon': -100.86444},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMHX': {\n",
        "        'position': {'lat': 34.775908, 'lon': -76.876189},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMKX': {\n",
        "        'position': {'lat': 42.9679, 'lon': -88.550667},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMLB': {\n",
        "        'position': {'lat': 28.113194, 'lon': -80.654083},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMOB': {\n",
        "        'position': {'lat': 30.679444, 'lon': -88.24},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMPX': {\n",
        "        'position': {'lat': 44.848889, 'lon': -93.565528},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMQT': {\n",
        "        'position': {'lat': 46.531111, 'lon': -87.548333},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMRX': {\n",
        "        'position': {'lat': 36.168611, 'lon': -83.401944},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMSX': {\n",
        "        'position': {'lat': 47.041, 'lon': -113.98622},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMTX': {\n",
        "        'position': {'lat': 41.262778, 'lon': -112.44777},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMUX': {\n",
        "        'position': {'lat': 37.155222, 'lon': -121.89844},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMVX': {\n",
        "        'position': {'lat': 47.527778, 'lon': -97.325556},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KMXX': {\n",
        "        'position': {'lat': 32.53665, 'lon': -85.78975},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KNKX': {\n",
        "        'position': {'lat': 32.919017, 'lon': -117.0418},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KNQA': {\n",
        "        'position': {'lat': 35.344722, 'lon': -89.873333},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KOAX': {\n",
        "        'position': {'lat': 41.320369, 'lon': -96.366819},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KOHX': {\n",
        "        'position': {'lat': 36.247222, 'lon': -86.5625},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KOKX': {\n",
        "        'position': {'lat': 40.865528, 'lon': -72.863917},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KOTX': {\n",
        "        'position': {'lat': 47.680417, 'lon': -117.62677},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KOUN': {\n",
        "        'position': {'lat': 35.236058, 'lon': -97.46235},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KPAH': {\n",
        "        'position': {'lat': 37.068333, 'lon': -88.771944},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KPBZ': {\n",
        "        'position': {'lat': 40.531717, 'lon': -80.217967},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KPDT': {\n",
        "        'position': {'lat': 45.69065, 'lon': -118.85293},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KPOE': {\n",
        "        'position': {'lat': 31.155278, 'lon': -92.976111},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KPUX': {\n",
        "        'position': {'lat': 38.45955, 'lon': -104.18135},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KRAX': {\n",
        "        'position': {'lat': 35.665519, 'lon': -78.48975},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KRGX': {\n",
        "        'position': {'lat': 39.754056, 'lon': -119.46202},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KRIW': {\n",
        "        'position': {'lat': 43.066089, 'lon': -108.4773},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KRLX': {\n",
        "        'position': {'lat': 38.311111, 'lon': -81.722778},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KRTX': {\n",
        "        'position': {'lat': 45.715039, 'lon': -122.965},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSFX': {\n",
        "        'position': {'lat': 43.1056, 'lon': -112.68613},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSGF': {\n",
        "        'position': {'lat': 37.235239, 'lon': -93.400419},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSHV': {\n",
        "        'position': {'lat': 32.450833, 'lon': -93.84125},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSJT': {\n",
        "        'position': {'lat': 31.371278, 'lon': -100.4925},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSOX': {\n",
        "        'position': {'lat': 33.817733, 'lon': -117.636},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KSRX': {\n",
        "        'position': {'lat': 35.290417, 'lon': -94.361889},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTBW': {\n",
        "        'position': {'lat': 27.7055, 'lon': -82.401778},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTFX': {\n",
        "        'position': {'lat': 47.459583, 'lon': -111.38533},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTLH': {\n",
        "        'position': {'lat': 30.397583, 'lon': -84.328944},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTLX': {\n",
        "        'position': {'lat': 35.333361, 'lon': -97.277761},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTWX': {\n",
        "        'position': {'lat': 38.99695, 'lon': -96.23255},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KTYX': {\n",
        "        'position': {'lat': 43.755694, 'lon': -75.679861},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KUDX': {\n",
        "        'position': {'lat': 44.124722, 'lon': -102.83},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KUEX': {\n",
        "        'position': {'lat': 40.320833, 'lon': -98.441944},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KVAX': {\n",
        "        'position': {'lat': 30.890278, 'lon': -83.001806},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KVBX': {\n",
        "        'position': {'lat': 34.83855, 'lon': -120.39791},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KVNX': {\n",
        "        'position': {'lat': 36.740617, 'lon': -98.127717},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KVTX': {\n",
        "        'position': {'lat': 34.412017, 'lon': -119.17875},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KVWX': {\n",
        "        'position': {'lat': 38.26025, 'lon': -87.724528},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'KYUX': {\n",
        "        'position': {'lat': 32.495281, 'lon': -114.65671},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'LPLA': {\n",
        "        'position': {'lat': 38.73028, 'lon': -27.32167},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'RKJK': {\n",
        "        'position': {'lat': 35.924167, 'lon': 126.622222},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'RKSG': {\n",
        "        'position': {'lat': 37.207569, 'lon': 127.285561},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "    'RODN': {\n",
        "        'position': {'lat': 26.3078, 'lon': 127.903469},\n",
        "        'active_ranges': [{'start': None, 'end': None}],\n",
        "    },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2AUvxuLkYLC"
      },
      "source": [
        "#### **Link Observation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj-JjohWlDgn"
      },
      "outputs": [],
      "source": [
        "from pyart.graph import RadarDisplay\n",
        "from netCDF4 import num2date\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import display as ipy_display\n",
        "\n",
        "from scipy.interpolate import griddata\n",
        "from scipy.ndimage import binary_closing, label\n",
        "from pyproj import Transformer\n",
        "\n",
        "\n",
        "\n",
        "################################################################### TOP-LEVEL LINKING FUNCTION #######################################################################\n",
        "\n",
        "\n",
        "\n",
        "def link_obs_data(files,\n",
        "                  lsr_df,\n",
        "                  spc_df,\n",
        "                  debug,\n",
        "                  cache_dir=\"Datasets/surface_obs_datasets/linked_obs_cache\",\n",
        "):\n",
        "    \"\"\"\n",
        "    files: List of (lvl2_key, CompositeReflectivity) tuples\n",
        "    lsr_df: pre-loaded LSR DataFrame\n",
        "    debug:  whether to print/display intermediate results\n",
        "\n",
        "    - For each radar scan, create dataframes containing LSR, synoptic, and spc data, as well as cell metadata\n",
        "    - Repeat for all cells in the radar scan\n",
        "    - Return a DataFrame containing observations for all cells over all radar scans\n",
        "    \"\"\"\n",
        "    # Store all storm cell dataframes for all scans\n",
        "    all_records = []\n",
        "\n",
        "    # For each radar scan, create dataframes containing LSR and synoptic data + cell metadata, for all cells in the radar scan\n",
        "    for radar_file_name, comp_scan in files:\n",
        "        # build simple cache path (based on radar_file_name)\n",
        "        safe_name = re.sub(r'[^A-Za-z0-9._-]', '_', radar_file_name)\n",
        "        cache_path = os.path.join(cache_dir, f\"{safe_name}.csv\")\n",
        "\n",
        "        # if cached, load and continue\n",
        "        if os.path.exists(cache_path):\n",
        "            if debug:\n",
        "                print(f\"[link_obs_data] Loading cached scan for {radar_file_name} -> {cache_path}\")\n",
        "            try:\n",
        "                scan_df = pd.read_csv(cache_path)\n",
        "\n",
        "                if 'scan_time' in scan_df.columns:\n",
        "                    scan_df['scan_time'] = pd.to_datetime(scan_df['scan_time'], utc=True, errors='coerce')\n",
        "                all_records.append(scan_df)\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"[link_obs_data] WARNING: failed to read cache {cache_path} ({e}), will recompute scan.\")\n",
        "\n",
        "\n",
        "        ############################################# IF CACHE DOESN'T EXIST, PROCEED WITH SURFACE-OBS LOGIC #####################################################\n",
        "\n",
        "\n",
        "        # Extract scan_time from the radar_scan.time field.\n",
        "        raw_times = comp_scan.time['data']             # e.g. array([1625694000.0, â€¦])\n",
        "        time_units = comp_scan.time['units']           # e.g. \"seconds since 1970-01-01T00:00:00Z\"\n",
        "        calendar = getattr(comp_scan.time, 'calendar', 'standard')\n",
        "\n",
        "        # Convert the first sweepâ€™s time to a datetime\n",
        "        scan_time = num2date(raw_times[0], time_units, calendar)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"[link_obs_data] No cached scan found, processing normally for {radar_file_name} @ {scan_time}\")\n",
        "\n",
        "        # Get cells from the Level-III composite (where each cell is an ID, center_lat, center_lon, bounding_lat [tuple], bounding_lon [tuple])\n",
        "        cells, cell_fig = get_cell_centers(\n",
        "            comp_scan=comp_scan,\n",
        "            sweep=0,\n",
        "            class_field='reflectivity',\n",
        "            threshold=35,\n",
        "            min_size=500,\n",
        "            debug=debug\n",
        "        )\n",
        "\n",
        "        # Store per-scan dataframes from individual cells\n",
        "        scan_records = []\n",
        "\n",
        "        # Build observational data for each cell\n",
        "        for cell_id, center_lat, center_lon, bounding_lat, bounding_lon in cells:\n",
        "            # Get obs data from synoptic\n",
        "            df_synoptic = filter_synoptic(\n",
        "                bounding_lat=bounding_lat,\n",
        "                bounding_lon=bounding_lon,\n",
        "                center_lat=center_lat,\n",
        "                center_lon=center_lon,\n",
        "                scan_time=scan_time,\n",
        "                time_window=timedelta(minutes=5),\n",
        "                debug=debug\n",
        "            )\n",
        "            if debug:\n",
        "                print(f\"\\n synoptic reports for the cell {cell_id} in the radar_file_name {radar_file_name} at the time {scan_time}: \\n\")\n",
        "                display(df_synoptic)\n",
        "\n",
        "            # Get obs data from lsr\n",
        "            df_lsr = filter_lsr(\n",
        "                lsr_df=lsr_df,\n",
        "                bounding_lat=bounding_lat,\n",
        "                bounding_lon=bounding_lon,\n",
        "                center_lat=center_lat,\n",
        "                center_lon=center_lon,\n",
        "                scan_time=scan_time,\n",
        "                debug=debug\n",
        "            )\n",
        "            if debug:\n",
        "                print(f\"\\n lsr reports for the cell {cell_id} in the radar_file_name {radar_file_name} at the time {scan_time}: \\n\")\n",
        "                display(df_lsr)\n",
        "\n",
        "            df_spc = filter_spc(\n",
        "                spc_df=spc_df,\n",
        "                bounding_lat=bounding_lat,\n",
        "                bounding_lon=bounding_lon,\n",
        "                center_lat=center_lat,\n",
        "                center_lon=center_lon,\n",
        "                scan_time=scan_time,\n",
        "            )\n",
        "            if debug:\n",
        "                print(f\"\\n spc reports for the cell {cell_id} in the radar_file_name {radar_file_name} at the time {scan_time}: \\n\")\n",
        "                display(df_spc)\n",
        "\n",
        "            # For each cell, concatenate the three dataframes observational dataframes\n",
        "            df_cell = pd.concat([df_synoptic, df_lsr, df_spc], ignore_index=True)\n",
        "\n",
        "            # If no observations, skip and move on to next cell\n",
        "            if df_cell.empty:\n",
        "                continue\n",
        "\n",
        "            # If not empty, then add columns containing cell metadata\n",
        "            df_cell['radar_file_name'] = radar_file_name\n",
        "            df_cell['scan_time'] = scan_time\n",
        "            df_cell['cell_id']   = cell_id\n",
        "            df_cell['cell_lat']  = center_lat\n",
        "            df_cell['cell_lon']  = center_lon\n",
        "\n",
        "            # Unpack tuples so operations are easier later\n",
        "            min_lat, max_lat = bounding_lat\n",
        "            min_lon, max_lon = bounding_lon\n",
        "\n",
        "            df_cell['bounding_lat_min'] = min_lat\n",
        "            df_cell['bounding_lat_max'] = max_lat\n",
        "            df_cell['bounding_lon_min'] = min_lon\n",
        "            df_cell['bounding_lon_max'] = max_lon\n",
        "\n",
        "\n",
        "            # Add each cell's dataframe into scan_records\n",
        "            scan_records.append(df_cell)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"\\n [link_obs_data] Complete dataframe for the cell {cell_id} in the radar_file_name {radar_file_name}: \\n\")\n",
        "                display(df_cell)\n",
        "                print(f\"\\n [link_obs_data] Overlayed observation data for cell {cell_id}: \")\n",
        "                _plot_observations_on_fig(cell_fig, df_cell, lon_col='station_lon', lat_col='station_lat', ms=50, alpha=0.85)\n",
        "                section_seperator(4)\n",
        "\n",
        "\n",
        "        # Write scan_records to cache\n",
        "        if not scan_records:\n",
        "            scan_df = pd.DataFrame(columns=[\n",
        "                \"source\", \"time\", \"station_lat\", \"station_lon\", \"gust\", \"distance_km\",\n",
        "                \"radar_file_name\", \"scan_time\", \"cell_id\", \"cell_lat\", \"cell_lon\",\n",
        "                \"bounding_lat_min\", \"bounding_lat_max\", \"bounding_lon_min\", \"bounding_lon_max\"\n",
        "            ])\n",
        "            if debug:\n",
        "                print(f\"[link_obs_data] No obs for scan {radar_file_name}; will cache empty dataframe.\")\n",
        "        else:\n",
        "            scan_df = pd.concat(scan_records, ignore_index=True)\n",
        "\n",
        "        try:\n",
        "            tmp_path = cache_path + \".tmp\"\n",
        "\n",
        "            if 'scan_time' in scan_df.columns:\n",
        "                scan_df['scan_time'] = pd.to_datetime(scan_df['scan_time'], utc=True, errors='coerce')\n",
        "            scan_df.to_csv(tmp_path, index=False)\n",
        "            os.replace(tmp_path, cache_path)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"[link_obs_data] Wrote cache for {radar_file_name} -> {cache_path}\")\n",
        "                section_seperator(4)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[link_obs_data] WARNING: failed to write cache ({e})\")\n",
        "        all_records.append(scan_df)\n",
        "\n",
        "\n",
        "    # Finally, handle all_records after looping through all radar scans\n",
        "    if not all_records:\n",
        "        # Return an empty DataFrame with expected columns\n",
        "        full = pd.DataFrame(columns=[\n",
        "            # Obs columns\n",
        "            \"source\",\n",
        "            \"time\",\n",
        "            \"station_lat\",\n",
        "            \"station_lon\",\n",
        "            \"gust\",\n",
        "            \"obs_distance\",\n",
        "\n",
        "            # Cell metadata\n",
        "            \"radar_file_name\",\n",
        "            \"scan_time\",\n",
        "            \"cell_id\",\n",
        "            \"cell_lat\",     # Where cell_lat and cell_lon are columns for center of cell\n",
        "            \"cell_lon\",\n",
        "            \"bounding_lat_min\", # The bounds of the cell (use for cropping our level 2 data later)\n",
        "            \"bounding_lat_max\",\n",
        "            \"bounding_lon_min\",\n",
        "            \"bounding_lon_max\"\n",
        "        ])\n",
        "    else:\n",
        "        full = pd.concat(all_records, ignore_index=True)\n",
        "    return full\n",
        "\n",
        "\n",
        "def _plot_observations_on_fig(\n",
        "    fig,\n",
        "    obs_df,\n",
        "    lon_col='station_lon',\n",
        "    lat_col='station_lat',\n",
        "    marker='o',\n",
        "    ms=40,\n",
        "    alpha=0.9,\n",
        "    edgecolor='k',\n",
        "    zorder=6):\n",
        "    \"\"\"\n",
        "    Robust overlay for Colab:\n",
        "     - prints debug info (how many obs will be plotted)\n",
        "     - plots points on the first axis of `fig`\n",
        "     - calls ax.relim()/ax.autoscale_view() to ensure points are visible\n",
        "     - displays the updated figure via IPython.display (works reliably in Colab)\n",
        "    \"\"\"\n",
        "    # Safety guards\n",
        "    if fig is None:\n",
        "        print(\"[_plot_observations_on_fig] got fig=None -> skipping overlay\")\n",
        "        return\n",
        "\n",
        "    if obs_df is None or obs_df.empty:\n",
        "        print(\"[_plot_observations_on_fig] obs_df empty -> showing base figure\")\n",
        "        ipy_display(fig)\n",
        "        return\n",
        "\n",
        "    # get or create axis\n",
        "    ax = fig.axes[0] if fig.axes else fig.add_subplot(111)\n",
        "\n",
        "    # make sure we have numeric lon/lat\n",
        "    lons = pd.to_numeric(obs_df[lon_col], errors='coerce')\n",
        "    lats = pd.to_numeric(obs_df[lat_col], errors='coerce')\n",
        "    valid = lons.notna() & lats.notna()\n",
        "    n_valid = int(valid.sum())\n",
        "\n",
        "    print(f\"[_plot_observations_on_fig] plotting {n_valid} observation(s) (from {len(obs_df)} rows)\")\n",
        "\n",
        "    if n_valid == 0:\n",
        "        # show the base figure so you still get the blob-only output\n",
        "        ipy_display(fig)\n",
        "        return\n",
        "\n",
        "    # scatter them on the axis\n",
        "    sc = ax.scatter(\n",
        "        lons[valid], lats[valid],\n",
        "        s=ms, marker=marker, alpha=alpha,\n",
        "        edgecolors=edgecolor, linewidths=0.5,\n",
        "        zorder=zorder, label='obs'\n",
        "    )\n",
        "\n",
        "    # Ensure new points fall inside the visible area:\n",
        "    try:\n",
        "        # recompute data limits and autoscale the view\n",
        "        ax.relim()\n",
        "        ax.autoscale_view()\n",
        "\n",
        "        # optionally pad the limits a little (5%)\n",
        "        xmin, xmax = ax.get_xlim()\n",
        "        ymin, ymax = ax.get_ylim()\n",
        "        padx = 0.05 * (xmax - xmin) if (xmax - xmin) != 0 else 0.01\n",
        "        pady = 0.05 * (ymax - ymin) if (ymax - ymin) != 0 else 0.01\n",
        "        ax.set_xlim(xmin - padx, xmax + padx)\n",
        "        ax.set_ylim(ymin - pady, ymax + pady)\n",
        "    except Exception as e:\n",
        "        # non-fatal; continue to display figure\n",
        "        print(f\"[_plot_observations_on_fig] autoscale failed: {e}\")\n",
        "\n",
        "    # tidy legend (dedupe)\n",
        "    try:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        uniq = {}\n",
        "        for h, l in zip(handles, labels):\n",
        "            if l not in uniq:\n",
        "                uniq[l] = h\n",
        "        if uniq:\n",
        "            ax.legend(list(uniq.values()), list(uniq.keys()), fontsize='small', loc='best')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # show the updated figure in Colab\n",
        "    ipy_display(fig)\n",
        "    # also call plt.pause(0.001) to flush (usually not necessary in Colab but harmless)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlOWiLpswWO2"
      },
      "source": [
        "#### **Save Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ackV3bucwZgD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import math\n",
        "import gc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from pyart.graph import RadarDisplay\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "\n",
        "def save_df_for_training(\n",
        "    df: pd.DataFrame,\n",
        "    base_dir: str,\n",
        "    year_override: int | None = None,\n",
        "    radar_site_col: str = \"radar_site\",\n",
        "    storm_id_col: str = \"storm_id\",\n",
        "    time_col: str = \"time\",\n",
        "    debug: bool = True,\n",
        "    scan_order: list | None = None,\n",
        "    drop_scans_after_save: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Save a SEVIR-like training package per storm folder:\n",
        "      base_dir/{year}/{RADAR_SITE}/storm_{storm_id}/\n",
        "\n",
        "    Inside each storm folder:\n",
        "      - one CONTEXT HDF (rows=times; all bbox_df columns except *_scan and *_cache_volume_path)\n",
        "      - one product HDF per *_scan column, each containing dataset '/data'\n",
        "        shaped (T, H, W, C) with float32 + NaN, chunked & compressed.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - H, W are determined per product within the storm by taking the maximum\n",
        "      (rays, gates) across available sweeps and scans; smaller frames are NaN-padded.\n",
        "    - Channels C:\n",
        "        * If comp.metadata['pseudo_host_sweep'] exists  â†’ C = 1 (use sweep 0).\n",
        "        * Otherwise                                   â†’ C = number of sweeps (nsweeps) in that scan.\n",
        "      The fileâ€™s C is the **maximum** channels seen across the storm for that product;\n",
        "      scans with fewer sweeps are padded with NaN channels.\n",
        "    \"\"\"\n",
        "    from time import perf_counter\n",
        "\n",
        "    def _fmt_bytes(n: int) -> str:\n",
        "        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
        "            if abs(n) < 1024.0:\n",
        "                return f\"{n:.1f} {unit}\"\n",
        "            n /= 1024.0\n",
        "        return f\"{n:.1f} PB\"\n",
        "\n",
        "    def _pretty_attr(val):\n",
        "        import numpy as _np\n",
        "        try:\n",
        "            if isinstance(val, (bytes, _np.bytes_)):\n",
        "                return val.decode(\"utf-8\", errors=\"replace\")\n",
        "            if isinstance(val, (_np.bool_, _np.integer, _np.floating)):\n",
        "                return val.item()\n",
        "            if isinstance(val, _np.ndarray):\n",
        "                if val.ndim == 0:\n",
        "                    return val.item()\n",
        "                if val.size > 16:\n",
        "                    return f\"array(shape={val.shape}, dtype={val.dtype})\"\n",
        "                return _np.array2string(val, threshold=16)\n",
        "            return val\n",
        "        except Exception:\n",
        "            return str(val)\n",
        "\n",
        "    import os, shutil, gc, warnings, json\n",
        "    from datetime import datetime\n",
        "    import numpy as np\n",
        "    import numpy.ma as ma\n",
        "    import pandas as pd\n",
        "\n",
        "    try:\n",
        "        import h5py\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"h5py is required for save_df_for_training()\") from e\n",
        "\n",
        "    t_all0 = perf_counter()\n",
        "\n",
        "    if df is None or len(df) == 0:\n",
        "        if debug:\n",
        "            print(\"[save] empty df â€” nothing to save.\")\n",
        "        return []\n",
        "\n",
        "    # ---------- discover product prefixes ----------\n",
        "    t_disc0 = perf_counter()\n",
        "    scan_cols = [c for c in df.columns if c.endswith(\"_scan\")]\n",
        "    if not scan_cols:\n",
        "        if debug:\n",
        "            print(\"[save] no *_scan columns detected â€” nothing to write.\")\n",
        "        return []\n",
        "    prefixes = [c[:-5] for c in scan_cols]  # strip _scan\n",
        "    if scan_order:\n",
        "        ordered = [p for p in scan_order if p in prefixes]\n",
        "        ordered += [p for p in prefixes if p not in ordered]\n",
        "        prefixes = ordered\n",
        "    t_disc1 = perf_counter()\n",
        "    if debug:\n",
        "        print(f\"[save] detected products (in order): {prefixes}\")\n",
        "        print(f\"[save] discovery time: {(t_disc1 - t_disc0)*1000:.1f} ms\")\n",
        "\n",
        "    # helper: derive year for a row\n",
        "    def _row_year(row):\n",
        "        if year_override is not None:\n",
        "            return int(year_override)\n",
        "        t = row.get(time_col)\n",
        "        try:\n",
        "            tt = pd.to_datetime(t)\n",
        "            return int(tt.year)\n",
        "        except Exception:\n",
        "            return int(datetime.utcnow().year)\n",
        "\n",
        "    # group by (year, site, storm_id) so each folder has one context HDF + per-product HDFs\n",
        "    t_grp0 = perf_counter()\n",
        "    groups = df.groupby([df.apply(_row_year, axis=1), df[radar_site_col], df[storm_id_col]], sort=False)\n",
        "    t_grp1 = perf_counter()\n",
        "    if debug:\n",
        "        print(f\"[save] grouping produced {len(groups)} storm group(s) in {(t_grp1 - t_grp0)*1000:.1f} ms\")\n",
        "\n",
        "    saved = []\n",
        "\n",
        "    # NEW: in-memory debug snapshots\n",
        "    debug_product_attrs = []   # list of {\"path\", \"file_attrs\", \"data_attrs\"}\n",
        "    debug_context_peek = []    # list of {\"context_path\", \"columns\", \"row\"}\n",
        "\n",
        "    # -------------- utilities --------------\n",
        "\n",
        "    def _choose_field_key(radar_obj, prefix: str):\n",
        "        \"\"\"Pick a field key from radar.fields robustly.\"\"\"\n",
        "        fkeys = list(getattr(radar_obj, \"fields\", {}).keys())\n",
        "        if not fkeys:\n",
        "            return None\n",
        "        # exact prefix match\n",
        "        if prefix in fkeys:\n",
        "            return prefix\n",
        "        # common aliases\n",
        "        prefer = []\n",
        "        low = prefix.lower()\n",
        "        if \"refl\" in low or \"dbz\" in low or \"reflect\" in low:\n",
        "            prefer += [\"reflectivity\", \"corrected_reflectivity\"]\n",
        "        if \"vel\" in low:\n",
        "            prefer += [\"velocity\", \"corrected_velocity\", \"dealiased_velocity\"]\n",
        "        if \"spectrum\" in low or \"width\" in low:\n",
        "            prefer += [\"spectrum_width\"]\n",
        "        for cand in prefer:\n",
        "            if cand in fkeys:\n",
        "                return cand\n",
        "        # substring match against available keys\n",
        "        for k in fkeys:\n",
        "            if low in k.lower():\n",
        "                return k\n",
        "        # fallback: reflectivity if present\n",
        "        if \"reflectivity\" in fkeys:\n",
        "            return \"reflectivity\"\n",
        "        # final fallback: first field\n",
        "        return fkeys[0]\n",
        "\n",
        "    def _is_pseudo(radar_obj) -> bool:\n",
        "        try:\n",
        "            meta = getattr(radar_obj, \"metadata\", {}) or {}\n",
        "        except Exception:\n",
        "            meta = {}\n",
        "        return \"pseudo_host_sweep\" in meta\n",
        "\n",
        "    def _get_sweep_bounds(radar_obj, sw: int):\n",
        "        \"\"\"Return (start, end) indices for rays of sweep sw; falls back gracefully.\"\"\"\n",
        "        try:\n",
        "            s = int(radar_obj.sweep_start_ray_index[\"data\"][sw])\n",
        "            e = int(radar_obj.sweep_end_ray_index[\"data\"][sw])\n",
        "            return s, e\n",
        "        except Exception:\n",
        "            # best effort: whole series\n",
        "            nrays = int(getattr(radar_obj, \"nrays\", 0)) or 0\n",
        "            return 0, nrays\n",
        "\n",
        "    def _frames_from_scan_all_sweeps(radar_obj, field_key):\n",
        "        \"\"\"\n",
        "        Returns a list of 2-D float32 arrays, one per sweep (or [sweep0] for pseudo),\n",
        "        each shaped (rays, gates) with NaNs where masked/invalid.\n",
        "        \"\"\"\n",
        "        fdict = radar_obj.fields.get(field_key)\n",
        "        if fdict is None:\n",
        "            return []\n",
        "\n",
        "        data = fdict.get(\"data\")\n",
        "        if data is None:\n",
        "            return []\n",
        "\n",
        "        # Which sweeps?\n",
        "        if _is_pseudo(radar_obj):\n",
        "            sweep_indices = [0]\n",
        "        else:\n",
        "            try:\n",
        "                ns = int(getattr(radar_obj, \"nsweeps\", len(radar_obj.sweep_number[\"data\"])))\n",
        "            except Exception:\n",
        "                ns = 1\n",
        "            sweep_indices = list(range(max(0, ns)))\n",
        "\n",
        "        out = []\n",
        "        for sw in sweep_indices:\n",
        "            s, e = _get_sweep_bounds(radar_obj, sw)\n",
        "            try:\n",
        "                sub = data[s:e, :]\n",
        "            except Exception:\n",
        "                sub = data  # fallback\n",
        "\n",
        "            if isinstance(sub, ma.MaskedArray):\n",
        "                arr = sub.filled(np.nan).astype(np.float32, copy=False)\n",
        "            else:\n",
        "                arr = sub.astype(np.float32, copy=False)\n",
        "                if not np.issubdtype(arr.dtype, np.floating):\n",
        "                    arr = arr.astype(np.float32, copy=False)\n",
        "\n",
        "            # ignore degenerate empty slices\n",
        "            if arr.size == 0 or arr.ndim != 2:\n",
        "                continue\n",
        "            out.append(arr)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # -------------- main loop --------------\n",
        "    for (yr, site, storm), sub in groups:\n",
        "        t_group0 = perf_counter()\n",
        "        sub = sub.sort_values(time_col).reset_index(drop=True)\n",
        "        T = len(sub)\n",
        "\n",
        "        # group summary\n",
        "        if debug:\n",
        "            try:\n",
        "                t0_dbg = pd.to_datetime(sub[time_col].iloc[0]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                t1_dbg = pd.to_datetime(sub[time_col].iloc[-1]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            except Exception:\n",
        "                t0_dbg = t1_dbg = \"NA\"\n",
        "            print(f\"\\n[save] ===== Group: year={yr} site={site} storm={storm} =====\")\n",
        "            print(f\"[save] rows(T)={T} time-span=[{t0_dbg} .. {t1_dbg}]\")\n",
        "\n",
        "        storm_dir = os.path.join(base_dir, str(yr), str(site), f\"storm_{storm}\")\n",
        "        os.makedirs(storm_dir, exist_ok=True)\n",
        "\n",
        "        # ---------- write context HDF ----------\n",
        "        t_meta0 = perf_counter()\n",
        "        context_df = sub.copy()\n",
        "        drop_cols = [c for c in context_df.columns if c.endswith(\"_scan\") or c.endswith(\"_cache_volume_path\")]\n",
        "        context_df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "        # FIX: add primary-key component time_unix_ms (UTC, int64 ms since epoch)\n",
        "        try:\n",
        "            t_utc = pd.to_datetime(context_df[time_col], utc=True, errors=\"coerce\")\n",
        "            context_df[\"time_unix_ms\"] = (t_utc.astype(\"int64\") // 1_000_000)\n",
        "        except Exception:\n",
        "            # last-resort fallback; you shouldn't hit this\n",
        "            context_df[\"time_unix_ms\"] = -1\n",
        "\n",
        "        # FIX: normalize longitudes to [-180, 180)\n",
        "        def _wrap180_series(s):\n",
        "            vals = pd.to_numeric(s, errors=\"coerce\")\n",
        "            return ((vals + 180.0) % 360.0) - 180.0\n",
        "\n",
        "        for _lon_col in [\"longitude\", \"tor_lon\", \"wind_lon\", \"hail_lon\", \"min_lon\", \"max_lon\"]:\n",
        "            if _lon_col in context_df.columns:\n",
        "                context_df[_lon_col] = _wrap180_series(context_df[_lon_col])\n",
        "\n",
        "        # FIX: normalize tor_endtime to \"\" or ISO8601 UTC\n",
        "        if \"tor_endtime\" in context_df.columns:\n",
        "            def _norm_endtime(x):\n",
        "                if pd.isna(x):\n",
        "                    return \"\"\n",
        "                s = str(x).strip()\n",
        "                if s in (\"\", \"-\", \"â€”\", \"â€“\", \"None\", \"NA\", \"nan\", \"--------------\"):\n",
        "                    return \"\"\n",
        "                try:\n",
        "                    return pd.to_datetime(s, utc=True).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                except Exception:\n",
        "                    return \"\"\n",
        "            context_df[\"tor_endtime\"] = context_df[\"tor_endtime\"].apply(_norm_endtime)\n",
        "\n",
        "\n",
        "        # Nice file names: include counts + time span\n",
        "        try:\n",
        "            t0 = pd.to_datetime(context_df[time_col].iloc[0]).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "            t1 = pd.to_datetime(context_df[time_col].iloc[-1]).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "        except Exception:\n",
        "            t0 = t1 = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "\n",
        "        context_name = f\"{site}_{storm}_context_T{T:03d}_{t0}_{t1}.h5\"\n",
        "        context_path = os.path.join(storm_dir, context_name)\n",
        "\n",
        "        try:\n",
        "            context_df.to_hdf(context_path, key=\"context\", mode=\"w\", format=\"table\")\n",
        "            if debug:\n",
        "                print(f\"[save] wrote context â†’ {context_path} (cols={len(context_df.columns)}, rows={len(context_df)})\")\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[save] to_hdf failed for context ({context_path}): {e}. Falling back to pickle.\")\n",
        "            import pickle as _pickle\n",
        "            with open(os.path.join(storm_dir, f\"{site}_{storm}_context_T{T:03d}_{t0}_{t1}.pkl\"), \"wb\") as f:\n",
        "                _pickle.dump(context_df.to_dict(orient=\"list\"), f, protocol=_pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        # Capture a one-row preview of the context for debug (first row)\n",
        "        if debug:\n",
        "            if len(context_df) > 0:\n",
        "                row0 = context_df.iloc[0].to_dict()\n",
        "            else:\n",
        "                row0 = {}\n",
        "            debug_context_peek.append({\n",
        "                \"context_path\": context_path,\n",
        "                \"columns\": list(context_df.columns),\n",
        "                \"row\": row0,\n",
        "            })\n",
        "\n",
        "        t_meta1 = perf_counter()\n",
        "        if debug:\n",
        "            try:\n",
        "                sz = os.path.getsize(context_path)\n",
        "                print(f\"[save] context write time: {(t_meta1 - t_meta0)*1000:.1f} ms, size={_fmt_bytes(sz)}\")\n",
        "            except Exception:\n",
        "                print(f\"[save] context write time: {(t_meta1 - t_meta0)*1000:.1f} ms\")\n",
        "\n",
        "        # compute SHA of the just-written context file\n",
        "        try:\n",
        "            def _sha256_local(p, chunk=1024*1024):\n",
        "                import hashlib\n",
        "                h = hashlib.sha256()\n",
        "                with open(p, \"rb\") as f:\n",
        "                    for b in iter(lambda: f.read(chunk), b\"\"):\n",
        "                        h.update(b)\n",
        "                return h.hexdigest()\n",
        "            context_sha256 = _sha256_local(context_path)\n",
        "        except Exception:\n",
        "            context_sha256 = \"\"\n",
        "\n",
        "\n",
        "        # ---------- write context schema sidecar (.schema.json) ----------\n",
        "        schema_name = os.path.splitext(context_name)[0] + \".schema.json\"\n",
        "        schema_path = os.path.join(storm_dir, schema_name)\n",
        "\n",
        "        schema = {\n",
        "          \"schema_version\": \"1.0.0\",\n",
        "          \"applies_to\": context_name,\n",
        "          \"applies_to_sha256\": context_sha256,\n",
        "          \"primary_keys\": [\"time_unix_ms\", \"storm_id\", \"radar_site\"],\n",
        "          \"columns\": {\n",
        "            \"time\":            { \"dtype\": \"string(ISO8601)\", \"unit\": \"UTC\",                  \"desc\": \"Analysis time (UTC)\" },\n",
        "            \"time_unix_ms\":    { \"dtype\": \"int64\",           \"unit\": \"ms since epoch\",       \"desc\": \"Analysis time (Unix milliseconds)\" },\n",
        "            \"storm_id\":        { \"dtype\": \"int32\",           \"unit\": \"\",                     \"desc\": \"Storm track identifier\" },\n",
        "            \"radar_site\":      { \"dtype\": \"string\",          \"unit\": \"\",                     \"desc\": \"NEXRAD site ID for radar data\" },\n",
        "            \"distance_to_site\":{ \"dtype\": \"float32\",         \"unit\": \"km\",                   \"desc\": \"Distance from storm to radar site\" },\n",
        "            \"latitude\":        { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Storm centroid latitude\" },\n",
        "            \"longitude\":       { \"dtype\": \"float32\",         \"unit\": \"deg_east_wrap180\",     \"desc\": \"Storm centroid longitude\" },\n",
        "            \"u_motion\":        { \"dtype\": \"float32\",         \"unit\": \"m/s\",                  \"desc\": \"Eastward storm motion\" },\n",
        "            \"v_motion\":        { \"dtype\": \"float32\",         \"unit\": \"m/s\",                  \"desc\": \"Northward storm motion\" },\n",
        "            \"10_dbz_echo_top\": { \"dtype\": \"float32\",         \"unit\": \"km\",                   \"desc\": \"Echo top height at 10 dBZ (MSL)\" },\n",
        "            \"20_dbz_echo_top\": { \"dtype\": \"float32\",         \"unit\": \"km\",                   \"desc\": \"Echo top height at 20 dBZ (MSL)\" },\n",
        "            \"30_dbz_echo_top\": { \"dtype\": \"float32\",         \"unit\": \"km\",                   \"desc\": \"Echo top height at 30 dBZ (MSL)\" },\n",
        "            \"40_dbz_echo_top\": { \"dtype\": \"float32\",         \"unit\": \"km\",                   \"desc\": \"Echo top height at 40 dBZ (MSL)\" },\n",
        "            \"column_max_refl\": { \"dtype\": \"float32\",         \"unit\": \"dBZ\",                  \"desc\": \"Column-maximum reflectivity\" },\n",
        "            \"tor_count\":       { \"dtype\": \"int32\",           \"unit\": \"\",                     \"desc\": \"Instantaneous tornado count\" },\n",
        "            \"max_tor_intensity\":{ \"dtype\": \"int32\",          \"unit\": \"EF-scale\",             \"desc\": \"Max tornado intensity (EF-scale)\" },\n",
        "            \"tor_genesis\":     { \"dtype\": \"int32\",           \"unit\": \"flag\",                 \"desc\": \"Tornado genesis flag (1 if new tornado)\" },\n",
        "            \"tor_event\":       { \"dtype\": \"int32\",           \"unit\": \"\",                     \"desc\": \"Tornado report number (ID)\" },\n",
        "            \"tor_intensity\":   { \"dtype\": \"int32\",           \"unit\": \"EF-scale\",             \"desc\": \"Tornado intensity rating (EF-scale)\" },\n",
        "            \"tor_endtime\":     { \"dtype\": \"string\",          \"unit\": \"UTC\",                  \"desc\": \"Tornado end time (UTC)\" },\n",
        "            \"tor_lon\":         { \"dtype\": \"float32\",         \"unit\": \"deg_east\",             \"desc\": \"Tornado report longitude\" },\n",
        "            \"tor_lat\":         { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Tornado report latitude\" },\n",
        "            \"tor_width\":       { \"dtype\": \"float32\",         \"unit\": \"yd\",                   \"desc\": \"Tornado path width (yards)\" },\n",
        "            \"tor_length\":      { \"dtype\": \"float32\",         \"unit\": \"mi\",                   \"desc\": \"Tornado path length (miles)\" },\n",
        "            \"wind_flag\":       { \"dtype\": \"int32\",           \"unit\": \"flag\",                 \"desc\": \"Severe wind report flag\" },\n",
        "            \"wind_event\":      { \"dtype\": \"int32\",           \"unit\": \"\",                     \"desc\": \"Wind report number (ID)\" },\n",
        "            \"wind_lon\":        { \"dtype\": \"float32\",         \"unit\": \"deg_east\",             \"desc\": \"Wind report longitude\" },\n",
        "            \"wind_lat\":        { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Wind report latitude\" },\n",
        "            \"wind_magnitude\":  { \"dtype\": \"float32\",         \"unit\": \"mph\",                  \"desc\": \"Wind report speed (mph)\" },\n",
        "            \"hail_flag\":       { \"dtype\": \"int32\",           \"unit\": \"flag\",                 \"desc\": \"Severe hail report flag\" },\n",
        "            \"hail_event\":      { \"dtype\": \"int32\",           \"unit\": \"\",                     \"desc\": \"Hail report number (ID)\" },\n",
        "            \"hail_lon\":        { \"dtype\": \"float32\",         \"unit\": \"deg_east\",             \"desc\": \"Hail report longitude\" },\n",
        "            \"hail_lat\":        { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Hail report latitude\" },\n",
        "            \"hail_magnitude\":  { \"dtype\": \"float32\",         \"unit\": \"in\",                   \"desc\": \"Hail report size (inches)\" },\n",
        "            \"reflectivity_matched_volume_s3_key\": { \"dtype\": \"string\", \"unit\": \"\",            \"desc\": \"Matched radar volume file name\" },\n",
        "            \"min_lat\":         { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Min track latitude\" },\n",
        "            \"max_lat\":         { \"dtype\": \"float32\",         \"unit\": \"deg\",                  \"desc\": \"Max track latitude\" },\n",
        "            \"min_lon\":         { \"dtype\": \"float32\",         \"unit\": \"deg_east_wrap180\",     \"desc\": \"Min track longitude\" },\n",
        "            \"max_lon\":         { \"dtype\": \"float32\",         \"unit\": \"deg_east_wrap180\",     \"desc\": \"Max track longitude\" }\n",
        "          }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(schema, f, indent=2, sort_keys=True)\n",
        "            if debug:\n",
        "                print(f\"[save] wrote context schema sidecar â†’ {schema_path}\")\n",
        "        except Exception as e:\n",
        "            if debug:\n",
        "                print(f\"[save][warn] failed to write schema sidecar: {e}\")\n",
        "\n",
        "        # precompute relpaths for product HDF attrs\n",
        "        # (relative from product file directory to context/schema files)\n",
        "        def _rel(p):\n",
        "            try:\n",
        "                return os.path.relpath(p, start=storm_dir)\n",
        "            except Exception:\n",
        "                return os.path.basename(p)\n",
        "\n",
        "        context_relpath = _rel(context_path)\n",
        "        context_schema_relpath = _rel(schema_path)\n",
        "\n",
        "        # ---------- per-product tensor HDF ----------\n",
        "        for prefix in prefixes:\n",
        "            t_prod0 = perf_counter()\n",
        "            scan_col = f\"{prefix}_scan\"\n",
        "\n",
        "            # first pass: determine (H, W, C_max) by scanning available frames across all sweeps\n",
        "            t_dim0 = perf_counter()\n",
        "            H = 0\n",
        "            W = 0\n",
        "            C = 0\n",
        "            chosen_field = None\n",
        "            any_present = False\n",
        "            min_nsweeps, max_nsweeps = 10**9, 0\n",
        "\n",
        "            for _, row in sub.iterrows():\n",
        "                scan = row.get(scan_col, None)\n",
        "                if scan is None:\n",
        "                    continue\n",
        "                any_present = True\n",
        "                if chosen_field is None:\n",
        "                    chosen_field = _choose_field_key(scan, prefix)\n",
        "\n",
        "                arr_list = _frames_from_scan_all_sweeps(scan, chosen_field)\n",
        "                if not arr_list:\n",
        "                    continue\n",
        "                # update dims over all sweeps in this scan\n",
        "                for a in arr_list:\n",
        "                    H = max(H, int(a.shape[0]))\n",
        "                    W = max(W, int(a.shape[1]))\n",
        "                C = max(C, len(arr_list))\n",
        "                min_nsweeps = min(min_nsweeps, len(arr_list))\n",
        "                max_nsweeps = max(max_nsweeps, len(arr_list))\n",
        "\n",
        "            t_dim1 = perf_counter()\n",
        "\n",
        "            if not any_present:\n",
        "                if debug:\n",
        "                    print(f\"[save] storm ({site},{storm},{yr}) â†’ no data for '{prefix}' â€” skipping file.\")\n",
        "                continue\n",
        "\n",
        "            if H == 0 or W == 0:\n",
        "                if debug:\n",
        "                    print(f\"[save] storm ({site},{storm},{yr}) â†’ '{prefix}': could not infer shape â€” skipping.\")\n",
        "                continue\n",
        "\n",
        "            # build file name with dims/time span (+ channels)\n",
        "            prod_name = f\"{site}_{storm}_{prefix}_T{T:03d}_{H}x{W}x{C}ch_{t0}_{t1}.h5\"\n",
        "            prod_path = os.path.join(storm_dir, prod_name)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"[save] product '{prefix}': chosen_field='{chosen_field}', H={H}, W={W}, C={C} \"\n",
        "                      f\"(nsweeps per time ~ min={min_nsweeps if min_nsweeps<10**9 else 'NA'}, max={max_nsweeps}) \"\n",
        "                      f\"[dimension pass {(t_dim1 - t_dim0)*1000:.1f} ms]\")\n",
        "                print(f\"[save] writing â†’ {prod_path}\")\n",
        "\n",
        "            import h5py\n",
        "            # ---- discover one representative scan (for attrs like units/site coords) ----\n",
        "            first_scan = None\n",
        "            for _, _row in sub.iterrows():\n",
        "                _sc = _row.get(scan_col, None)\n",
        "                if _sc is not None:\n",
        "                    first_scan = _sc\n",
        "                    break\n",
        "\n",
        "            # Pull units (fallback dBZ) and site coords if we can\n",
        "            _units = \"dBZ\"\n",
        "            _site_lat = _site_lon = _site_alt = np.nan\n",
        "            _is_pseudo_any = False\n",
        "            if first_scan is not None:\n",
        "                try:\n",
        "                    if chosen_field and chosen_field in first_scan.fields:\n",
        "                        _units = first_scan.fields[chosen_field].get(\"units\", _units)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    _site_lat = float(first_scan.latitude[\"data\"][0])\n",
        "                    _site_lon = float(first_scan.longitude[\"data\"][0])\n",
        "                    _site_alt = float(first_scan.altitude[\"data\"][0])\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    _is_pseudo_any = bool(_is_pseudo(first_scan))\n",
        "                except Exception:\n",
        "                    _is_pseudo_any = False\n",
        "\n",
        "            # ---- open file & create datasets ----\n",
        "            t_h5open0 = perf_counter()\n",
        "            with h5py.File(prod_path, \"w\") as h5:\n",
        "                dset = h5.create_dataset(\n",
        "                    \"data\",\n",
        "                    shape=(T, H, W, C),\n",
        "                    dtype=\"float32\",\n",
        "                    chunks=(1, H, W, C),\n",
        "                    compression=\"gzip\",\n",
        "                    compression_opts=4,\n",
        "                    shuffle=True,\n",
        "                    fillvalue=np.nan,\n",
        "                )\n",
        "                dset.attrs[\"units\"] = str(_units)\n",
        "                dset.attrs[\"missing_value\"] = np.nan\n",
        "                dset.attrs[\"valid_range\"] = np.array([-30.0, 95.0], dtype=\"float32\")\n",
        "                dset.attrs[\"description\"] = \"Reflectivity tensor with shape (T,H,W,C) = (time, rays, gates, sweeps).\"\n",
        "\n",
        "                h5.attrs[\"dataset_version\"] = \"1.0.0\"\n",
        "                h5.attrs[\"schema_version\"] = \"1.0.0\"\n",
        "                h5.attrs[\"radar_site\"] = str(site)\n",
        "                h5.attrs[\"storm_id\"] = str(storm)\n",
        "                h5.attrs[\"year\"] = int(yr)\n",
        "                h5.attrs[\"product_prefix\"] = str(prefix)\n",
        "                h5.attrs[\"shape_T_H_W_C\"] = (int(T), int(H), int(W), int(C))\n",
        "                if chosen_field:\n",
        "                    h5.attrs[\"field_key\"] = str(chosen_field)\n",
        "                h5.attrs[\"source_system\"] = \"NEXRAD Level-II\"\n",
        "                h5.attrs[\"grid_type\"] = \"polar_host_reindexed\"\n",
        "                h5.attrs[\"channels_are_sweeps\"] = (not _is_pseudo_any)\n",
        "                h5.attrs[\"composite_method\"] = \"MAX_ALL_SWEEPS\" if _is_pseudo_any else \"NONE\"\n",
        "                h5.attrs[\"site_lat\"] = float(_site_lat)\n",
        "                h5.attrs[\"site_lon\"] = float(_site_lon)\n",
        "                h5.attrs[\"site_alt_m\"] = float(_site_alt)\n",
        "                h5.attrs[\"generation_software\"] = \"hrss 1.0.0\"\n",
        "                h5.attrs[\"license\"] = \"See Zenodo record for license and citation requirements.\"\n",
        "                # NEW: link to context + schema (relative paths)\n",
        "                h5.attrs[\"context_relpath\"] = context_relpath\n",
        "                h5.attrs[\"context_schema_relpath\"] = context_schema_relpath\n",
        "                h5.attrs[\"context_sha256\"] = str(context_sha256 or \"\")\n",
        "\n",
        "\n",
        "                # time vectors\n",
        "                try:\n",
        "                    times_utf8 = [pd.to_datetime(t).strftime(\"%Y-%m-%dT%H:%M:%SZ\") for t in sub[time_col]]\n",
        "                    h5.create_dataset(\"time\", data=np.asarray(times_utf8, dtype=\"S20\"))\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    t_ms = np.array([int(pd.to_datetime(t).value // 1_000_000) for t in sub[time_col]], dtype=\"int64\")\n",
        "                    h5.create_dataset(\"time_unix_ms\", data=t_ms)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                # geometry\n",
        "                az_dset = h5.create_dataset(\n",
        "                    \"azimuth_deg\", shape=(T, H), dtype=\"float32\",\n",
        "                    chunks=(1, H), compression=\"gzip\", compression_opts=4, shuffle=True, fillvalue=np.nan\n",
        "                )\n",
        "                rng_dset = h5.create_dataset(\n",
        "                    \"range_m\", shape=(T, W), dtype=\"float32\",\n",
        "                    chunks=(1, W), compression=\"gzip\", compression_opts=4, shuffle=True, fillvalue=np.nan\n",
        "                )\n",
        "                elv_dset = h5.create_dataset(\n",
        "                    \"elevation_deg\", shape=(T, C), dtype=\"float32\",\n",
        "                    chunks=(1, C), compression=\"gzip\", compression_opts=4, shuffle=True, fillvalue=np.nan\n",
        "                )\n",
        "                host_idx_dset = h5.create_dataset(\n",
        "                    \"azimuth_host_sweep_index\", shape=(T,), dtype=\"int16\",\n",
        "                    chunks=True, compression=\"gzip\", compression_opts=4, shuffle=True\n",
        "                )\n",
        "\n",
        "                # NEW: per-frame bbox datasets\n",
        "                def _get_series(name):\n",
        "                    try:\n",
        "                        return np.asarray(context_df[name].to_numpy(), dtype=np.float32)\n",
        "                    except Exception:\n",
        "                        if debug:\n",
        "                            print(f\"[save][warn] context missing column '{name}', filling NaNs\")\n",
        "                        return np.full((T,), np.nan, dtype=np.float32)\n",
        "\n",
        "                bbox_min_lat = _get_series(\"min_lat\")\n",
        "                bbox_max_lat = _get_series(\"max_lat\")\n",
        "                bbox_min_lon = _get_series(\"min_lon\")\n",
        "                bbox_max_lon = _get_series(\"max_lon\")\n",
        "\n",
        "                h5.create_dataset(\"bbox_min_lat\", data=bbox_min_lat, dtype=\"float32\", chunks=True, compression=\"gzip\", compression_opts=4, shuffle=True)\n",
        "                h5.create_dataset(\"bbox_max_lat\", data=bbox_max_lat, dtype=\"float32\", chunks=True, compression=\"gzip\", compression_opts=4, shuffle=True)\n",
        "                h5.create_dataset(\"bbox_min_lon\", data=bbox_min_lon, dtype=\"float32\", chunks=True, compression=\"gzip\", compression_opts=4, shuffle=True)\n",
        "                h5.create_dataset(\"bbox_max_lon\", data=bbox_max_lon, dtype=\"float32\", chunks=True, compression=\"gzip\", compression_opts=4, shuffle=True)\n",
        "\n",
        "                # provenance\n",
        "                try:\n",
        "                    vlen = h5py.special_dtype(vlen=str)\n",
        "                    srckey_dset = h5.create_dataset(\"source_key\", shape=(T,), dtype=vlen)\n",
        "                except Exception:\n",
        "                    srckey_dset = None\n",
        "\n",
        "                t_h5open1 = perf_counter()\n",
        "                if debug:\n",
        "                    print(f\"[save] HDF open+create: {(t_h5open1 - t_h5open0)*1000:.1f} ms \"\n",
        "                          f\"(chunk=(1,{H},{W},{C}), gzip=4)\")\n",
        "\n",
        "                # snapshot attrs while file is open (no extra I/O later)\n",
        "                if debug:\n",
        "                    file_attrs_snapshot = {k: _pretty_attr(v) for k, v in h5.attrs.items()}\n",
        "                    data_attrs_snapshot = {k: _pretty_attr(v) for k, v in dset.attrs.items()}\n",
        "                    debug_product_attrs.append({\n",
        "                        \"path\": prod_path,\n",
        "                        \"file_attrs\": file_attrs_snapshot,\n",
        "                        \"data_attrs\": data_attrs_snapshot,\n",
        "                    })\n",
        "\n",
        "                # ========================= WRITE FRAMES ===============================\n",
        "                t_write_total0 = perf_counter()\n",
        "                nan_probe_indices = {0, T-1, T//2}  # sample a few frames for NaN frac probe\n",
        "\n",
        "                for i, row in sub.iterrows():\n",
        "                    t_frame0 = perf_counter()\n",
        "                    scan = row.get(scan_col, None)\n",
        "\n",
        "                    cube = np.empty((H, W, C), dtype=np.float32)\n",
        "                    cube[:] = np.nan\n",
        "\n",
        "                    # Defaults for geometry writes at this time index\n",
        "                    H_host = 0\n",
        "                    W_common = 0\n",
        "                    host_idx = 0\n",
        "                    host_az = None\n",
        "                    rng = None\n",
        "                    lim_sweeps_written = 0\n",
        "\n",
        "                    if scan is not None:\n",
        "                        if chosen_field is None:\n",
        "                            chosen_field = _choose_field_key(scan, prefix)\n",
        "\n",
        "                        if _is_pseudo(scan):\n",
        "                            try:\n",
        "                                fld = scan.fields[chosen_field][\"data\"]\n",
        "                                if not isinstance(fld, ma.MaskedArray):\n",
        "                                    fld = ma.MaskedArray(fld, mask=np.zeros_like(fld, dtype=bool))\n",
        "                                s0 = int(scan.sweep_start_ray_index[\"data\"][0])\n",
        "                                e0 = int(scan.sweep_end_ray_index[\"data\"][0])\n",
        "                                rng = np.asarray(scan.range[\"data\"], dtype=np.float32)\n",
        "                                W_common = min(W, (rng.shape[0] if rng is not None else W))\n",
        "                                host_block = fld[s0:e0, :W_common].filled(np.nan).astype(np.float32, copy=False)\n",
        "                                h_host = min(H, host_block.shape[0])\n",
        "                                w_host = min(W, host_block.shape[1])\n",
        "                                cube[:h_host, :w_host, 0] = host_block[:h_host, :w_host]\n",
        "                                host_idx = 0\n",
        "                                host_az = np.asarray(scan.azimuth[\"data\"][s0:e0], dtype=np.float32)\n",
        "                                H_host = min(H, host_az.size)\n",
        "                                lim_sweeps_written = 1\n",
        "                            except Exception as e:\n",
        "                                if debug:\n",
        "                                    print(f\"[save][warn] pseudo write failed at t={i}: {e}\")\n",
        "\n",
        "                            # elevation: single value\n",
        "                            try:\n",
        "                                el0 = float(scan.fixed_angle[\"data\"][0])\n",
        "                            except Exception:\n",
        "                                el0 = float(np.nanmean(np.asarray(scan.elevation[\"data\"][s0:e0], dtype=float))) if H_host > 0 else np.nan\n",
        "                            elv_dset[i, 0] = el0\n",
        "\n",
        "                        else:\n",
        "                            # --- per-time host geometry for multi-tilt ---\n",
        "                            try:\n",
        "                                sidx = scan.sweep_start_ray_index[\"data\"].astype(int)\n",
        "                                eidx = scan.sweep_end_ray_index[\"data\"].astype(int)\n",
        "                                nsweeps_t = int(len(sidx))\n",
        "                            except Exception as e:\n",
        "                                if debug:\n",
        "                                    print(f\"[save][warn] sweep bounds missing at t={i}: {e}\")\n",
        "                                nsweeps_t = 0\n",
        "                                sidx = eidx = np.array([], dtype=int)\n",
        "\n",
        "                            if nsweeps_t > 0:\n",
        "                                rays_per = [int(max(0, eidx[j] - sidx[j])) for j in range(nsweeps_t)]\n",
        "                                host_idx = int(np.argmax(rays_per))\n",
        "                                hs, he = sidx[host_idx], eidx[host_idx]\n",
        "\n",
        "                                host_az = np.unwrap(np.deg2rad(np.asarray(scan.azimuth[\"data\"][hs:he], dtype=float)))\n",
        "                                H_host = min(H, host_az.size)\n",
        "\n",
        "                                try:\n",
        "                                    rng = np.asarray(scan.range[\"data\"], dtype=np.float32)\n",
        "                                except Exception:\n",
        "                                    rng = None\n",
        "                                W_common = min(W, (rng.shape[0] if rng is not None else W))\n",
        "\n",
        "                                # Elevations per sweep (for first C channels)\n",
        "                                try:\n",
        "                                    fixed_angles = np.asarray(scan.fixed_angle[\"data\"], dtype=np.float32)\n",
        "                                except Exception:\n",
        "                                    try:\n",
        "                                        fixed_angles = np.array([\n",
        "                                            float(np.nanmean(np.asarray(scan.elevation[\"data\"][sidx[j]:eidx[j]], dtype=float)))\n",
        "                                            for j in range(nsweeps_t)\n",
        "                                        ], dtype=np.float32)\n",
        "                                    except Exception:\n",
        "                                        fixed_angles = None\n",
        "                                if fixed_angles is not None and fixed_angles.size > 0:\n",
        "                                    elv_dset[i, :min(C, fixed_angles.size)] = fixed_angles[:min(C, fixed_angles.size)]\n",
        "\n",
        "                                lim_sweeps = min(C, nsweeps_t)\n",
        "                                lim_sweeps_written = lim_sweeps\n",
        "\n",
        "                                for c in range(lim_sweeps):\n",
        "                                    s, e = int(sidx[c]), int(eidx[c])\n",
        "                                    # source azimuths\n",
        "                                    src_az = np.unwrap(np.deg2rad(np.asarray(scan.azimuth[\"data\"][s:e], dtype=float)))\n",
        "\n",
        "                                    # 0/2Ï€-aware nearest-neighbor to host rays\n",
        "                                    two_pi = 2.0 * np.pi\n",
        "                                    k = int(np.round((host_az.mean() - src_az.mean()) / two_pi))\n",
        "                                    src_u = src_az + k * two_pi\n",
        "                                    order_idx = np.argsort(src_u)\n",
        "                                    src_sorted = np.maximum.accumulate(src_u[order_idx])\n",
        "                                    idx_sorted = order_idx\n",
        "                                    src_ext = np.concatenate([src_sorted - two_pi, src_sorted, src_sorted + two_pi])\n",
        "                                    idx_ext = np.concatenate([idx_sorted,          idx_sorted,          idx_sorted])\n",
        "                                    j = np.searchsorted(src_ext, host_az[:H_host])\n",
        "                                    j0 = np.clip(j - 1, 0, src_ext.size - 1)\n",
        "                                    j1 = np.clip(j,       0, src_ext.size - 1)\n",
        "                                    pick = np.where(np.abs(host_az[:H_host] - src_ext[j0]) <=\n",
        "                                                    np.abs(host_az[:H_host] - src_ext[j1]), j0, j1)\n",
        "                                    src_idx = idx_ext[pick]\n",
        "                                    src_idx = np.clip(src_idx, 0, max(0, (e - s) - 1))\n",
        "\n",
        "                                    # slice & map gates to host rays (use common W)\n",
        "                                    fld = scan.fields[chosen_field][\"data\"]\n",
        "                                    if not isinstance(fld, ma.MaskedArray):\n",
        "                                        fld = ma.MaskedArray(fld, mask=np.zeros_like(fld, dtype=bool))\n",
        "                                    field = fld[s:e, :W_common]\n",
        "                                    ch_re = field[src_idx, :]\n",
        "                                    cube[:H_host, :W_common, c] = ch_re.filled(np.nan).astype(np.float32, copy=False)\n",
        "                            else:\n",
        "                                if debug:\n",
        "                                    print(f\"[save][warn] no sweeps at t={i}\")\n",
        "\n",
        "                    # write main tensor\n",
        "                    dset[i, :, :, :] = cube\n",
        "\n",
        "                    # write geometry/provenance\n",
        "                    if host_az is not None and H_host > 0:\n",
        "                        az_dset[i, :H_host] = np.degrees(host_az[:H_host]).astype(np.float32, copy=False)\n",
        "                    if rng is not None and W_common > 0:\n",
        "                        rng_dset[i, :W_common] = rng[:W_common]\n",
        "                    host_idx_dset[i] = int(host_idx)\n",
        "\n",
        "                    if srckey_dset is not None:\n",
        "                        keycol = f\"{prefix}_matched_volume_s3_key\"\n",
        "                        try:\n",
        "                            srckey_dset[i] = str(row.get(keycol, \"\") or \"\")\n",
        "                        except Exception:\n",
        "                            srckey_dset[i] = \"\"\n",
        "\n",
        "                    # ---- per-frame debug summary (cheap) ----\n",
        "                    if debug and (i in nan_probe_indices or i < 3):\n",
        "                        view = cube\n",
        "                        n_tot = view.size\n",
        "                        n_nan = int(np.isnan(view).sum())\n",
        "                        frac_nan = n_nan / n_tot if n_tot else 0.0\n",
        "                        print(f\"[save] t={i:04d} \"\n",
        "                              f\"H_host={H_host} W_common={W_common} host_idx={host_idx} \"\n",
        "                              f\"channels_written={lim_sweeps_written} NaN_frac={frac_nan:.3f} \"\n",
        "                              f\"frame_write={(perf_counter()-t_frame0)*1000:.1f} ms\")\n",
        "\n",
        "                t_write_total1 = perf_counter()\n",
        "                if debug:\n",
        "                    print(f\"[save] wrote {T} frames in {(t_write_total1 - t_write_total0):.3f} s \"\n",
        "                          f\"(avg {(t_write_total1 - t_write_total0)*1000/T:.1f} ms/frame)\")\n",
        "\n",
        "            # ---- after closing file, report size ----\n",
        "            t_prod1 = perf_counter()\n",
        "            if debug:\n",
        "                try:\n",
        "                    fsz = os.path.getsize(prod_path)\n",
        "                    print(f\"[save] closed '{prefix}' file in {(t_prod1 - t_prod0):.3f} s, size={_fmt_bytes(fsz)}\")\n",
        "                except Exception:\n",
        "                    print(f\"[save] closed '{prefix}' file in {(t_prod1 - t_prod0):.3f} s\")\n",
        "\n",
        "            saved.append({\"storm_dir\": storm_dir, \"product\": prefix, \"path\": prod_path})\n",
        "\n",
        "        # encourage GC per storm\n",
        "        t_gc0 = perf_counter()\n",
        "        gc.collect()\n",
        "        t_gc1 = perf_counter()\n",
        "        if debug:\n",
        "            print(f\"[save] GC after group: {(t_gc1 - t_gc0)*1000:.1f} ms\")\n",
        "            print(f\"[save] ===== End Group: site={site} storm={storm} ===== \"\n",
        "                  f\"(elapsed {(perf_counter() - t_group0):.3f} s)\")\n",
        "\n",
        "    # ---------- optionally drop in-memory scans ----------\n",
        "    if drop_scans_after_save:\n",
        "        t_drop0 = perf_counter()\n",
        "        if debug:\n",
        "            print(\"[save] dropping in-memory *_scan objects from dataframe.\")\n",
        "        for p in prefixes:\n",
        "            col = f\"{p}_scan\"\n",
        "            if col in df.columns:\n",
        "                df[col] = None\n",
        "        gc.collect()\n",
        "        t_drop1 = perf_counter()\n",
        "        if debug:\n",
        "            print(f\"[save] drop scans + GC: {(t_drop1 - t_drop0)*1000:.1f} ms\")\n",
        "\n",
        "    # ===== FINAL DEBUG DUMP (no file reads): tensor HDF attrs + one-row context preview =====\n",
        "    if debug:\n",
        "        base_abs = os.path.abspath(base_dir)\n",
        "        print(f\"[save] completed. Wrote context + products for {len(groups)} storm group(s) into {base_abs}\")\n",
        "        print(f\"[save] TOTAL elapsed: {(perf_counter() - t_all0):.3f} s\")\n",
        "\n",
        "        print(\"\\n[save] ===== DEBUG: Tensor HDF Attributes (captured in-memory) =====\")\n",
        "        for snap in debug_product_attrs:\n",
        "            print(f\"[save] --- {snap['path']} ---\")\n",
        "            print(\"[save] file attrs:\")\n",
        "            for k in sorted(snap[\"file_attrs\"].keys()):\n",
        "                print(f\"[save]   {k} = {snap['file_attrs'][k]!r}\")\n",
        "            print(\"[save] /data attrs:\")\n",
        "            for k in sorted(snap[\"data_attrs\"].keys()):\n",
        "                print(f\"[save]   {k} = {snap['data_attrs'][k]!r}\")\n",
        "\n",
        "        print(\"\\n[save] ===== DEBUG: Context HDF Columns (first row values; captured in-memory) =====\")\n",
        "        for ctx in debug_context_peek:\n",
        "            print(f\"[save] --- {ctx['context_path']} ---\")\n",
        "            cols = ctx[\"columns\"]\n",
        "            row  = ctx[\"row\"]\n",
        "            print(f\"[save] columns={len(cols)}\")\n",
        "            for col in cols:\n",
        "                val = row.get(col, None)\n",
        "                try:\n",
        "                    if isinstance(val, (pd.Timestamp, np.datetime64)):\n",
        "                        sval = pd.to_datetime(val).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                    elif isinstance(val, (float, np.floating)):\n",
        "                        sval = f\"{float(val):.6g}\"\n",
        "                    else:\n",
        "                        sval = str(val)\n",
        "                except Exception:\n",
        "                    sval = repr(val)\n",
        "                print(f\"[save]   {col}: {sval}\")\n",
        "\n",
        "    return saved\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRiNoLb_Xchb"
      },
      "source": [
        "#### **Build Provenance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxk6krn9XoWX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------- small helpers -------------------\n",
        "\n",
        "def _utc_iso(ts):\n",
        "    return datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "def _sha256(path, chunk=1024*1024):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for b in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _read_context_sha_from_schema(schema_path, expected_fname=None):\n",
        "    \"\"\"\n",
        "    Read applies_to_sha256 from a context schema JSON if it matches the file.\n",
        "    Returns sha (str) or None.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(schema_path):\n",
        "        return None\n",
        "    try:\n",
        "        with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            js = json.load(f)\n",
        "        applies = js.get(\"applies_to\", \"\")\n",
        "        sha = (js.get(\"applies_to_sha256\", \"\") or \"\").strip()\n",
        "        if expected_fname and applies and applies != expected_fname:\n",
        "            return None\n",
        "        if len(sha) == 64 and all(c in \"0123456789abcdefABCDEF\" for c in sha):\n",
        "            return sha.lower()\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "_fname_re = re.compile(\n",
        "    r\"^(?P<site>[A-Z0-9]{4})_(?P<storm>\\d+?)_\"\n",
        "    r\"(?P<kind>context|[a-zA-Z0-9_]+?)\"\n",
        "    r\"(?:_T(?P<T>\\d+))?\"\n",
        "    r\"(?:_(?P<H>\\d+)x(?P<W>\\d+)x(?P<C>\\d+)ch)?\"\n",
        "    r\"_(?P<t0>\\d{8}T\\d{6}Z)_(?P<t1>\\d{8}T\\d{6}Z)\\.h5$\"\n",
        ")\n",
        "\n",
        "def _parse_file_bits(fname):\n",
        "    m = _fname_re.match(fname)\n",
        "    if not m:\n",
        "        return {}\n",
        "    d = m.groupdict()\n",
        "    for k in (\"storm\",\"T\",\"H\",\"W\",\"C\"):\n",
        "        if d.get(k) is not None:\n",
        "            d[k] = int(d[k])\n",
        "    return d\n",
        "\n",
        "def _rel(root, path):\n",
        "    try:\n",
        "        return os.path.relpath(path, start=root)\n",
        "    except Exception:\n",
        "        return os.path.basename(path)\n",
        "\n",
        "def _debug_preview_csv(path: str, label: str):\n",
        "    \"\"\"Print header + first row, like your context preview style.\"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader, None)\n",
        "            print(f\"\\n[catalog] ===== DEBUG: {label} (columns + first row) =====\")\n",
        "            print(f\"[catalog] --- {path} ---\")\n",
        "            if not header:\n",
        "                print(\"[catalog][warn] empty file or missing header\")\n",
        "                return\n",
        "            print(f\"[catalog] columns={len(header)}\")\n",
        "            row = next(reader, None)\n",
        "            if row is None:\n",
        "                print(\"[catalog][warn] no data rows\")\n",
        "                for col in header:\n",
        "                    print(f\"[catalog]   {col}: <NA>\")\n",
        "                return\n",
        "            for col, val in zip(header, row):\n",
        "                print(f\"[catalog]   {col}: {'' if val is None else str(val)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[catalog][warn] failed preview for {label}: {e}\")\n",
        "\n",
        "# ------------------- main builder -------------------\n",
        "\n",
        "def build_year_manifest_and_catalog(year_dir: str,\n",
        "                                    manifest_name: str = \"manifest.csv\",\n",
        "                                    catalog_name: str = \"catalog.csv\",\n",
        "                                    update_schema_checksums: bool = False,\n",
        "                                    # kept for backward compatibility; ignored (we never write sidecars)\n",
        "                                    write_missing_sidecars: bool = False,\n",
        "                                    debug: bool = True):\n",
        "    \"\"\"\n",
        "    Produce:\n",
        "      - manifest.csv (minimal integrity view)\n",
        "      - catalog.csv  (event-level exploration view)\n",
        "\n",
        "    SHA policy (no sidecars):\n",
        "      * context_hdf â†’ prefer schema's applies_to_sha256 (if filename matches), else compute.\n",
        "      * product_hdf/other â†’ compute.\n",
        "\n",
        "    We skip manifest/catalog themselves and any lingering '*.sha256' files.\n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(f\"[catalog] scanning year_dir={year_dir}\")\n",
        "\n",
        "    manifest_rows = []\n",
        "    events = {}  # (site, storm) -> dict\n",
        "\n",
        "    for root, dirs, files in os.walk(year_dir):\n",
        "        for fn in files:\n",
        "            # Skip our outputs and old sidecars\n",
        "            if fn == manifest_name or fn == catalog_name or fn.endswith(\".sha256\"):\n",
        "                continue\n",
        "\n",
        "            path = os.path.join(root, fn)\n",
        "            rel  = _rel(year_dir, path)\n",
        "            try:\n",
        "                size = os.path.getsize(path)\n",
        "                mtime_iso = _utc_iso(os.path.getmtime(path))\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "\n",
        "            lower = fn.lower()\n",
        "            if lower.endswith(\".schema.json\"):\n",
        "                ftype = \"context_schema\"\n",
        "            elif lower.endswith(\".h5\"):\n",
        "                ftype = \"context_hdf\" if \"_context_\" in fn else \"product_hdf\"\n",
        "            elif lower.endswith(\".pkl\"):\n",
        "                ftype = \"context_pkl\" if \"_context_\" in fn else \"other_pkl\"\n",
        "            else:\n",
        "                ftype = \"other\"\n",
        "\n",
        "            bits = _parse_file_bits(fn) if ftype in (\"context_hdf\",\"product_hdf\") else {}\n",
        "\n",
        "            # ---------- SHA (no sidecars) ----------\n",
        "            if ftype == \"context_hdf\":\n",
        "                schema_path = os.path.join(root, os.path.splitext(fn)[0] + \".schema.json\")\n",
        "                sha = _read_context_sha_from_schema(schema_path, expected_fname=fn) or _sha256(path)\n",
        "                if update_schema_checksums and os.path.exists(schema_path):\n",
        "                    try:\n",
        "                        with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                            js = json.load(f)\n",
        "                    except Exception:\n",
        "                        js = {}\n",
        "                    js[\"applies_to\"] = fn\n",
        "                    js[\"applies_to_sha256\"] = sha\n",
        "                    try:\n",
        "                        with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                            json.dump(js, f, indent=2, sort_keys=True)\n",
        "                        if debug:\n",
        "                            print(f\"[catalog] updated schema checksum: {_rel(year_dir, schema_path)}\")\n",
        "                    except Exception as e:\n",
        "                        if debug:\n",
        "                            print(f\"[catalog][warn] failed updating schema for {schema_path}: {e}\")\n",
        "            else:\n",
        "                sha = _sha256(path)\n",
        "\n",
        "            # ---------- manifest row (trimmed) ----------\n",
        "            # NOTE: drop T,H,W,C,t0_utc,t1_utc here\n",
        "            row = {\n",
        "                \"relpath\": rel,\n",
        "                \"file_type\": ftype,\n",
        "                \"size_bytes\": size,\n",
        "                \"sha256\": sha,\n",
        "                \"mtime_utc\": mtime_iso,\n",
        "                \"site\": bits.get(\"site\",\"\"),\n",
        "                \"storm_id\": bits.get(\"storm\",\"\"),\n",
        "                \"kind\": bits.get(\"kind\",\"\"),\n",
        "            }\n",
        "            manifest_rows.append(row)\n",
        "\n",
        "            # ---------- event aggregation (catalog) ----------\n",
        "            if ftype in (\"context_hdf\",\"product_hdf\"):\n",
        "                site = bits.get(\"site\")\n",
        "                storm = bits.get(\"storm\")\n",
        "                if site and storm is not None:\n",
        "                    key = (site, int(storm))\n",
        "                    ev = events.setdefault(key, {\n",
        "                        \"site\": site,\n",
        "                        \"storm_id\": int(storm),\n",
        "                        \"storm_dir\": _rel(year_dir, root),\n",
        "                        # we'll keep these in memory to populate t0/t1/T/products/dims,\n",
        "                        # but we won't write context_relpath/context_sha256/product_files to CSV\n",
        "                        \"context_relpath\": \"\",\n",
        "                        \"context_sha256\": \"\",\n",
        "                        \"t0_utc\": \"\",\n",
        "                        \"t1_utc\": \"\",\n",
        "                        \"T\": \"\",\n",
        "                        \"products\": [],\n",
        "                        \"product_files\": [],\n",
        "                        \"product_dims\": [],\n",
        "                        \"total_bytes\": 0,\n",
        "                    })\n",
        "                    ev[\"total_bytes\"] += size\n",
        "\n",
        "                    if ftype == \"context_hdf\":\n",
        "                        ev[\"context_relpath\"] = rel  # kept internal only\n",
        "                        ev[\"context_sha256\"] = sha   # kept internal only\n",
        "                        ev[\"t0_utc\"] = bits.get(\"t0\",\"\")\n",
        "                        ev[\"t1_utc\"] = bits.get(\"t1\",\"\")\n",
        "                        ev[\"T\"] = bits.get(\"T\",\"\")\n",
        "\n",
        "                    elif ftype == \"product_hdf\":\n",
        "                        prefix = bits.get(\"kind\",\"\") or \"\"\n",
        "                        if prefix and prefix not in ev[\"products\"]:\n",
        "                            ev[\"products\"].append(prefix)\n",
        "                        ev[\"product_files\"].append(rel)  # kept internal only\n",
        "                        if all(bits.get(k) for k in (\"H\",\"W\",\"C\")):\n",
        "                            ev[\"product_dims\"].append(f\"{prefix}:{bits['H']}x{bits['W']}x{bits['C']}\")\n",
        "\n",
        "    # ---------- write manifest.csv (overwrite; minimal) ----------\n",
        "    manifest_path = os.path.join(year_dir, manifest_name)\n",
        "    manifest_fields = [\n",
        "        \"relpath\",\"file_type\",\"size_bytes\",\"sha256\",\"mtime_utc\",\n",
        "        \"site\",\"storm_id\",\"kind\"\n",
        "    ]\n",
        "    with open(manifest_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=manifest_fields)\n",
        "        w.writeheader()\n",
        "        for r in sorted(manifest_rows, key=lambda x: x[\"relpath\"]):\n",
        "            w.writerow(r)\n",
        "\n",
        "    # ---------- write catalog.csv (overwrite; trimmed) ----------\n",
        "    # NOTE: drop context_relpath, context_sha256, product_files here\n",
        "    catalog_path = os.path.join(year_dir, catalog_name)\n",
        "    catalog_fields = [\n",
        "        \"site\",\"storm_id\",\"storm_dir\",\n",
        "        \"t0_utc\",\"t1_utc\",\"T\",\n",
        "        \"n_products\",\"products\",\"dims\",\n",
        "        \"total_bytes\"\n",
        "    ]\n",
        "    with open(catalog_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=catalog_fields)\n",
        "        w.writeheader()\n",
        "        for (site, storm), ev in sorted(events.items(), key=lambda k: (k[0][0], k[0][1])):\n",
        "            w.writerow({\n",
        "                \"site\": ev[\"site\"],\n",
        "                \"storm_id\": ev[\"storm_id\"],\n",
        "                \"storm_dir\": ev[\"storm_dir\"],\n",
        "                \"t0_utc\": ev[\"t0_utc\"],\n",
        "                \"t1_utc\": ev[\"t1_utc\"],\n",
        "                \"T\": ev[\"T\"],\n",
        "                \"n_products\": len(ev[\"products\"]),\n",
        "                \"products\": \",\".join(sorted(ev[\"products\"])),\n",
        "                \"dims\": \";\".join(sorted(ev[\"product_dims\"])),\n",
        "                \"total_bytes\": ev[\"total_bytes\"],\n",
        "            })\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[catalog] wrote manifest â†’ {manifest_path}\")\n",
        "        print(f\"[catalog] wrote catalog  â†’ {catalog_path}\")\n",
        "        print(f\"[catalog] events={len(events)}, files={len(manifest_rows)}\")\n",
        "        _debug_preview_csv(manifest_path, \"manifest preview\")\n",
        "        _debug_preview_csv(catalog_path, \"catalog preview\")\n",
        "\n",
        "    return manifest_path, catalog_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcsRlrL9WUna"
      },
      "source": [
        "#### **Main Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfUmWoImWUna"
      },
      "outputs": [],
      "source": [
        "import s3fs\n",
        "import cProfile\n",
        "import pstats\n",
        "import threading\n",
        "import os, gc, ctypes, tempfile\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "import sys, traceback, faulthandler, warnings\n",
        "import queue as _q\n",
        "import signal\n",
        "\n",
        "from threading import Thread\n",
        "from IPython.display import display\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "\n",
        "faulthandler.enable()\n",
        "\n",
        "\n",
        "# Process one SID on a child process after GR-S is linked, to keep RAM usage down\n",
        "def _process_one_group(group_pkl_path, radar_info, training_base, debug_flag, sid, errq):\n",
        "    import pandas as pd\n",
        "    import s3fs, gc, ctypes, os, sys, traceback, warnings\n",
        "\n",
        "    sys.stdout = _QueueStream(errq, \"log\", sid)\n",
        "    sys.stderr = _QueueStream(errq, \"err\", sid)\n",
        "\n",
        "    def _showwarn(message, category, filename, lineno, file=None, line=None):\n",
        "        try:\n",
        "            errq.put_nowait((\"warn\", f\"[child sid={sid}] {category.__name__}: {message} ({filename}:{lineno})\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    warnings.showwarning = _showwarn\n",
        "\n",
        "    # (optionally) make Python unbuffered semantics\n",
        "    try:\n",
        "        import os\n",
        "        os.environ.setdefault(\"PYTHONUNBUFFERED\", \"1\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    def log(msg):\n",
        "        try:\n",
        "            errq.put((\"log\", f\"[child sid={sid} pid={os.getpid()}] {msg}\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        fs = s3fs.S3FileSystem(\n",
        "            anon=True, skip_instance_cache=True,\n",
        "            use_listings_cache=False, default_cache_type=None\n",
        "        )\n",
        "\n",
        "        if debug_flag: log(\"[_process_one_group] starting\")\n",
        "        group = pd.read_pickle(group_pkl_path)\n",
        "\n",
        "        if debug_flag: log(f\"[_process_one_group] loaded group shape={group.shape}\")\n",
        "\n",
        "\n",
        "        ############################################################ LINK GRS TO RADAR SCANS ############################################################\n",
        "\n",
        "\n",
        "        if debug_flag: log(\"[_process_one_group] calling find_radar_scans()\")\n",
        "        linked_radar_df = find_radar_scans(\n",
        "            group, site_column=\"radar_site\",\n",
        "            time_column=\"time\",\n",
        "            level2_base=\"unidata-nexrad-level2\",\n",
        "            cache_dir=\"Datasets/nexrad_datasets/level_two_raw\",\n",
        "            product_filter=[\"reflectivity\"],\n",
        "            time_tolerance_seconds=29,\n",
        "            keep_in_memory=True,\n",
        "            debug=False\n",
        "        )\n",
        "        if debug_flag: log(f\"[_process_one_group] linked_radar_df shape={linked_radar_df.shape}\")\n",
        "\n",
        "\n",
        "        ################################################################## BUILD BBOX ############################################################\n",
        "\n",
        "\n",
        "        if debug_flag: log(\"[_process_one_group] calling build_bboxes_for_linked_df()\")\n",
        "        plot_dir = os.path.join(\"Logs\", \"plots\", f\"bbox_sid{sid}_pid{os.getpid()}\")\n",
        "        bboxed_df = build_bboxes_for_linked_df(\n",
        "            linked_radar_df,\n",
        "            class_field='reflectivity',\n",
        "            threshold=20,\n",
        "            min_size=6000,\n",
        "            pad_km=5.0,\n",
        "            grid_res_m=250.0,\n",
        "            buffer_km=5.0,\n",
        "            include_nearby_km=3.0,\n",
        "            debug=False,\n",
        "            debug_plot_dir=plot_dir,\n",
        "            debug_plot_limit=0       # Default is two plots per storm, set to zero when building dataset\n",
        "\n",
        "        )\n",
        "        if debug_flag: log(f\"[_process_one_group] bboxed_df shape={bboxed_df.shape}\")\n",
        "\n",
        "\n",
        "        ################################################################ SAVE FOR TRAINING ############################################################\n",
        "\n",
        "\n",
        "        if debug_flag: log(\"[_process_one_group] calling save_df_for_training()\")\n",
        "        save_df_for_training(\n",
        "            df=bboxed_df,\n",
        "            base_dir=training_base,    # Now a top-level parameter when main_pipeline is called\n",
        "            year_override=None,\n",
        "            radar_site_col=\"radar_site\",\n",
        "            storm_id_col=\"storm_id\",\n",
        "            time_col=\"time\",\n",
        "            debug=False,\n",
        "            drop_scans_after_save=True # Should ALWAYS be true (free up memory)\n",
        "        )\n",
        "        if debug_flag: log(\"[_process_one_group] done\")\n",
        "\n",
        "\n",
        "        ###############################################################################################################################################\n",
        "\n",
        "\n",
        "    except Exception:\n",
        "        tb = traceback.format_exc()\n",
        "        try:\n",
        "            errq.put((\"exc\", f\"[child sid={sid}] EXCEPTION\\n{tb}\"))\n",
        "        finally:\n",
        "            sys.exit(1)\n",
        "\n",
        "    finally:\n",
        "        try: fs.invalidate_cache()\n",
        "        except Exception: pass\n",
        "        try: fs.close()\n",
        "        except Exception: pass\n",
        "        try: s3fs.S3FileSystem.clear_instance_cache()\n",
        "        except Exception: pass\n",
        "        gc.collect()\n",
        "        try: ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "        except Exception: pass\n",
        "\n",
        "\n",
        "\n",
        "def _run_group_in_child(sid, group, radar_info, training_base, debug):\n",
        "    p = None\n",
        "    errq = None\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as td:\n",
        "            pkl_path = os.path.join(td, f\"group_{sid}.pkl\")\n",
        "            group.to_pickle(pkl_path, protocol=5)\n",
        "\n",
        "            ctx = mp.get_context(\"fork\")\n",
        "            errq = ctx.Queue(maxsize=1000)\n",
        "\n",
        "            p = ctx.Process(\n",
        "                target=_process_one_group,\n",
        "                args=(pkl_path, radar_info, training_base, debug, sid, errq)\n",
        "            )\n",
        "            p.start()\n",
        "\n",
        "            # stream logs while child runs\n",
        "            while True:\n",
        "                try:\n",
        "                    kind, msg = errq.get(timeout=0.5)\n",
        "                    print(msg, flush=True)\n",
        "                except _q.Empty:\n",
        "                    pass\n",
        "                if not p.is_alive():\n",
        "                    break\n",
        "\n",
        "            p.join()\n",
        "\n",
        "            # drain anything left\n",
        "            try:\n",
        "                while True:\n",
        "                    kind, msg = errq.get_nowait()\n",
        "                    print(msg, flush=True)\n",
        "            except _q.Empty:\n",
        "                pass\n",
        "\n",
        "            if p.exitcode != 0:\n",
        "                print(f\"[main_pipeline] SID {sid} child exitcode={p.exitcode}\", flush=True)\n",
        "                raise RuntimeError(f\"Child for SID {sid} failed with exit code {p.exitcode}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        # Explicit, deterministic cleanup on Ctrl-C\n",
        "        if p and p.is_alive():\n",
        "            try:\n",
        "                p.terminate()             # SIGTERM\n",
        "                p.join(timeout=5)\n",
        "                if p.is_alive():\n",
        "                    os.kill(p.pid, signal.SIGKILL)  # hard kill if needed\n",
        "                    p.join()\n",
        "            except Exception:\n",
        "                pass\n",
        "        raise  # re-propagate so outer finally runs\n",
        "\n",
        "    finally:\n",
        "        # tidy queue resources even if interrupted\n",
        "        if errq is not None:\n",
        "            try:\n",
        "                # best effort drain to avoid join_thread hang\n",
        "                try:\n",
        "                    while True:\n",
        "                        kind, msg = errq.get_nowait()\n",
        "                        print(msg, flush=True)\n",
        "                except _q.Empty:\n",
        "                    pass\n",
        "                errq.close()\n",
        "                errq.join_thread()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################################################################################################\n",
        "#                                                                        MAIN PIPELINE\n",
        "##########################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def main_pipeline(debug_flag, year, radar_info, train_rewrite: bool = False, training_base: str = \"Datasets/training_datasets/level_two\"):\n",
        "    # Unpack year into start and end datetime objects in the format YYYY, MM, DD\n",
        "    start = date(year, 1, 1)\n",
        "    end   = date(year, 12, 31)\n",
        "\n",
        "    # Pre-index already-finished storms (fast, one-time)\n",
        "    existing_index = None\n",
        "    if not train_rewrite:\n",
        "        existing_index = build_saved_storm_index(training_base, debug=debug_flag)\n",
        "\n",
        "    # Start plot thread for concurrent plotting\n",
        "    plot_thread = Thread(target=_plotter_loop, daemon=True)\n",
        "    plot_thread.start()\n",
        "\n",
        "    # Wrap everything in a try-finally block to close plotting queuing / threads when finished\n",
        "    try:\n",
        "        ################################################################### LOAD DATASETS ###############################################################################\n",
        "\n",
        "\n",
        "        # Load lsr-iasate reports\n",
        "        lsr_df = load_raw_lsr(\n",
        "            start=start,\n",
        "            end  =end,\n",
        "            debug=False,\n",
        "            cache_dir=\"Datasets/surface_obs_datasets/lsr_raw\",\n",
        "            force_refresh=False,  # If true, then reload cached files (set to false / remove param entirely later)\n",
        "        )\n",
        "        if debug_flag:\n",
        "            print(f\"\\n lsr_df shape: {lsr_df.shape} \\n\")\n",
        "            display(lsr_df.head())\n",
        "\n",
        "\n",
        "        # Load spc reports     NOTE -> remember to actually get the 2017_wind.csv after block comes off\n",
        "        spc_df = load_raw_spc(\n",
        "            start=start,\n",
        "            end  =end,\n",
        "            debug=False\n",
        "        )\n",
        "        if debug_flag:\n",
        "            print(f\"\\n spc_df shape: {spc_df.shape} \\n\")\n",
        "            display(spc_df.head())\n",
        "\n",
        "\n",
        "        ################################################################ LOAD GR-S TRACKS  ###############################################################################\n",
        "\n",
        "\n",
        "        # Load gr-s tracks\n",
        "        grs_df = load_grs_tracks(\n",
        "            year=year,\n",
        "            radar_info=radar_info,\n",
        "            base_url=\"https://data-osdf.rda.ucar.edu/ncar/rda/d841006/tracks\",\n",
        "            min_rows=60, # Storm must spend [min_rows] minutes within max_distance_km of a given radar\n",
        "            max_distance_km=250,\n",
        "            debug=False,\n",
        "            timeout=10, # How much seconds before a request is skipped\n",
        "            save_dir=\"Datasets/cell_tracks/raw_grs\",\n",
        "            max_gap_hours=6.0 # Highest allowable time-discontinuity for rows in storm_id\n",
        "        )\n",
        "        if debug_flag:\n",
        "            print(f\"\\n grs_df shape: {grs_df.shape} \\n\")\n",
        "            display(grs_df.head(2000))\n",
        "\n",
        "\n",
        "        ################################################################ LINK GR-S TRACKS  ###############################################################################\n",
        "\n",
        "\n",
        "        # Link gr-s tracks to radar scan\n",
        "        grouped = grs_df.groupby(\"storm_id\")\n",
        "\n",
        "\n",
        "        # Run the child subprocesses\n",
        "        for sid, group in grouped:\n",
        "            # BREAK AFTER PROCESSING ONE STORM (FOR TESTING)\n",
        "            # break\n",
        "\n",
        "            if debug_flag:\n",
        "                site_col = \"radar_site\"\n",
        "                print(f\"[main_pipeline] storm_id={sid} with {len(group)} rows; site(s): {group[site_col].unique().tolist()}\")\n",
        "\n",
        "            # Compute the year for this group (robust)\n",
        "            try:\n",
        "                row_year = int(pd.to_datetime(group[\"time\"].iloc[0]).year)\n",
        "            except Exception:\n",
        "                row_year = int(year)  # fallback to pipeline's year arg\n",
        "\n",
        "            # Fast skip if (year, SID) already saved (unless rewrite=True)\n",
        "            if should_skip_sid(row_year, int(sid), existing_index, rewrite=train_rewrite, debug=False):\n",
        "                continue\n",
        "\n",
        "            # If not saved, then start the radar scan and bbox process\n",
        "            try:\n",
        "                _run_group_in_child(\n",
        "                    sid=sid,\n",
        "                    group=group,\n",
        "                    radar_info=radar_info,\n",
        "                    debug=False,\n",
        "                    training_base=training_base)\n",
        "            except Exception as e:\n",
        "                if debug_flag:\n",
        "                    print(f\"[main_pipeline] SID {sid} child failed: {e}\")\n",
        "                continue # Skip and move on\n",
        "\n",
        "\n",
        "\n",
        "        ########################################################## BUILD DATASET-WIDE PROVENANCE ############################################################################\n",
        "\n",
        "\n",
        "        # Build manifest and catalog csvs\n",
        "        year_dir = os.path.join(training_base, str(year))\n",
        "        build_year_manifest_and_catalog(\n",
        "            year_dir,\n",
        "            update_schema_checksums=True,  # set True once to backfill (not used anymore)\n",
        "            debug=True\n",
        "        )\n",
        "\n",
        "    finally:\n",
        "        # Always close the concurrent plotting thread\n",
        "        raw_queue.put(None)   # signal â€œdoneâ€\n",
        "        plot_thread.join()\n",
        "\n",
        "        # Aggressive cleanup: drop big refs, GC, trim arenas\n",
        "        aggressive_memory_cleanup(locals())\n",
        "\n",
        "\n",
        "\n",
        "# Handle environment for running pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set spawn method for child processes to spawn\n",
        "        try:\n",
        "            mp.set_start_method(\"fork\")\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "\n",
        "        # Run main pipeline\n",
        "        with cProfile.Profile() as pr:\n",
        "            main_pipeline(\n",
        "                debug_flag=False,         # NOTE -> Individual components have separate debug flags\n",
        "                year=2017,               # Unpack into start and end times (datetime objects) within main_pipeline\n",
        "                radar_info=radar_info,\n",
        "                train_rewrite=False,     # If rewrite is true, then ignore what is currently in training dataset and rewrite entire dataset\n",
        "                training_base=\"Datasets/training_datasets/level_two\"\n",
        "                )\n",
        "    finally:\n",
        "        # Print time spent in slowest functions\n",
        "        stats = pstats.Stats(pr)\n",
        "        stats.sort_stats('cumtime').print_stats(30)\n",
        "\n",
        "        del stats, pr\n",
        "        import gc, ctypes\n",
        "        gc.collect()\n",
        "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWX-9KLMF7wi"
      },
      "source": [
        "# ðŸŸ¦ Build Starter Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1zjhK2SGiQl"
      },
      "source": [
        "### **Copy + Build Subset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhhMJQ34GoP6"
      },
      "outputs": [],
      "source": [
        "# HRSS Starter dataset builder (Colab-friendly; no globals)\n",
        "# Copies full radar sites into <dst_base>/<year>/<SITE>/storm_*/*,\n",
        "# then rebuilds manifest.csv + catalog.csv per year (lean, no sidecars).\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Optional\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------- helpers --------------------------\n",
        "def _fmt_bytes(n: int) -> str:\n",
        "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n",
        "        if abs(n) < 1024.0:\n",
        "            return f\"{n:.1f} {unit}\"\n",
        "        n /= 1024.0\n",
        "    return f\"{n:.1f} EB\"\n",
        "\n",
        "_fname_re = re.compile(\n",
        "    r\"^(?P<site>[A-Z0-9]{4})_(?P<storm>\\d+?)_\"\n",
        "    r\"(?P<kind>context|[a-zA-Z0-9_]+?)\"\n",
        "    r\"(?:_T(?P<T>\\d+))?\"\n",
        "    r\"(?:_(?P<H>\\d+)x(?P<W>\\d+)x(?P<C>\\d+)ch)?\"\n",
        "    r\"_(?P<t0>\\d{8}T\\d{6}Z)_(?P<t1>\\d{8}T\\d{6}Z)\\.h5$\"\n",
        ")\n",
        "\n",
        "def _list_sites(year_dir: Path) -> list[str]:\n",
        "    sites = []\n",
        "    if not year_dir.exists():\n",
        "        return sites\n",
        "    for p in year_dir.iterdir():\n",
        "        if p.is_dir() and len(p.name) == 4:  # e.g., KDLH\n",
        "            sites.append(p.name)\n",
        "    return sorted(sites)\n",
        "\n",
        "def _copy_site_tree(src_site_dir: Path, dst_site_dir: Path, skip_names: set, skip_suffixes: set, debug: bool = True) -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Copy everything under <SITE>/storm_*/*, skipping manifests/sidecars.\n",
        "    Returns (n_files, total_bytes).\n",
        "    \"\"\"\n",
        "    n_files = 0\n",
        "    total_bytes = 0\n",
        "    if not src_site_dir.exists():\n",
        "        if debug:\n",
        "            print(f\"[starter][warn] missing site: {src_site_dir}\")\n",
        "        return (0, 0)\n",
        "\n",
        "    dst_site_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for storm_dir in sorted(src_site_dir.glob(\"storm_*\")):\n",
        "        if not storm_dir.is_dir():\n",
        "            continue\n",
        "        dst_storm_dir = dst_site_dir / storm_dir.name\n",
        "        dst_storm_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for fn in sorted(os.listdir(storm_dir)):\n",
        "            if fn in skip_names:\n",
        "                if debug:\n",
        "                    print(f\"[starter][skip] {storm_dir/fn} (manifest/catalog)\")\n",
        "                continue\n",
        "            if any(fn.endswith(suf) for suf in skip_suffixes):\n",
        "                if debug:\n",
        "                    print(f\"[starter][skip] {storm_dir/fn} (legacy sidecar)\")\n",
        "                continue\n",
        "\n",
        "            src = storm_dir / fn\n",
        "            if not src.is_file():\n",
        "                continue\n",
        "            dst = dst_storm_dir / fn\n",
        "\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(src, dst)\n",
        "            n_files += 1\n",
        "            try:\n",
        "                total_bytes += os.path.getsize(dst)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if debug:\n",
        "                print(f\"[starter][copy] {src} â†’ {dst}\")\n",
        "\n",
        "    return (n_files, total_bytes)\n",
        "\n",
        "# -------------------------- main builder --------------------------\n",
        "def build_hrss_starter(\n",
        "    src_base: str = \"/content/drive/MyDrive/Datasets/training_datasets/level_two\",\n",
        "    dst_base: str = \"/content/drive/MyDrive/Datasets/training_datasets/starter\",\n",
        "    years: Iterable[str] = (\"2017\",),\n",
        "    sites: Optional[Iterable[str]] = (\"KDLH\", \"KUEX\"),  # None â†’ copy ALL sites found for each year\n",
        "    clear_dest_site_first: bool = True,\n",
        "    debug: bool = True,\n",
        "    skip_filenames: Iterable[str] = (\"manifest.csv\", \"catalog.csv\"),\n",
        "    skip_suffixes: Iterable[str] = (\".sha256\",),\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a 'starter' subset by copying whole radar sites from src_base into dst_base,\n",
        "    preserving <year>/<SITE>/storm_*/ layout and then regenerating per-year manifest/catalog.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    src_base : str\n",
        "        Root of the full dataset (e.g., \".../Datasets/training_datasets/level_two\").\n",
        "    dst_base : str\n",
        "        Root to write the starter copy into (e.g., \".../Datasets/training_datasets/starter\").\n",
        "    years : Iterable[str]\n",
        "        Years to include (strings).\n",
        "    sites : Optional[Iterable[str]]\n",
        "        Specific site IDs to include; None â†’ include all sites present for each year.\n",
        "    clear_dest_site_first : bool\n",
        "        If True, remove existing destination site directories before copying.\n",
        "    debug : bool\n",
        "        Verbose logging.\n",
        "    skip_filenames : Iterable[str]\n",
        "        Exact filenames to skip (e.g., manifest.csv, catalog.csv).\n",
        "    skip_suffixes : Iterable[str]\n",
        "        Filename suffixes to skip (e.g., \".sha256\").\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    src_base = Path(src_base)\n",
        "    dst_base = Path(dst_base)\n",
        "    year_summaries = []\n",
        "\n",
        "    skip_names_set = set(skip_filenames)\n",
        "    skip_suffixes_set = set(skip_suffixes)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[starter] src_base={src_base}\")\n",
        "        print(f\"[starter] dst_base={dst_base}\")\n",
        "        print(f\"[starter] years={list(years)}\")\n",
        "        print(f\"[starter] sites={'ALL' if sites is None else list(sites)}\")\n",
        "        print(f\"[starter] clear_dest_site_first={clear_dest_site_first}\")\n",
        "\n",
        "    for year in years:\n",
        "        yr = str(year)\n",
        "        src_year_dir = src_base / yr\n",
        "        dst_year_dir = dst_base / yr\n",
        "\n",
        "        if not src_year_dir.exists():\n",
        "            print(f\"[starter][warn] missing year: {src_year_dir} â€” skipping\")\n",
        "            continue\n",
        "\n",
        "        site_list = list(sites) if sites is not None else _list_sites(src_year_dir)\n",
        "        if debug:\n",
        "            print(f\"[starter] Year {yr}: sites to copy = {site_list}\")\n",
        "\n",
        "        total_files = 0\n",
        "        total_bytes = 0\n",
        "\n",
        "        for site in site_list:\n",
        "            src_site_dir = src_year_dir / site\n",
        "            if not src_site_dir.exists():\n",
        "                if debug:\n",
        "                    print(f\"[starter][warn] site not found in {yr}: {site} â€” skipping\")\n",
        "                continue\n",
        "\n",
        "            dst_site_dir = dst_year_dir / site\n",
        "\n",
        "            if clear_dest_site_first and dst_site_dir.exists():\n",
        "                if debug:\n",
        "                    print(f\"[starter] clearing destination site dir â†’ {dst_site_dir}\")\n",
        "                shutil.rmtree(dst_site_dir, ignore_errors=True)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"[starter] copying site {site} for year {yr} â€¦\")\n",
        "            n_files, bytes_copied = _copy_site_tree(\n",
        "                src_site_dir, dst_site_dir, skip_names_set, skip_suffixes_set, debug=debug\n",
        "            )\n",
        "            total_files += n_files\n",
        "            total_bytes += bytes_copied\n",
        "\n",
        "            if debug:\n",
        "                print(f\"[starter] site {site}: copied {n_files} files, {_fmt_bytes(bytes_copied)}\")\n",
        "\n",
        "        # Rebuild manifest/catalog for this year directory (overwrites)\n",
        "        try:\n",
        "            year_dir_str = str(dst_year_dir)\n",
        "            if debug:\n",
        "                print(f\"[starter] rebuilding manifest/catalog in â†’ {year_dir_str}\")\n",
        "            build_year_manifest_and_catalog(\n",
        "                year_dir_str,\n",
        "                update_schema_checksums=True,  # backfill applies_to_sha256 in context schemas if missing\n",
        "                debug=debug\n",
        "            )\n",
        "        except NameError:\n",
        "            raise RuntimeError(\n",
        "                \"build_year_manifest_and_catalog(...) is not defined in this notebook. \"\n",
        "                \"Paste your function definition cell above and rerun.\"\n",
        "            )\n",
        "\n",
        "        year_summaries.append((yr, total_files, total_bytes))\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\n[starter] ===== SUMMARY =====\")\n",
        "        for yr, nf, nb in year_summaries:\n",
        "            print(f\"[starter] {yr}: files={nf}, bytes={_fmt_bytes(nb)}\")\n",
        "        print(f\"[starter] OUTPUT ROOT: {dst_base}\")\n",
        "        print(f\"[starter] TOTAL elapsed: {(time.perf_counter()-t0):.2f} s\")\n",
        "\n",
        "\n",
        "# -------------------------- run it (edit args if desired) --------------------------\n",
        "build_hrss_starter(\n",
        "    src_base=\"Datasets/training_datasets/level_two\",       # where full dataset currently lives\n",
        "    dst_base=\"Datasets/training_datasets/starter\",         # where the starter subset should be written\n",
        "    years=(\"2017\",),                                       # which years to include\n",
        "    sites=(\"KDLH\", \"KUEX\"),                                # which radar sites to include;\n",
        "                                                           # set to None to copy ALL sites found for each year\n",
        "\n",
        "    clear_dest_site_first=True,                            # wipe existing starter before copying (should always be true)\n",
        "    debug=True,\n",
        "    skip_filenames=(\"manifest.csv\", \"catalog.csv\"),        # filenames to skip (legacy or unwanted artifacts)\n",
        "    skip_suffixes=(\".sha256\",),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqe8OvT-GyMV"
      },
      "source": [
        "### **Build Zenodo Bundle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DguTOKk-G94Y",
        "outputId": "21c4e5bd-2e90-4e56-9df5-58b80a25337b"
      },
      "outputs": [],
      "source": [
        "# HRSS â†’ Zenodo bundle packer (manifest.csv stays inside the archive)\n",
        "# Produces:\n",
        "#   - <out_dir>/<bundle_name>.<zip|tar.gz>\n",
        "#   - <out_dir>/checksums.sha256   (hash for the archive itself)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import tarfile\n",
        "import zipfile\n",
        "import hashlib\n",
        "import shutil\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Optional\n",
        "\n",
        "# -------------------------- helpers --------------------------\n",
        "def _fmt_bytes(n: int) -> str:\n",
        "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n",
        "        if abs(n) < 1024.0:\n",
        "            return f\"{n:.1f} {unit}\"\n",
        "        n /= 1024.0\n",
        "    return f\"{n:.1f} EB\"\n",
        "\n",
        "def _sha256(path: str, chunk: int = 1<<20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for b in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _should_skip(name: str,\n",
        "                 exclude_names: set[str],\n",
        "                 exclude_suffixes: set[str]) -> bool:\n",
        "    base = os.path.basename(name)\n",
        "    if base in exclude_names:\n",
        "        return True\n",
        "    if any(base.endswith(s) for s in exclude_suffixes):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _gather_files(root: Path,\n",
        "                  exclude_names: set[str],\n",
        "                  exclude_suffixes: set[str],\n",
        "                  debug: bool = True) -> list[Path]:\n",
        "    files = []\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        # silently skip Jupyter checkpoint dirs without exposing a param\n",
        "        if \".ipynb_checkpoints\" in Path(dirpath).parts:\n",
        "            if debug:\n",
        "                print(f\"[zen][skipdir] {dirpath} (.ipynb_checkpoints)\")\n",
        "            continue\n",
        "        for fn in filenames:\n",
        "            if _should_skip(fn, exclude_names, exclude_suffixes):\n",
        "                if debug:\n",
        "                    print(f\"[zen][skip] {Path(dirpath)/fn}\")\n",
        "                continue\n",
        "            if fn == \".DS_Store\":\n",
        "                if debug:\n",
        "                    print(f\"[zen][skip] {Path(dirpath)/fn} (.DS_Store)\")\n",
        "                continue\n",
        "            p = Path(dirpath)/fn\n",
        "            if p.is_file():\n",
        "                files.append(p)\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "def _is_subpath(child: Path, parent: Path) -> bool:\n",
        "    try:\n",
        "        child.resolve().relative_to(parent.resolve())\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# -------------------------- main packer --------------------------\n",
        "def build_zenodo_bundle(\n",
        "    src_dir: str,\n",
        "    out_dir: str,\n",
        "    bundle_name: str,\n",
        "    zip_mode: str = \"zip\",                    # \"zip\" or \"tar.gz\"\n",
        "    arc_prefix: Optional[str] = None,         # folder prefix inside archive; default = bundle_name\n",
        "    exclude_names: Iterable[str] = (\"checksums.sha256\",),  # don't re-embed old checksum files\n",
        "    exclude_suffixes: Iterable[str] = (\".sha256\",),\n",
        "    clear_out_dir_first: bool = True,         # nuke out_dir before building\n",
        "    zip_store: bool = True,                   # NEW: True = no compression (fastest for .h5)\n",
        "    zip_deflate_level: int = 6,               # used only when zip_store=False\n",
        "    debug: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Package a HRSS dataset subtree for Zenodo.\n",
        "\n",
        "    zip_store:\n",
        "        If True (default), use ZIP_STORED (no compression) â€” fastest and ideal for already-compressed HDF5.\n",
        "        If False, use ZIP_DEFLATED with 'zip_deflate_level' (1=fastest, 9=smallest).\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    src_dir = Path(src_dir)\n",
        "    out_dir = Path(out_dir)\n",
        "\n",
        "    if not src_dir.exists():\n",
        "        raise FileNotFoundError(f\"src_dir does not exist: {src_dir}\")\n",
        "\n",
        "    # Safety: out_dir must NOT be inside src_dir (or equal)\n",
        "    if _is_subpath(out_dir, src_dir) or src_dir.resolve() == out_dir.resolve():\n",
        "        raise RuntimeError(\n",
        "            f\"out_dir must not be inside src_dir.\\n  src_dir={src_dir}\\n  out_dir={out_dir}\\n\"\n",
        "            \"Pick an out_dir that is outside the source tree.\"\n",
        "        )\n",
        "\n",
        "    if clear_out_dir_first and out_dir.exists():\n",
        "        if debug:\n",
        "            print(f\"[zen] clearing out_dir â†’ {out_dir}\")\n",
        "        shutil.rmtree(out_dir, ignore_errors=True)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if arc_prefix is None:\n",
        "        arc_prefix = bundle_name\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[zen] src_dir={src_dir}\")\n",
        "        print(f\"[zen] out_dir={out_dir}\")\n",
        "        print(f\"[zen] bundle_name={bundle_name}\")\n",
        "        print(f\"[zen] zip_mode={zip_mode}\")\n",
        "        print(f\"[zen] arc_prefix={arc_prefix}\")\n",
        "        print(f\"[zen] exclude_names={list(exclude_names)}\")\n",
        "        print(f\"[zen] exclude_suffixes={list(exclude_suffixes)}\")\n",
        "        print(f\"[zen] clear_out_dir_first={clear_out_dir_first}\")\n",
        "        if zip_mode == \"zip\":\n",
        "            print(f\"[zen] zip_store={zip_store} (deflate_level={zip_deflate_level if not zip_store else 'N/A'})\")\n",
        "\n",
        "    exclude_names = set(exclude_names)\n",
        "    exclude_suffixes = set(exclude_suffixes)\n",
        "\n",
        "    # 1) Gather files\n",
        "    files = _gather_files(src_dir, exclude_names, exclude_suffixes, debug=debug)\n",
        "    if debug:\n",
        "        print(f\"[zen] files to archive: {len(files)}\")\n",
        "        for p in files[:8]:\n",
        "            print(f\"[zen]   + {p.relative_to(src_dir)}\")\n",
        "        if len(files) > 8:\n",
        "            print(f\"[zen]   ... (+{len(files)-8} more)\")\n",
        "\n",
        "    # 2) Create the archive\n",
        "    if zip_mode not in {\"zip\",\"tar.gz\"}:\n",
        "        raise ValueError(\"zip_mode must be 'zip' or 'tar.gz'\")\n",
        "\n",
        "    if zip_mode == \"zip\":\n",
        "        archive_path = out_dir / f\"{bundle_name}.zip\"\n",
        "        if debug:\n",
        "            print(f\"[zen] creating ZIP â†’ {archive_path}\")\n",
        "        compression = zipfile.ZIP_STORED if zip_store else zipfile.ZIP_DEFLATED\n",
        "        zkwargs = {\"compression\": compression}\n",
        "        if not zip_store:\n",
        "            # compresslevel is honored only for ZIP_DEFLATED\n",
        "            zkwargs[\"compresslevel\"] = int(zip_deflate_level)\n",
        "\n",
        "        with zipfile.ZipFile(archive_path, \"w\", **zkwargs) as zf:\n",
        "            total_bytes = 0\n",
        "            for fp in files:\n",
        "                rel = fp.relative_to(src_dir)\n",
        "                arcname = str(Path(arc_prefix) / rel)\n",
        "                zf.write(fp, arcname)  # store/deflate decided by ZipFile(...)\n",
        "                try:\n",
        "                    total_bytes += fp.stat().st_size\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if debug:\n",
        "            mode_str = \"STORED(no-compress)\" if zip_store else f\"DEFLATED(level={zip_deflate_level})\"\n",
        "            print(f\"[zen] ZIP mode={mode_str}; contents â‰ˆ {_fmt_bytes(total_bytes)}; file size = {_fmt_bytes(archive_path.stat().st_size)}\")\n",
        "\n",
        "    else:\n",
        "        # tar.gz path (still single-threaded in Python). If you need even faster,\n",
        "        # consider: create 'w' (no-compress) tar then compress with 'pigz' externally.\n",
        "        archive_path = out_dir / f\"{bundle_name}.tar.gz\"\n",
        "        if debug:\n",
        "            print(f\"[zen] creating TAR.GZ â†’ {archive_path}\")\n",
        "        with tarfile.open(archive_path, \"w:gz\") as tf:\n",
        "            total_bytes = 0\n",
        "            for fp in files:\n",
        "                rel = fp.relative_to(src_dir)\n",
        "                arcname = str(Path(arc_prefix) / rel)\n",
        "                tf.add(fp, arcname=arcname)\n",
        "                try:\n",
        "                    total_bytes += fp.stat().st_size\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if debug:\n",
        "            print(f\"[zen] TAR.GZ contents â‰ˆ {_fmt_bytes(total_bytes)}; file size = {_fmt_bytes(archive_path.stat().st_size)}\")\n",
        "\n",
        "    # 3) Write top-level checksums.sha256 for the ARCHIVE\n",
        "    checksums_path = out_dir / \"checksums.sha256\"\n",
        "    arch_hash = _sha256(str(archive_path))\n",
        "    with open(checksums_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"{arch_hash}  {archive_path.name}\\n\")\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[zen] wrote checksums.sha256 â†’ {checksums_path}\")\n",
        "        print(f\"[zen]   {arch_hash}  {archive_path.name}\")\n",
        "        print(f\"[zen] DONE in {(time.perf_counter()-t0):.2f} s\")\n",
        "        print(\"\\n[zen] Upload the following to Zenodo:\")\n",
        "        print(f\"[zen]   - {archive_path}\")\n",
        "        print(f\"[zen]   - {checksums_path}\")\n",
        "        print(\"[zen] After downloading, users can verify quickly with:\")\n",
        "        print(f\"[zen]   sha256sum -c {checksums_path.name}  # (Linux)  OR\")\n",
        "        print(f\"[zen]   shasum -a 256 -c {checksums_path.name}  # (macOS)\")\n",
        "\n",
        "# -------------------------- run it (edit args) --------------------------\n",
        "build_zenodo_bundle(\n",
        "    src_dir=\"Datasets/training_datasets/starter/2017\",\n",
        "    out_dir=\"Datasets/training_datasets/starter/zenodo_bundle\",\n",
        "    bundle_name=\"hrss-starter-2017\",\n",
        "    zip_mode=\"zip\",                 # \"zip\" or \"tar.gz\"\n",
        "    arc_prefix=\"hrss-starter-2017\",\n",
        "    exclude_names=(\"checksums.sha256\",),\n",
        "    exclude_suffixes=(\".sha256\",),\n",
        "    clear_out_dir_first=True,\n",
        "    zip_store=True,                 # <- FAST: no recompression of .h5\n",
        "    zip_deflate_level=3,            # ignored when zip_store=True\n",
        "    debug=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58-Lwz6khno"
      },
      "source": [
        "# *ðŸŸª Miscellaneous*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqOFt99vvt-"
      },
      "source": [
        "### **Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2VN-zYOuJ6r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM8Cd44m_dQN"
      },
      "source": [
        "### **View Source Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QCHb69lnWc5"
      },
      "source": [
        "#### **View GR-S Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4iSAbj-K0-O_",
        "outputId": "d565ee39-94a1-4a12-9cf1-3cf58e4aa561"
      },
      "outputs": [],
      "source": [
        "# Vertical composite (fast) for your GridRad file + 50km x 50km zoom inset\n",
        "import numpy as np\n",
        "import netCDF4 as nc\n",
        "import matplotlib.pyplot as plt\n",
        "import xarray as xr\n",
        "import os\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "nc_path = \"Datasets/temp/nexrad_3d_v4_2_20100121T120000Z.nc\"\n",
        "out_dir = \"Datasets/temp\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "ds = nc.Dataset(nc_path, \"r\")\n",
        "alts = ds.variables['Altitude'][:]      # shape (29,)\n",
        "lats = ds.variables['Latitude'][:]      # shape (1248,)\n",
        "lons = ds.variables['Longitude'][:]     # shape (2304,)\n",
        "# convert lon to -180..180 for plotting if needed\n",
        "lons_plot = lons.copy()\n",
        "lons_plot[lons_plot > 180] -= 360.0\n",
        "\n",
        "nalt = len(alts)\n",
        "nlat = len(lats)\n",
        "nlon = len(lons)\n",
        "shape3 = (nalt, nlat, nlon)\n",
        "print(\"Grid shape (alt,lat,lon):\", shape3)\n",
        "\n",
        "# read sparse arrays\n",
        "idx_raw = ds.variables['index'][:]           # (N_nonempty,)\n",
        "dbz_raw = ds.variables['Reflectivity'][:]    # (N_nonempty,)\n",
        "\n",
        "print(\"sparse count:\", len(idx_raw))\n",
        "print(\"dbz min/max (raw):\", np.nanmin(dbz_raw), np.nanmax(dbz_raw))\n",
        "\n",
        "# check for 1-based indexing and convert to 0-based\n",
        "if idx_raw.min() == 1:\n",
        "    print(\"index appears 1-based -> converting to 0-based\")\n",
        "    idx0 = idx_raw - 1\n",
        "else:\n",
        "    idx0 = idx_raw.copy()\n",
        "\n",
        "# Attempt unravel in C-order first\n",
        "def unravel_check(order):\n",
        "    try:\n",
        "        a_idx, y_idx, x_idx = np.unravel_index(idx0, shape3, order=order)\n",
        "        # basic bounds check\n",
        "        ok = (a_idx.min() >= 0 and a_idx.max() < nalt and\n",
        "              y_idx.min() >= 0 and y_idx.max() < nlat and\n",
        "              x_idx.min() >= 0 and x_idx.max() < nlon)\n",
        "        return ok, (a_idx, y_idx, x_idx)\n",
        "    except Exception as e:\n",
        "        return False, (None, None, None)\n",
        "\n",
        "okC, (aC, yC, xC) = unravel_check('C')\n",
        "if okC:\n",
        "    a_idx, y_idx, x_idx = aC, yC, xC\n",
        "    used_order = 'C'\n",
        "else:\n",
        "    okF, (aF, yF, xF) = unravel_check('F')\n",
        "    if okF:\n",
        "        a_idx, y_idx, x_idx = aF, yF, xF\n",
        "        used_order = 'F'\n",
        "    else:\n",
        "        raise RuntimeError(\"Could not unravel 'index' into (alt,lat,lon) with either order='C' or 'F'.\")\n",
        "\n",
        "print(\"Using unravel order =\", used_order)\n",
        "print(\"sample a_idx unique (first 10):\", np.unique(a_idx)[:10])\n",
        "print(\"sample lat idx min/max:\", y_idx.min(), y_idx.max())\n",
        "\n",
        "# Build composite fast using np.maximum.at on flattened 2D index (lat,lon)\n",
        "linear2d = (y_idx.astype(np.int64) * nlon) + x_idx.astype(np.int64)   # 1D index into lat*lon grid\n",
        "flat_size = nlat * nlon\n",
        "\n",
        "# initialize with -inf so maximum works; cast dbz to float\n",
        "composite_flat = np.full(flat_size, -np.inf, dtype=np.float32)\n",
        "# apply max at each linear2d location\n",
        "np.maximum.at(composite_flat, linear2d, dbz_raw.astype(np.float32))\n",
        "\n",
        "# reshape back to 2D (lat, lon)\n",
        "composite2d = composite_flat.reshape((nlat, nlon))\n",
        "# convert -inf -> nan where no observations\n",
        "composite2d[~np.isfinite(composite2d)] = np.nan\n",
        "\n",
        "# diagnostics\n",
        "valid_count = np.isfinite(composite2d).sum()\n",
        "print(\"Composite: valid grid points:\", valid_count, \"/\", composite2d.size)\n",
        "if valid_count > 0:\n",
        "    print(\"Composite min/max/mean (valid):\", np.nanmin(composite2d), np.nanmax(composite2d), np.nanmean(composite2d))\n",
        "\n",
        "# --- STORM CENTER (input) ---\n",
        "# Provided in file: Longitude (deg E) = 272.91, Latitude = 29.86\n",
        "storm_lon_degE = 272.91\n",
        "storm_lat = 29.86\n",
        "# convert lon to -180..180 for plotting/projections\n",
        "storm_lon = storm_lon_degE - 360.0 if storm_lon_degE > 180 else storm_lon_degE\n",
        "print(f\"Storm lon (plot coords): {storm_lon}, lat: {storm_lat}\")\n",
        "\n",
        "# desired half-extents (meters)\n",
        "half_size_m = 25_000.0  # 25 km half -> 50 km total\n",
        "\n",
        "# compute lat/lon bounds for the 50x50 km window (prefer pyproj, fallback to approximate)\n",
        "try:\n",
        "    from pyproj import Transformer\n",
        "    # AEQD centered on storm location\n",
        "    aeqd_proj = f\"+proj=aeqd +lat_0={storm_lat} +lon_0={storm_lon} +units=m +datum=WGS84\"\n",
        "    t_geog2xy = Transformer.from_crs(\"EPSG:4326\", aeqd_proj, always_xy=True)\n",
        "    t_xy2geog = Transformer.from_crs(aeqd_proj, \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "    # box corners in local meters\n",
        "    x0, y0 = 0.0, 0.0\n",
        "    corners_xy = np.array([\n",
        "        [-half_size_m, -half_size_m],\n",
        "        [-half_size_m,  half_size_m],\n",
        "        [ half_size_m,  half_size_m],\n",
        "        [ half_size_m, -half_size_m],\n",
        "    ])\n",
        "    # transform back to lon/lat\n",
        "    corner_lons = []\n",
        "    corner_lats = []\n",
        "    for x, y in corners_xy:\n",
        "        lon_pt, lat_pt = t_xy2geog.transform(x, y)\n",
        "        corner_lons.append(lon_pt)\n",
        "        corner_lats.append(lat_pt)\n",
        "\n",
        "    min_lon_box = min(corner_lons)\n",
        "    max_lon_box = max(corner_lons)\n",
        "    min_lat_box = min(corner_lats)\n",
        "    max_lat_box = max(corner_lats)\n",
        "    print(\"Computed zoom box via pyproj (lon/lat):\", (min_lon_box, min_lat_box, max_lon_box, max_lat_box))\n",
        "except Exception as e:\n",
        "    print(\"pyproj failed or unavailable, falling back to approximate degree conversion:\", e)\n",
        "    # Approx conversion: 1 deg lat ~ 110.574 km; 1 deg lon ~ 111.320*cos(lat) km\n",
        "    km_per_deg_lat = 110.574\n",
        "    km_per_deg_lon = 111.320 * np.cos(np.deg2rad(storm_lat))\n",
        "    delta_lat = half_size_m / 1000.0 / km_per_deg_lat\n",
        "    delta_lon = half_size_m / 1000.0 / km_per_deg_lon\n",
        "    min_lat_box = storm_lat - delta_lat\n",
        "    max_lat_box = storm_lat + delta_lat\n",
        "    min_lon_box = storm_lon - delta_lon\n",
        "    max_lon_box = storm_lon + delta_lon\n",
        "    print(\"Approx zoom box (lon/lat):\", (min_lon_box, min_lat_box, max_lon_box, max_lat_box))\n",
        "\n",
        "# Ensure lon bounds are in the same range as lons_plot (which are -180..180)\n",
        "# If lons_plot are in -180..180 they already are; ensure box lons also in that range\n",
        "if min_lon_box < -180:\n",
        "    min_lon_box += 360\n",
        "if max_lon_box > 180:\n",
        "    max_lon_box -= 360\n",
        "\n",
        "# find the index ranges in the grid for the zoom window\n",
        "lat_mask = (lats >= min_lat_box) & (lats <= max_lat_box)\n",
        "lon_mask = (lons_plot >= min_lon_box) & (lons_plot <= max_lon_box)\n",
        "\n",
        "# if masks are empty (due to resolution or rounding), expand by 1 index\n",
        "if lat_mask.sum() == 0:\n",
        "    # find nearest index\n",
        "    lat_idx_near = np.abs(lats - storm_lat).argmin()\n",
        "    lat_mask = np.zeros_like(lat_mask); lat_mask[lat_idx_near] = True\n",
        "if lon_mask.sum() == 0:\n",
        "    lon_idx_near = np.abs(lons_plot - storm_lon).argmin()\n",
        "    lon_mask = np.zeros_like(lon_mask); lon_mask[lon_idx_near] = True\n",
        "\n",
        "lat_inds = np.where(lat_mask)[0]\n",
        "lon_inds = np.where(lon_mask)[0]\n",
        "iy0, iy1 = lat_inds.min(), lat_inds.max()\n",
        "ix0, ix1 = lon_inds.min(), lon_inds.max()\n",
        "print(\"Zoom index ranges lat:\", iy0, iy1, \" lon:\", ix0, ix1)\n",
        "print(\"Zoom window size (grid cells):\", (iy1-iy0+1), (ix1-ix0+1))\n",
        "\n",
        "# subset of composite for inset (note: do NOT modify original composite2d)\n",
        "sub = composite2d[iy0:iy1+1, ix0:ix1+1]\n",
        "\n",
        "# Plot main figure and inset\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12,6))\n",
        "extent = (lons_plot.min(), lons_plot.max(), lats.min(), lats.max())\n",
        "im = ax.imshow(composite2d, origin='lower', extent=extent, aspect='auto')\n",
        "ax.set_title(\"Vertical Composite Reflectivity (max over altitude)\\n\" + getattr(ds, 'Analysis_time', ''))\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "cbar = fig.colorbar(im, ax=ax, label='dBZ')\n",
        "\n",
        "# draw rectangle showing zoom window on main axes\n",
        "rect_lon = min_lon_box\n",
        "rect_lat = min_lat_box\n",
        "rect_width = max_lon_box - min_lon_box\n",
        "rect_height = max_lat_box - min_lat_box\n",
        "# If rectangle crosses dateline (rect_width negative), fix by using absolute width (rare here)\n",
        "if rect_width < 0:\n",
        "    rect_width = (max_lon_box + 360) - min_lon_box\n",
        "\n",
        "rect = Rectangle((rect_lon, rect_lat), rect_width, rect_height,\n",
        "                 linewidth=2, edgecolor='white', facecolor='none', linestyle='--')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "# inset axes (position as fraction of figure: [left, bottom, width, height])\n",
        "axins = fig.add_axes([0.62, 0.55, 0.32, 0.38])\n",
        "# compute extent for inset using actual lon/lat bounds of the selected indices\n",
        "inset_extent = (lons_plot[ix0], lons_plot[ix1], lats[iy0], lats[iy1])\n",
        "# show subset; use origin='lower' and set aspect='auto'\n",
        "im2 = axins.imshow(sub, origin='lower', extent=inset_extent, aspect='auto')\n",
        "axins.set_title(\"Zoom: 50 km Ã— 50 km\")\n",
        "axins.set_xlabel(\"Lon\")\n",
        "axins.set_ylabel(\"Lat\")\n",
        "# draw crosshair at storm center in the inset (if inside)\n",
        "if (storm_lon >= inset_extent[0] and storm_lon <= inset_extent[1] and\n",
        "    storm_lat >= inset_extent[2] and storm_lat <= inset_extent[3]):\n",
        "    axins.plot(storm_lon, storm_lat, marker='+', color='k', markersize=12, markeredgewidth=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Separate figure for the 50 km Ã— 50 km zoom ---\n",
        "fig_zoom, ax_zoom = plt.subplots(1, 1, figsize=(6, 6))\n",
        "im_zoom = ax_zoom.imshow(sub, origin='lower', extent=inset_extent, aspect='auto')\n",
        "ax_zoom.set_title(\"Vertical Composite Reflectivity\\n50 km Ã— 50 km Zoom\")\n",
        "ax_zoom.set_xlabel(\"Longitude\")\n",
        "ax_zoom.set_ylabel(\"Latitude\")\n",
        "cbar_zoom = fig_zoom.colorbar(im_zoom, ax=ax_zoom, label='dBZ')\n",
        "\n",
        "# crosshair for storm center (same logic as before)\n",
        "if (storm_lon >= inset_extent[0] and storm_lon <= inset_extent[1] and\n",
        "    storm_lat >= inset_extent[2] and storm_lat <= inset_extent[3]):\n",
        "    ax_zoom.plot(storm_lon, storm_lat, marker='+', color='k', markersize=12, markeredgewidth=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ds.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJiDcqa8uGmL"
      },
      "source": [
        "#### **View NEXRAD Level-3 Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xTVmAzG3RID0",
        "outputId": "43d648f7-65da-46a9-ed5d-f183da1a906b"
      },
      "outputs": [],
      "source": [
        "import s3fs\n",
        "import matplotlib.pyplot as plt\n",
        "import pyart\n",
        "import os, tarfile, re\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from io import BytesIO\n",
        "from collections import Counter\n",
        "from scipy.ndimage import distance_transform_edt, gaussian_filter\n",
        "from scipy.spatial import cKDTree\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "############################################################################# HELPER ##############################################################################\n",
        "\n",
        "\n",
        "def _fill_nans_by_nearest_avg(arr, k: int = 4, max_distance: Optional[float] = None,\n",
        "                              fill_global_mean: bool = True, debug: bool = False):\n",
        "    \"\"\"\n",
        "    Fill NaNs in a 2D numeric array by averaging the k nearest non-NaN neighbors.\n",
        "    Compatible with SciPy versions that do or do not support cKDTree.query(n_jobs=...).\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # handle masked arrays\n",
        "    if hasattr(arr, \"filled\"):\n",
        "        arr = arr.filled(np.nan)\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "\n",
        "    mask = np.isnan(arr)\n",
        "    if not mask.any():\n",
        "        return arr\n",
        "\n",
        "    coords_non_nan = np.column_stack(np.nonzero(~mask))\n",
        "    vals = arr[~mask]\n",
        "    coords_nan = np.column_stack(np.nonzero(mask))\n",
        "\n",
        "    if coords_non_nan.shape[0] == 0:\n",
        "        if debug:\n",
        "            print(\"[_fill_nans_by_nearest_avg] no non-NaN cells found\")\n",
        "        return arr\n",
        "\n",
        "    tree = cKDTree(coords_non_nan)\n",
        "\n",
        "    kq = min(k, coords_non_nan.shape[0])\n",
        "\n",
        "    # compatibility: some scipy versions support n_jobs, others don't\n",
        "    try:\n",
        "        # prefer parallel if available\n",
        "        dists, idxs = tree.query(coords_nan, k=kq, n_jobs=-1)\n",
        "    except TypeError:\n",
        "        # older scipy: no n_jobs argument\n",
        "        dists, idxs = tree.query(coords_nan, k=kq)\n",
        "\n",
        "    # normalize shapes when kq == 1\n",
        "    if kq == 1:\n",
        "        dists = dists[:, None]\n",
        "        idxs = idxs[:, None]\n",
        "\n",
        "    if max_distance is not None:\n",
        "        valid_mask = (dists <= max_distance)\n",
        "    else:\n",
        "        valid_mask = np.ones_like(dists, dtype=bool)\n",
        "\n",
        "    fill_vals = np.empty(coords_nan.shape[0], dtype=float)\n",
        "    fill_vals.fill(np.nan)\n",
        "    sums = np.zeros(coords_nan.shape[0], dtype=float)\n",
        "    counts = np.zeros(coords_nan.shape[0], dtype=int)\n",
        "\n",
        "    for neigh_slot in range(kq):\n",
        "        valid = valid_mask[:, neigh_slot]\n",
        "        if not valid.any():\n",
        "            continue\n",
        "        idxs_slot = idxs[:, neigh_slot][valid]\n",
        "        sums[valid] += vals[idxs_slot]\n",
        "        counts[valid] += 1\n",
        "\n",
        "    nonzero = counts > 0\n",
        "    fill_vals[nonzero] = sums[nonzero] / counts[nonzero]\n",
        "\n",
        "    arr_filled = arr.copy()\n",
        "    arr_filled[mask] = fill_vals\n",
        "\n",
        "    if fill_global_mean:\n",
        "        global_mean = np.nanmean(vals) if vals.size > 0 else np.nan\n",
        "        if np.isnan(global_mean):\n",
        "            if debug:\n",
        "                print(\"[_fill_nans_by_nearest_avg] global mean is NaN; leaving remaining NaNs\")\n",
        "        else:\n",
        "            arr_filled[np.isnan(arr_filled)] = global_mean\n",
        "\n",
        "    return arr_filled\n",
        "\n",
        "\n",
        "\n",
        "def _load_composite_pickle(pkl_path: str, debug: bool=False):\n",
        "    try:\n",
        "        with open(pkl_path, \"rb\") as fh:\n",
        "            comp = pickle.load(fh)\n",
        "        if debug:\n",
        "            print(f\"[cache] loaded composite from {pkl_path}\")\n",
        "        return comp\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"[cache] failed to load pickle {pkl_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "############################################################################ LEVEL 2 #############################################################################\n",
        "\n",
        "\n",
        "'''\n",
        "# List files for a given date and radar site\n",
        "fs = s3fs.S3FileSystem(anon=True)\n",
        "\n",
        "# e.g., KTLX on 20210720\n",
        "prefix = \"noaa-nexrad-level2/2021/07/20/KTLX/\"\n",
        "files = fs.ls(prefix)\n",
        "print(\"Available scans:\", files[:5])\n",
        "\n",
        "\n",
        "# Choose one file and open it via Py-ART directly from S3\n",
        "sample = files[0]\n",
        "with fs.open(sample, 'rb') as fobj:\n",
        "    radar = pyart.io.read_nexrad_archive(fobj)\n",
        "\n",
        "# Inspect metadata\n",
        "print(dir(radar))\n",
        "\n",
        "print(\"Tilts:\", radar.sweep_number)\n",
        "print(\"Fields available:\", radar.fields.keys())\n",
        "\n",
        "# Extract base reflectivity (usually 'reflectivity')\n",
        "reflect = radar.fields['reflectivity']['data'][0]  # first tilt\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(36,36))\n",
        "display = pyart.graph.RadarDisplay(radar)\n",
        "display.plot('reflectivity', sweep=0, vmin=10, vmax=60, title=\"KTLX Reflectivity\")\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "\n",
        "################################################################## DISPLAY AVALIABLE PRODUCTS #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "def extract_product_token_from_second_segment(member_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Strict: return exactly the first 3 alnum chars of the 3rd underscore segment.\n",
        "    Example:\n",
        "      \"KEWX_SDUS24_N1QDFX_201707171046\" -> \"N1Q\"\n",
        "      \"KXXX_ABC_DHR_2019...\"             -> \"DHR\"\n",
        "    Returns None if the segment is missing or doesn't yield alnum chars.\n",
        "    \"\"\"\n",
        "    if not isinstance(member_name, str):\n",
        "        return None\n",
        "    parts = member_name.split('_')\n",
        "    if len(parts) < 3:\n",
        "        return None\n",
        "    seg = parts[2].upper()\n",
        "    seg = re.sub(r'[^A-Z0-9]', '', seg)  # keep only alnum\n",
        "    if len(seg) == 0:\n",
        "        return None\n",
        "    return seg[:3]  # EXACTLY 3 chars (or fewer if seg <3)\n",
        "\n",
        "\n",
        "# Inspect first tar in tmp_tar_dir, show members and token counts ---\n",
        "def inspect_first_tar(tmp_tar_dir: str = \"Datasets/nexrad_datasets/tmp_tarbell\",\n",
        "                      top_n: int = 20,\n",
        "                      sample_map_n: int = 40,\n",
        "                      debug: bool = True):\n",
        "    \"\"\"\n",
        "    Inspect the first TAR in tmp_tar_dir, list members, extract product tokens\n",
        "    using the second-underscore segment rule, and print the most common tokens.\n",
        "    Also prints a short sample mapping (member -> extracted token).\n",
        "    Returns (tar_path, names, counts) like previous helper.\n",
        "    \"\"\"\n",
        "    files = sorted([os.path.join(tmp_tar_dir, f) for f in os.listdir(tmp_tar_dir)\n",
        "                    if f.lower().endswith(('.tar', '.tgz', '.tar.gz'))])\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No tar files found in {tmp_tar_dir}\")\n",
        "    tar_path = files[0]\n",
        "    if debug:\n",
        "        print(f\"[inspect_first_tar] Inspecting: {tar_path}\")\n",
        "\n",
        "    tf = tarfile.open(tar_path, mode=\"r:*\")\n",
        "    members = [m for m in tf.getmembers() if m.isfile()]\n",
        "    names = [m.name for m in members]\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[inspect_first_tar] {len(names)} members found. Showing first {min(200, len(names))} names:\")\n",
        "        for n in names[:200]:\n",
        "            print(\"  \", n)\n",
        "\n",
        "    # build token list using strict rule\n",
        "    tokens = []\n",
        "    sample_map = []\n",
        "    for n in names:\n",
        "        tok = extract_product_token_from_second_segment(n)\n",
        "        tokens.append(tok if tok is not None else \"<NONE>\")\n",
        "        if len(sample_map) < sample_map_n:\n",
        "            sample_map.append((n, tok))\n",
        "\n",
        "    counts = Counter(tokens)\n",
        "    if debug:\n",
        "        print(\"\\n[inspect_first_tar] Sample mapping (first {} members):\".format(len(sample_map)))\n",
        "        for mname, tok in sample_map:\n",
        "            print(f\"  {mname:60s} -> {tok}\")\n",
        "        print(f\"\\n[inspect_first_tar] Top {top_n} product tokens and counts:\")\n",
        "        for token, cnt in counts.most_common(top_n):\n",
        "            print(f\"  {token:6s} : {cnt}\")\n",
        "\n",
        "    try:\n",
        "        tf.close()\n",
        "    except Exception:\n",
        "        print(\"continuing\")\n",
        "        pass\n",
        "\n",
        "    return tar_path, names, counts\n",
        "\n",
        "\n",
        "\n",
        "################################################################## DISPLAY SELECTED PRODUCT ##################################################################\n",
        "\n",
        "\n",
        "\n",
        "def display_product_from_path(path: str, product_token: str = \"_dhr\", field_preference: str = \"reflectivity\", debug: bool = True):\n",
        "    \"\"\"\n",
        "    Look inside 'path' (a directory) and find the FIRST file whose filename contains product_token\n",
        "    (case-insensitive). Treat that file as the pickled cached composite (pkl) if possible:\n",
        "        comp = _load_composite_pickle(pkl_path, debug=debug)\n",
        "    If that fails, fall back to trying to read the file contents with pyart.io.read_nexrad_level3.\n",
        "    After comp is obtained, perform the same plotting / gridding workflow as before.\n",
        "\n",
        "    Returns (comp, member_name)\n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(f\"[display_product_from_path] scanning directory {path!r} for token '{product_token}'\")\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"path not found: {path}\")\n",
        "\n",
        "    if not os.path.isdir(path):\n",
        "        raise ValueError(f\"path must be a directory containing files; got file: {path}\")\n",
        "\n",
        "    # list files in deterministic order\n",
        "    names = sorted(os.listdir(path))\n",
        "    candidate_name = None\n",
        "    token_uc = (product_token or \"\").lower()\n",
        "\n",
        "    for nm in names:\n",
        "        if token_uc in nm.lower():\n",
        "            candidate_name = nm\n",
        "            break\n",
        "\n",
        "    if candidate_name is None:\n",
        "        # helpful fallback: show top tokens\n",
        "        tokens = []\n",
        "        for nm in names:\n",
        "            parts = nm.split('_')\n",
        "            tok = None\n",
        "            if len(parts) >= 3:\n",
        "                tok = re.sub(r'[^A-Z0-9]', '', parts[2].upper())\n",
        "            tokens.append(tok or \"<NONE>\")\n",
        "        counts = Counter(tokens)\n",
        "        raise ValueError(f\"No file matched token '{product_token}' in directory {path!r}. Top tokens: {counts.most_common(40)}\")\n",
        "\n",
        "    member_path = os.path.join(path, candidate_name)\n",
        "    if debug:\n",
        "        print(f\"[display_product_from_path] selected file: {member_path}\")\n",
        "\n",
        "    # Attempt to load cached composite via helper (user has this in-scope)\n",
        "    comp = None\n",
        "    pkl_path = member_path\n",
        "    try:\n",
        "        if os.path.exists(pkl_path):\n",
        "            # user-provided function expected in-scope\n",
        "            comp = _load_composite_pickle(pkl_path, debug=debug)\n",
        "            if comp is not None and debug:\n",
        "                print(f\"[display_product_from_path] composite cache HIT: {pkl_path}\")\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"[display_product_from_path] _load_composite_pickle failed: {e}\")\n",
        "\n",
        "    # If cache load failed, try reading raw file bytes with pyart\n",
        "    if comp is None:\n",
        "        try:\n",
        "            with open(member_path, \"rb\") as fh:\n",
        "                blob = fh.read()\n",
        "            bio = BytesIO(blob)\n",
        "            # try reading with pyart (same approach as before)\n",
        "            try:\n",
        "                comp = pyart.io.read_nexrad_level3(bio)\n",
        "            except Exception:\n",
        "                # maybe pyart expects seekable object â€” retry\n",
        "                try:\n",
        "                    bio.seek(0)\n",
        "                    comp = pyart.io.read_nexrad_level3(bio)\n",
        "                except Exception as e:\n",
        "                    raise RuntimeError(f\"pyart failed to read file {member_path}: {e}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to open/read {member_path}: {e}\")\n",
        "\n",
        "    if comp is None:\n",
        "        raise RuntimeError(\"pyart failed to produce composite from selected file\")\n",
        "\n",
        "    # --- plotting & grid logic (kept mostly the same as your original) ---\n",
        "    fields = list(comp.fields.keys()) if hasattr(comp, 'fields') else []\n",
        "    if debug:\n",
        "        print(f\"[display_product_from_path] composite fields: {fields}\")\n",
        "    try:\n",
        "        raw_times = comp.time['data']\n",
        "        units = comp.time.get('units', 'unknown')\n",
        "        if debug:\n",
        "            print(f\"[display_product_from_path] comp.time units: {units}; raw_times[0]: {raw_times[0] if len(raw_times)>0 else 'N/A'}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    plot_field = field_preference if field_preference in fields else (fields[0] if fields else None)\n",
        "    if plot_field is None:\n",
        "        raise RuntimeError(\"No plottable fields found in composite\")\n",
        "\n",
        "    # RadarDisplay primary plotting (defensive)\n",
        "    try:\n",
        "        from pyart.graph import RadarDisplay\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "        display = RadarDisplay(comp)\n",
        "        display.plot(plot_field, sweep=0, ax=ax, title=f\"{os.path.basename(member_path)} -> {plot_field}\")\n",
        "        plt.show()\n",
        "        if debug:\n",
        "            print(f\"[display_product_from_path] plotted field '{plot_field}' for file '{candidate_name}'\")\n",
        "    except Exception as e:\n",
        "        # fallback imshow of first sweep\n",
        "        try:\n",
        "            sweep = 0\n",
        "            start = comp.sweep_start_ray_index['data'][sweep]\n",
        "            stop  = comp.sweep_end_ray_index['data'][sweep]\n",
        "            arr = comp.fields[plot_field]['data'][start:stop, :]\n",
        "            arr_plot = arr.filled(np.nan) if isinstance(arr, np.ma.MaskedArray) else arr\n",
        "            fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "            im = ax.imshow(arr_plot, origin='lower', aspect='auto')\n",
        "            ax.set_title(f\"Fallback plot: {os.path.basename(member_path)} -> {plot_field}\")\n",
        "            fig.colorbar(im, ax=ax, label=plot_field)\n",
        "            plt.show()\n",
        "            if debug:\n",
        "                print(f\"[display_product_from_path] fallback plotted '{plot_field}'. (RadarDisplay error: {e})\")\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Both RadarDisplay and fallback plotting failed: {e}; {e2}\")\n",
        "\n",
        "    # Try gridding with pyart.map.grid_from_radars; if that fails, use polar->cartesian binning fallback\n",
        "    try:\n",
        "        from pyart.map import grid_from_radars\n",
        "        # radar lat/lon from the composite/radar object â€” be defensive\n",
        "        try:\n",
        "            radar_lat = float(comp.latitude['data'][0])\n",
        "            radar_lon = float(comp.longitude['data'][0])\n",
        "        except Exception:\n",
        "            # try scalars if array-like isn't present\n",
        "            radar_lat = float(comp.latitude['data']) if isinstance(comp.latitude['data'], (float, int)) else None\n",
        "            radar_lon = float(comp.longitude['data']) if isinstance(comp.longitude['data'], (float, int)) else None\n",
        "\n",
        "        if radar_lat is None or radar_lon is None:\n",
        "            raise RuntimeError(\"Could not determine radar latitude/longitude from composite for gridding\")\n",
        "\n",
        "        half_side_m = 25_000.0  # 25 km each way -> 50 km total\n",
        "        grid_shape = (1, 101, 101)\n",
        "        grid_limits = ((0.0, 1000.0), (-half_side_m, half_side_m), (-half_side_m, half_side_m))\n",
        "\n",
        "        grid = grid_from_radars([comp],\n",
        "                                grid_shape=grid_shape,\n",
        "                                grid_limits=grid_limits,\n",
        "                                grid_origin=(radar_lat, radar_lon),\n",
        "                                fields=[plot_field])\n",
        "\n",
        "        gdata = grid.fields[plot_field]['data'][0, :, :]\n",
        "        extent_km = (-half_side_m/1000.0, half_side_m/1000.0, -half_side_m/1000.0, half_side_m/1000.0)\n",
        "\n",
        "        # fill NaNs nearest (using your helper)\n",
        "        gdata_filled = _fill_nans_by_nearest_avg(gdata.astype(float), k=4, max_distance=None, fill_global_mean=True)\n",
        "\n",
        "        # optional smoothing\n",
        "        try:\n",
        "            gdata_sm = gaussian_filter(gdata_filled, sigma=0.6, mode='nearest') if gaussian_filter is not None else gdata_filled\n",
        "        except Exception:\n",
        "            gdata_sm = gdata_filled\n",
        "\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "        im = ax.imshow(gdata_sm, origin='lower', extent=extent_km, aspect='equal')\n",
        "        ax.set_title(f\"{os.path.basename(member_path)} -> {plot_field} â€” 50Ã—50 km (centered on radar) [filled]\")\n",
        "        ax.set_xlabel(\"km East of radar\")\n",
        "        ax.set_ylabel(\"km North of radar\")\n",
        "        fig.colorbar(im, ax=ax, label=plot_field)\n",
        "        plt.show()\n",
        "        print(\"[50km crop] plotted using pyart.map.grid_from_radars() with nearest fill/smoothing\")\n",
        "\n",
        "    except Exception as grid_err:\n",
        "        # fallback: polar->cartesian histogram binning (same as original)\n",
        "        try:\n",
        "            sweep = 0\n",
        "            start = comp.sweep_start_ray_index['data'][sweep]\n",
        "            stop = comp.sweep_end_ray_index['data'][sweep]\n",
        "            ranges_m = comp.range['data']\n",
        "            azimuths = comp.azimuth['data'][start:stop]\n",
        "            vals = comp.fields[plot_field]['data'][start:stop, :]\n",
        "            vals = np.array(vals)\n",
        "            R = ranges_m / 1000.0\n",
        "            AZ = np.deg2rad(azimuths)[:, None]\n",
        "            R2D = np.tile(R, (len(azimuths), 1))\n",
        "            x = R2D * np.sin(AZ)\n",
        "            y = R2D * np.cos(AZ)\n",
        "\n",
        "            xmin, xmax = -25.0, 25.0\n",
        "            ymin, ymax = -25.0, 25.0\n",
        "            nbins = 101\n",
        "            xi = np.linspace(xmin, xmax, nbins)\n",
        "            yi = np.linspace(ymin, ymax, nbins)\n",
        "\n",
        "            xf = x.ravel()\n",
        "            yf = y.ravel()\n",
        "            vf = vals.ravel()\n",
        "            if np.ma.is_masked(vf):\n",
        "                vf = vf.filled(np.nan)\n",
        "\n",
        "            H_sum, xedges, yedges = np.histogram2d(xf, yf, bins=[xi, yi], range=[[xmin, xmax], [ymin, ymax]], weights=vf)\n",
        "            H_count, _, _ = np.histogram2d(xf, yf, bins=[xi, yi], range=[[xmin, xmax], [ymin, ymax]])\n",
        "            with np.errstate(invalid='ignore', divide='ignore'):\n",
        "                H_mean = H_sum / np.where(H_count == 0, np.nan, H_count)\n",
        "\n",
        "            imdata = H_mean.T\n",
        "            imdata_filled = _fill_nans_by_nearest_avg(imdata.astype(float), k=4, max_distance=None, fill_global_mean=True)\n",
        "\n",
        "            try:\n",
        "                imdata_sm = gaussian_filter(imdata_filled, sigma=0.6, mode='nearest') if gaussian_filter is not None else imdata_filled\n",
        "            except Exception:\n",
        "                imdata_sm = imdata_filled\n",
        "\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "            im = ax.imshow(imdata_sm, origin='lower', extent=[xmin, xmax, ymin, ymax], aspect='equal')\n",
        "            ax.set_title(f\"Fallback 50Ã—50 km crop: {os.path.basename(member_path)} -> {plot_field} [filled]\")\n",
        "            ax.set_xlabel(\"km East of radar\")\n",
        "            ax.set_ylabel(\"km North of radar\")\n",
        "            fig.colorbar(im, ax=ax, label=plot_field)\n",
        "            plt.show()\n",
        "            print(\"[50km crop] plotted using fallback binning (nearest fill/smoothing).\")\n",
        "        except Exception as fallback_err:\n",
        "            print(\"[50km crop] failed:\")\n",
        "            print(\"  grid error:\", grid_err)\n",
        "            print(\"  fallback error:\", fallback_err)\n",
        "\n",
        "    return comp, candidate_name\n",
        "\n",
        "\n",
        "\n",
        "_, names, counts = inspect_first_tar(\"Datasets/nexrad_datasets/tmp_tarbell\")\n",
        "\n",
        "tar_path = \"Datasets/temp\"\n",
        "\n",
        "# see printed top tokens; pick one (e.g. 'DHR')\n",
        "comp, member_name = display_product_from_path(\"Datasets/temp\", product_token=\"_dhr_\", field_preference=\"reflectivity\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Dl2ukz2jdzyy",
        "outputId": "ae79629a-7c9e-4dfd-bf2c-47b67f017357"
      },
      "outputs": [],
      "source": [
        "import os, re, pickle, math, warnings, traceback\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "STORM_DIR = \"Datasets/training_datasets/level_three_cropped/2017/KDLH/storm_3411\"\n",
        "OUT_GIF   = \"dhr_anim.gif\"\n",
        "OUT_MP4   = \"dhr_anim.mp4\"\n",
        "FPS       = 4\n",
        "CMAP      = \"pyart_NWSRef\"\n",
        "VMIN, VMAX = 0, 60\n",
        "FIGSIZE   = (7, 6)\n",
        "DPI       = 140\n",
        "PAD_INCH  = 0.05\n",
        "TITLE_FMT = \"{title}\\n{ts:%Y-%m-%d %H:%M:%S} UTC\"\n",
        "\n",
        "# Set to False to silence debug prints about failed adapters\n",
        "VERBOSE = True\n",
        "\n",
        "# Try pyart colormap if available\n",
        "try:\n",
        "    import pyart\n",
        "    cmap = plt.get_cmap(CMAP)\n",
        "except Exception:\n",
        "    warnings.warn(\"pyart not found or cmap missing; using default matplotlib cmap.\")\n",
        "    cmap = plt.get_cmap(\"viridis\")\n",
        "    pyart = None\n",
        "\n",
        "# Helpers\n",
        "_ts_re = re.compile(r'(\\d{8})[_-]?(\\d{4,6})')\n",
        "\n",
        "def parse_timestamp_from_name(name: str):\n",
        "    m = _ts_re.search(name)\n",
        "    if not m:\n",
        "        return None\n",
        "    ymd, hm = m.groups()\n",
        "    year = int(ymd[:4]); mon = int(ymd[4:6]); day = int(ymd[6:8])\n",
        "    hour = int(hm[:2]); minute = int(hm[2:4])\n",
        "    second = int(hm[4:6]) if len(hm) >= 6 else 0\n",
        "    try:\n",
        "        return datetime(year, mon, day, hour, minute, second)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def safe_title_from_name(name: str):\n",
        "    return Path(name).stem.replace(\"_\", \" \")\n",
        "\n",
        "# field name tests (more tolerant)\n",
        "def is_reflectivity_name(k: str):\n",
        "    if not k:\n",
        "        return False\n",
        "    s = k.lower()\n",
        "    # common reflectivity keys encountered in pyart / NEXRAD\n",
        "    return any(sub in s for sub in (\"dhr\", \"reflect\", \"refl\", \"dbz\", \"n0r\"))\n",
        "\n",
        "def is_velocity_name(k: str):\n",
        "    if not k:\n",
        "        return False\n",
        "    s = k.lower()\n",
        "    return \"vel\" in s or \"velocity\" in s or \"doppler\" in s\n",
        "\n",
        "# debug summary helper (kept short)\n",
        "def debug_object_summary(obj, label=None, maxlen=800):\n",
        "    if not VERBOSE:\n",
        "        return\n",
        "    print(\"---- DEBUG OBJECT SUMMARY ----\")\n",
        "    if label:\n",
        "        print(\"Label:\", label)\n",
        "    print(\"Type:\", type(obj))\n",
        "    try:\n",
        "        r = repr(obj)\n",
        "        print(\"Repr head:\", r[:maxlen].replace(\"\\n\", \" \"))\n",
        "    except Exception as e:\n",
        "        print(\"Could not get repr:\", e)\n",
        "    if isinstance(obj, dict):\n",
        "        print(\"dict keys (first 50):\", list(obj.keys())[:50])\n",
        "    if hasattr(obj, \"keys\") and not isinstance(obj, dict):\n",
        "        try:\n",
        "            print(\"obj.keys() (first 50):\", list(obj.keys())[:50])\n",
        "        except Exception:\n",
        "            pass\n",
        "    if hasattr(obj, \"fields\"):\n",
        "        try:\n",
        "            f = obj.fields\n",
        "            print(\".fields type:\", type(f), \"len (if possible):\",\n",
        "                  (len(f) if hasattr(f, \"__len__\") else \"unknown\"),\n",
        "                  \"keys (first 50):\", list(f.keys())[:50] if isinstance(f, dict) else \"not dict\")\n",
        "        except Exception as e:\n",
        "            print(\"Error reading .fields:\", e)\n",
        "    if hasattr(obj, \"__dict__\"):\n",
        "        try:\n",
        "            print(\"__dict__ keys (first 50):\", list(obj.__dict__.keys())[:50])\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"---- end debug ----\")\n",
        "\n",
        "\n",
        "# ---------------- ADAPTER ----------------\n",
        "def adapt_pyart_radar(obj):\n",
        "    \"\"\"\n",
        "    Unified adapter: accepts pyart Radar OR pyart-like objects/dicts that expose a .fields mapping.\n",
        "    Returns a single dict with a single plot_fn closure that handles:\n",
        "      - pcolormesh(gate_x, gate_y, data) when gate coords available or can be constructed\n",
        "      - pyart.graph.RadarDisplay.plot for Radar objects (fallback)\n",
        "      - imshow(arr) as last resort\n",
        "    If object only has velocity-like fields, returns None.\n",
        "    \"\"\"\n",
        "    # attempt to detect pyart Radar class\n",
        "    RadarClass = None\n",
        "    try:\n",
        "        if pyart is not None:\n",
        "            from pyart.core import Radar as RadarClass\n",
        "    except Exception:\n",
        "        RadarClass = None\n",
        "\n",
        "    is_radar = RadarClass is not None and isinstance(obj, RadarClass)\n",
        "\n",
        "    # Get fields mapping (works for Radar and dict-like objects)\n",
        "    fields = None\n",
        "    if is_radar:\n",
        "        try:\n",
        "            fields = obj.fields\n",
        "        except Exception:\n",
        "            fields = None\n",
        "    else:\n",
        "        if isinstance(obj, dict):\n",
        "            fields = obj.get(\"fields\", None) or obj\n",
        "        else:\n",
        "            fields = getattr(obj, \"fields\", None)\n",
        "\n",
        "    if not fields:\n",
        "        return None\n",
        "\n",
        "    # coerce to dict if needed\n",
        "    if not isinstance(fields, dict):\n",
        "        try:\n",
        "            keys = list(fields.keys())\n",
        "            fields = {k: fields[k] for k in keys}\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    available_keys = list(fields.keys())\n",
        "    refl_keys = [k for k in available_keys if is_reflectivity_name(k)]\n",
        "    vel_keys = [k for k in available_keys if is_velocity_name(k)]\n",
        "\n",
        "    # if no reflectivity-like keys, skip (velocity-only)\n",
        "    if not refl_keys:\n",
        "        if VERBOSE:\n",
        "            print(f\"[SKIP] .fields present but no reflectivity-like keys. Example keys: {available_keys}\")\n",
        "        return None\n",
        "\n",
        "    # use the first reflectivity-like key\n",
        "    field_key = refl_keys[0]\n",
        "\n",
        "    # timestamp heuristics\n",
        "    ts = None\n",
        "    if is_radar:\n",
        "        try:\n",
        "            ts = pyart.util.datetimes_from_radar(obj)[0]\n",
        "        except Exception:\n",
        "            ts = getattr(obj, \"time\", None) or getattr(obj, \"timestamp\", None) or None\n",
        "    else:\n",
        "        ts = getattr(obj, \"time\", None) or getattr(obj, \"timestamp\", None) or getattr(obj, \"valid_time\", None)\n",
        "\n",
        "    title = f\"DHR (field={field_key})\"\n",
        "\n",
        "    # helper accessors (work for Radar objects and dict-like)\n",
        "    def _get_raw_field():\n",
        "        # returns the raw field container (dict-like or ndarray)\n",
        "        try:\n",
        "            if is_radar:\n",
        "                return obj.fields.get(field_key)\n",
        "            else:\n",
        "                return fields.get(field_key)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _get_data_array():\n",
        "        raw = _get_raw_field()\n",
        "        if raw is None:\n",
        "            return None\n",
        "        # typical pyart field: dict with \"data\"\n",
        "        try:\n",
        "            if isinstance(raw, dict) and \"data\" in raw:\n",
        "                return np.asarray(raw[\"data\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "        # object with .data attribute\n",
        "        try:\n",
        "            if hasattr(raw, \"data\"):\n",
        "                return np.asarray(getattr(raw, \"data\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "        # raw might be already an array-like\n",
        "        try:\n",
        "            return np.asarray(raw)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _get_gate_xy():\n",
        "        # Prefer obj attributes (Radar), then dict keys (duck-typing).\n",
        "        gx = None; gy = None\n",
        "        try:\n",
        "            if is_radar:\n",
        "                gx = getattr(obj, \"gate_x\", None)\n",
        "                gy = getattr(obj, \"gate_y\", None)\n",
        "            else:\n",
        "                # dict-like: could be keys or attributes\n",
        "                gx = getattr(obj, \"gate_x\", None) if hasattr(obj, \"gate_x\") else (obj.get(\"gate_x\") if isinstance(obj, dict) else None)\n",
        "                gy = getattr(obj, \"gate_y\", None) if hasattr(obj, \"gate_y\") else (obj.get(\"gate_y\") if isinstance(obj, dict) else None)\n",
        "                # also try top-level keys\n",
        "                if gx is None or gy is None:\n",
        "                    gx = (obj.get(\"gate_x\") if isinstance(obj, dict) else gx) or getattr(obj, \"gate_x\", None)\n",
        "                    gy = (obj.get(\"gate_y\") if isinstance(obj, dict) else gy) or getattr(obj, \"gate_y\", None)\n",
        "        except Exception:\n",
        "            gx = gy = None\n",
        "        return gx, gy\n",
        "\n",
        "    # helper to try to find 1D azimuth/range arrays\n",
        "    def _get_azimuth_and_range():\n",
        "        az = None; rg = None\n",
        "        # try attributes first (Radar)\n",
        "        try:\n",
        "            if is_radar:\n",
        "                az = getattr(obj, \"azimuth\", None)\n",
        "                rg = getattr(obj, \"range\", None)\n",
        "            else:\n",
        "                # dict-like keys or attributes\n",
        "                if isinstance(obj, dict):\n",
        "                    az = obj.get(\"azimuth\", None) or obj.get(\"az\", None)\n",
        "                    rg = obj.get(\"range\", None)\n",
        "                else:\n",
        "                    az = getattr(obj, \"azimuth\", None) or getattr(obj, \"az\", None)\n",
        "                    rg = getattr(obj, \"range\", None)\n",
        "        except Exception:\n",
        "            az = rg = None\n",
        "        # unwrap possible pyart lazy containers\n",
        "        try:\n",
        "            if az is not None and hasattr(az, \"data\"):\n",
        "                az = getattr(az, \"data\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if rg is not None and hasattr(rg, \"data\"):\n",
        "                rg = getattr(rg, \"data\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        # coerce to numpy arrays where possible\n",
        "        try:\n",
        "            if az is not None:\n",
        "                az = np.asarray(az)\n",
        "        except Exception:\n",
        "            az = None\n",
        "        try:\n",
        "            if rg is not None:\n",
        "                rg = np.asarray(rg)\n",
        "        except Exception:\n",
        "            rg = None\n",
        "        return az, rg\n",
        "\n",
        "    # unified plot function with extra debug and meshgrid construction\n",
        "    def plot_fn(ax):\n",
        "        try:\n",
        "            if VERBOSE:\n",
        "                print(\"----- plot_fn DEBUG START -----\")\n",
        "                print(\"is_radar:\", is_radar, \"field_key:\", field_key)\n",
        "\n",
        "            raw_field = _get_raw_field()\n",
        "            if VERBOSE:\n",
        "                try:\n",
        "                    rf_repr = repr(raw_field)\n",
        "                    print(\"raw_field type:\", type(raw_field))\n",
        "                    print(\"raw_field repr head:\", rf_repr[:500].replace(\"\\n\", \" \"))\n",
        "                except Exception as _e:\n",
        "                    print(\"Could not repr raw_field:\", _e)\n",
        "\n",
        "            data_arr = _get_data_array()\n",
        "            gx, gy = _get_gate_xy()\n",
        "\n",
        "            # show data array diagnostics\n",
        "            if data_arr is None:\n",
        "                if VERBOSE:\n",
        "                    print(\"[DEBUG] data array is None for field\", field_key)\n",
        "            else:\n",
        "                try:\n",
        "                    darr = np.asarray(data_arr)\n",
        "                    print(\"[DEBUG] data_arr: type=\", type(data_arr), \"ndim=\", darr.ndim,\n",
        "                          \"shape=\", getattr(darr, \"shape\", None),\n",
        "                          \"dtype=\", darr.dtype if hasattr(darr, \"dtype\") else None)\n",
        "                    # quick stats\n",
        "                    try:\n",
        "                        finite_count = np.isfinite(darr).sum()\n",
        "                        total = darr.size\n",
        "                        nan_count = np.isnan(darr).sum() if np.issubdtype(darr.dtype, np.floating) else 0\n",
        "                        print(f\"[DEBUG] data_arr: size={total}, finite={finite_count}, nans={nan_count}\")\n",
        "                        if np.ma.is_masked(darr):\n",
        "                            print(\"[DEBUG] data_arr is a masked array with mask summary:\", getattr(darr.mask, \"sum\", lambda: None)())\n",
        "                    except Exception as _e:\n",
        "                        print(\"Could not compute data stats:\", _e)\n",
        "                except Exception as _e:\n",
        "                    print(\"Could not coerce data_arr to ndarray:\", _e)\n",
        "                    darr = None\n",
        "\n",
        "            # gate diagnostics\n",
        "            if gx is None or gy is None:\n",
        "                if VERBOSE:\n",
        "                    print(\"[DEBUG] gate_x/gate_y are None (not present on object). gx:\", gx, \"gy:\", gy)\n",
        "            else:\n",
        "                try:\n",
        "                    gx_arr = np.asarray(gx)\n",
        "                    gy_arr = np.asarray(gy)\n",
        "                    print(\"[DEBUG] gate_x: type=\", type(gx), \"ndim=\", getattr(gx_arr, \"ndim\", None), \"shape=\", getattr(gx_arr, \"shape\", None), \"dtype=\", gx_arr.dtype if hasattr(gx_arr, \"dtype\") else None)\n",
        "                    print(\"[DEBUG] gate_y: type=\", type(gy), \"ndim=\", getattr(gy_arr, \"ndim\", None), \"shape=\", getattr(gy_arr, \"shape\", None), \"dtype=\", gy_arr.dtype if hasattr(gy_arr, \"dtype\") else None)\n",
        "                    # simple ranges\n",
        "                    try:\n",
        "                        print(\"[DEBUG] gate_x range:\", np.nanmin(gx_arr), \"->\", np.nanmax(gx_arr))\n",
        "                        print(\"[DEBUG] gate_y range:\", np.nanmin(gy_arr), \"->\", np.nanmax(gy_arr))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                except Exception as e:\n",
        "                    print(\"Could not coerce gate_x/gate_y to arrays:\", e)\n",
        "                    gx_arr = gy_arr = None\n",
        "\n",
        "            # Attempt pcolormesh if gate coords look usable\n",
        "            try:\n",
        "                used_pcolormesh = False\n",
        "\n",
        "                # 1) If gate_x/gate_y are numeric 2D arrays, use them directly\n",
        "                if gx is not None and gy is not None and data_arr is not None:\n",
        "                    gx_arr = np.asarray(gx)\n",
        "                    gy_arr = np.asarray(gy)\n",
        "                    darr = np.asarray(data_arr)\n",
        "                    if gx_arr.ndim == 2 and gy_arr.ndim == 2 and darr.ndim == 2:\n",
        "                        if gx_arr.shape == darr.shape and gy_arr.shape == darr.shape:\n",
        "                            if VERBOSE:\n",
        "                                print(\"[DEBUG] gate_x/gate_y 2D match data; attempting pcolormesh.\")\n",
        "                            im = ax.pcolormesh(gx_arr, gy_arr, darr, vmin=VMIN, vmax=VMAX, cmap=cmap, shading='auto')\n",
        "                            if not any(hasattr(a, \"get_ylabel\") and a.get_ylabel() for a in ax.figure.axes):\n",
        "                                ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"dBZ\")\n",
        "                            ax.set_aspect(\"equal\")\n",
        "                            ax.set_xlabel(\"X (m)\")\n",
        "                            ax.set_ylabel(\"Y (m)\")\n",
        "                            used_pcolormesh = True\n",
        "                            if VERBOSE:\n",
        "                                print(f\"[plot_fn] plotted with gate_x/gate_y (pcolormesh) for {title}\")\n",
        "                            print(\"----- plot_fn DEBUG END (pcolormesh successful) -----\")\n",
        "                            return\n",
        "                        else:\n",
        "                            if VERBOSE:\n",
        "                                print(\"[DEBUG] gate_x/gate_y shapes do NOT match data shape.\")\n",
        "                                print(\"       gate_x.shape:\", gx_arr.shape, \"gate_y.shape:\", gy_arr.shape, \"data.shape:\", darr.shape)\n",
        "\n",
        "                # 2) If gate_x/gate_y are not numeric 2D, try to build them from 1D azimuth & range arrays\n",
        "                if (gx is None or gy is None or not (hasattr(gx, \"__array__\") and hasattr(gy, \"__array__\"))) and data_arr is not None:\n",
        "                    if VERBOSE:\n",
        "                        print(\"[DEBUG] Attempting to construct gate_x/gate_y from azimuth & range vectors.\")\n",
        "                    az, rg = _get_azimuth_and_range()\n",
        "                    if az is None or rg is None:\n",
        "                        if VERBOSE:\n",
        "                            print(\"[DEBUG] azimuth or range not found (az, rg):\", az, rg)\n",
        "                    else:\n",
        "                        try:\n",
        "                            # Ensure 1D\n",
        "                            az1 = np.asarray(az).squeeze()\n",
        "                            rg1 = np.asarray(rg).squeeze()\n",
        "                            if az1.ndim == 1 and rg1.ndim == 1:\n",
        "                                if VERBOSE:\n",
        "                                    print(\"[DEBUG] azimuth length:\", az1.size, \"range length:\", rg1.size, \"data shape:\", np.asarray(data_arr).shape)\n",
        "                                # Ensure orientation: data likely (nrays, ngates) = (len(az1), len(rg1))\n",
        "                                if np.asarray(data_arr).shape[0] == az1.size and np.asarray(data_arr).shape[1] == rg1.size:\n",
        "                                    # az in degrees -> radians\n",
        "                                    az_rad = np.deg2rad(az1)\n",
        "                                    # create 2D grids shaped (nrays, ngates)\n",
        "                                    # R: shape (1, ngates) broadcast to (nrays, ngates)\n",
        "                                    R = rg1[None, :]\n",
        "                                    A = az_rad[:, None]\n",
        "                                    gx_new = R * np.sin(A)\n",
        "                                    gy_new = R * np.cos(A)\n",
        "                                    if VERBOSE:\n",
        "                                        print(\"[DEBUG] constructed gate_x/gate_y via meshgrid; shapes:\", gx_new.shape, gy_new.shape)\n",
        "                                        print(\"[DEBUG] gate_x range:\", gx_new.min(), \"->\", gx_new.max())\n",
        "                                        print(\"[DEBUG] gate_y range:\", gy_new.min(), \"->\", gy_new.max())\n",
        "                                    # plot\n",
        "                                    im = ax.pcolormesh(gx_new, gy_new, np.asarray(data_arr), vmin=VMIN, vmax=VMAX, cmap=cmap, shading='auto')\n",
        "                                    if not any(hasattr(a, \"get_ylabel\") and a.get_ylabel() for a in ax.figure.axes):\n",
        "                                        ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"dBZ\")\n",
        "                                    ax.set_aspect(\"equal\")\n",
        "                                    ax.set_xlabel(\"X (m)\")\n",
        "                                    ax.set_ylabel(\"Y (m)\")\n",
        "                                    used_pcolormesh = True\n",
        "                                    if VERBOSE:\n",
        "                                        print(f\"[plot_fn] plotted with constructed gate_x/gate_y (meshgrid) for {title}\")\n",
        "                                    print(\"----- plot_fn DEBUG END (meshgrid pcolormesh successful) -----\")\n",
        "                                    return\n",
        "                                else:\n",
        "                                    if VERBOSE:\n",
        "                                        print(\"[DEBUG] az/range lengths do not match data dims. az.size, rg.size, data.shape:\", az1.size, rg1.size, np.asarray(data_arr).shape)\n",
        "                            else:\n",
        "                                if VERBOSE:\n",
        "                                    print(\"[DEBUG] az or range are not 1D arrays. az.ndim, rg.ndim:\", getattr(az1, 'ndim', None), getattr(rg1, 'ndim', None))\n",
        "                        except Exception:\n",
        "                            if VERBOSE:\n",
        "                                print(\"Exception while constructing gate coords from az/range:\")\n",
        "                                traceback.print_exc()\n",
        "\n",
        "                # 3) If it's a real Radar and pyart plotting is available, try RadarDisplay.plot\n",
        "                if is_radar and pyart is not None:\n",
        "                    try:\n",
        "                        if VERBOSE:\n",
        "                            print(\"[DEBUG] Trying pyart.graph.RadarDisplay.plot() fallback\")\n",
        "                        disp = pyart.graph.RadarDisplay(obj)\n",
        "                        disp.plot(field=field_key, sweep=0, ax=ax, vmin=VMIN, vmax=VMAX, cmap=cmap, colorbar_flag=True)\n",
        "                        ax.set_aspect('equal')\n",
        "                        if VERBOSE:\n",
        "                            print(f\"[plot_fn] used pyart RadarDisplay.plot for {title}\")\n",
        "                        print(\"----- plot_fn DEBUG END (RadarDisplay.plot successful) -----\")\n",
        "                        return\n",
        "                    except Exception:\n",
        "                        if VERBOSE:\n",
        "                            print(\"RadarDisplay.plot raised an exception, stack follows:\")\n",
        "                            traceback.print_exc()\n",
        "\n",
        "                # Final fallback: imshow of 2D array\n",
        "                if data_arr is not None:\n",
        "                    try:\n",
        "                        if VERBOSE:\n",
        "                            print(\"[DEBUG] Using imshow fallback now.\")\n",
        "                        im = ax.imshow(np.asarray(data_arr), origin=\"lower\", vmin=VMIN, vmax=VMAX, cmap=cmap, interpolation=\"nearest\")\n",
        "                        if not any(hasattr(a, \"get_ylabel\") and a.get_ylabel() for a in ax.figure.axes):\n",
        "                            ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"dBZ\")\n",
        "                        ax.set_xlabel(\"Range index\")\n",
        "                        ax.set_ylabel(\"Azimuth index\")\n",
        "                        ax.set_aspect(\"auto\")\n",
        "                        if VERBOSE:\n",
        "                            print(f\"[plot_fn] used imshow fallback for {title}\")\n",
        "                        print(\"----- plot_fn DEBUG END (imshow used) -----\")\n",
        "                        return\n",
        "                    except Exception:\n",
        "                        if VERBOSE:\n",
        "                            print(\"imshow fallback failed, stack follows:\")\n",
        "                            traceback.print_exc()\n",
        "\n",
        "                # Nothing plotted\n",
        "                if VERBOSE:\n",
        "                    print(f\"[plot_fn] nothing plottable for {title}\")\n",
        "                    print(\"----- plot_fn DEBUG END (nothing plotted) -----\")\n",
        "        except Exception:\n",
        "            print(\"Unexpected error inside plot_fn:\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return {\"plot_fn\": plot_fn, \"title\": title, \"timestamp\": ts}\n",
        "\n",
        "\n",
        "\n",
        "def normalize_timestamp(ts):\n",
        "    \"\"\"\n",
        "    Convert ts -> datetime or None.\n",
        "    Handles: datetime, numpy.datetime64, list/array (take first), numeric epoch,\n",
        "    ISO-like strings, or returns None for dicts / unknown formats.\n",
        "    \"\"\"\n",
        "    if ts is None:\n",
        "        return None\n",
        "\n",
        "    # datetime already\n",
        "    if isinstance(ts, datetime):\n",
        "        return ts\n",
        "\n",
        "    # numpy datetime64\n",
        "    try:\n",
        "        if np and isinstance(ts, np.datetime64):\n",
        "            # convert numpy datetime64 -> datetime (UTC)\n",
        "            return datetime.utcfromtimestamp((ts.astype('datetime64[s]').astype('int')))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # list/tuple/ndarray -> try first element\n",
        "    if isinstance(ts, (list, tuple)) or (hasattr(ts, \"shape\") and getattr(ts, \"ndim\", None) is not None):\n",
        "        try:\n",
        "            if len(ts) == 0:\n",
        "                return None\n",
        "            return normalize_timestamp(ts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # numbers -> treat as epoch seconds\n",
        "    if isinstance(ts, (int, float, np.integer, np.floating)):\n",
        "        try:\n",
        "            return datetime.utcfromtimestamp(float(ts))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # strings -> try iso / common patterns\n",
        "    if isinstance(ts, str):\n",
        "        s = ts.strip()\n",
        "        # common trailing Z or timezone like +00:00 or +0000\n",
        "        if s.endswith(\"Z\"):\n",
        "            s = s[:-1]\n",
        "        # attempt fromisoformat (handles YYYY-MM-DDTHH:MM:SS[.ffffff][+HH:MM])\n",
        "        try:\n",
        "            return datetime.fromisoformat(s)\n",
        "        except Exception:\n",
        "            # try to parse numeric string epoch\n",
        "            try:\n",
        "                f = float(s)\n",
        "                return datetime.utcfromtimestamp(f)\n",
        "            except Exception:\n",
        "                # try compact pattern like YYYYMMDDHHMMSS or YYYYMMDD_HHMMSS\n",
        "                m = re.search(r'(\\d{4})(\\d{2})(\\d{2})[T_ -]?(\\d{2})(\\d{2})(\\d{2})', s)\n",
        "                if m:\n",
        "                    y,mo,d,h,mi,se = map(int, m.groups())\n",
        "                    try:\n",
        "                        return datetime(y, mo, d, h, mi, se)\n",
        "                    except Exception:\n",
        "                        return None\n",
        "                return None\n",
        "\n",
        "    # dicts and other mapping types -> give up\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def adapt_pickle_object(obj, name=None):\n",
        "    a = adapt_pyart_radar(obj)\n",
        "    if a:\n",
        "        return a\n",
        "    # If we get here, adaptation failed -> show debug summary if verbose\n",
        "    if VERBOSE:\n",
        "        print(f\"[ADAPT FAIL] object not adapted for file: {name}\")\n",
        "        debug_object_summary(obj, label=name)\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- LOAD & RENDER ----------------\n",
        "def load_frames(storm_dir):\n",
        "    frames = []\n",
        "    filename_velocity_patterns = (\"_n0s_\", \"_vel\", \"_velocity\", \"_v_\", \"_nvw\")\n",
        "    for fn in sorted(os.listdir(storm_dir)):\n",
        "        lowfn = fn.lower()\n",
        "        if any(pat in lowfn for pat in filename_velocity_patterns):\n",
        "            if VERBOSE:\n",
        "                print(f\"[FILE SKIP] filename indicates velocity/metadata scan, skipping: {fn}\")\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(storm_dir, fn)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "\n",
        "        if not (lowfn.endswith((\".pkl\", \".pickle\", \".h5\", \".hdf5\", \".hdf\"))):\n",
        "            continue\n",
        "\n",
        "        obj = None\n",
        "        if lowfn.endswith((\".pkl\", \".pickle\")):\n",
        "            try:\n",
        "                with open(path, \"rb\") as f:\n",
        "                    obj = pickle.load(f)\n",
        "            except Exception as e:\n",
        "                if VERBOSE:\n",
        "                    print(f\"[skip] pickle load failed for {fn}: {e}\")\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        adapted = adapt_pickle_object(obj, name=fn)\n",
        "        if adapted is None:\n",
        "            if VERBOSE:\n",
        "                print(f\"[skip] could not adapt {fn} to a plottable DHR\")\n",
        "            continue\n",
        "\n",
        "        # normalize timestamp BEFORE appending\n",
        "        raw_ts = adapted.get(\"timestamp\")\n",
        "        ts = normalize_timestamp(raw_ts) or parse_timestamp_from_name(fn)\n",
        "        if VERBOSE and raw_ts is not None and ts is None:\n",
        "            print(f\"[TS WARN] could not normalize timestamp from adapter for {fn}; raw type: {type(raw_ts)}\")\n",
        "\n",
        "        frames.append({\n",
        "            \"path\": path,\n",
        "            \"plot_fn\": adapted[\"plot_fn\"],\n",
        "            \"title\": adapted.get(\"title\", fn),\n",
        "            \"timestamp\": ts,\n",
        "            \"name\": fn\n",
        "        })\n",
        "\n",
        "    # sort chronologically: (None last), then timestamp, then filename\n",
        "    frames.sort(key=lambda d: (d[\"timestamp\"] is None, d[\"timestamp\"] or datetime.max, d[\"name\"]))\n",
        "    return frames\n",
        "\n",
        "\n",
        "def render_frames(frames, figsize=FIGSIZE, dpi=DPI):\n",
        "    \"\"\"\n",
        "    Render each frame to an RGB numpy array (H,W,3) robust to different\n",
        "    Matplotlib backends / versions.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for i, fr in enumerate(frames, 1):\n",
        "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
        "        fr[\"plot_fn\"](ax)\n",
        "        ts = fr[\"timestamp\"]\n",
        "        title = TITLE_FMT.format(title=fr[\"title\"], ts=ts) if ts else fr[\"title\"]\n",
        "        ax.set_title(title, fontsize=11)\n",
        "        fig.tight_layout(pad=PAD_INCH)\n",
        "        # draw the canvas\n",
        "        fig.canvas.draw()\n",
        "\n",
        "        # Try the simplest/fastest method first\n",
        "        im_array = None\n",
        "        try:\n",
        "            # preferred, but not always present\n",
        "            s = fig.canvas.tostring_rgb()\n",
        "            w, h = fig.canvas.get_width_height()\n",
        "            im_array = np.frombuffer(s, dtype=np.uint8).reshape(h, w, 3)\n",
        "        except Exception:\n",
        "            # fallback: use print_to_buffer() which returns a RGBA/ARGB buffer\n",
        "            try:\n",
        "                buf, (w, h) = fig.canvas.print_to_buffer()\n",
        "                arr4 = np.frombuffer(buf, dtype=np.uint8).reshape(h, w, 4)\n",
        "                # detect which channel is alpha (the one with mean closest to 255)\n",
        "                means = arr4.mean(axis=(0, 1))\n",
        "                alpha_idx = int(np.argmax(means))\n",
        "                # if alpha channel looks like full-255 but other channels also might be high,\n",
        "                # we still remove the detected alpha channel and keep the other three in order.\n",
        "                rgb = arr4.take([j for j in range(4) if j != alpha_idx], axis=2)\n",
        "                im_array = rgb\n",
        "            except Exception as e:\n",
        "                # last resort: try tostring_argb (older builds)\n",
        "                try:\n",
        "                    s2 = fig.canvas.tostring_argb()\n",
        "                    w, h = fig.canvas.get_width_height()\n",
        "                    arr4 = np.frombuffer(s2, dtype=np.uint8).reshape(h, w, 4)\n",
        "                    # ARGB -> drop A and reorder to RGB from positions [1,2,3]\n",
        "                    im_array = arr4[:, :, [1, 2, 3]]\n",
        "                except Exception as e2:\n",
        "                    plt.close(fig)\n",
        "                    raise RuntimeError(\"Failed to extract image buffer from Matplotlib canvas: \"\n",
        "                                       f\"{e} / {e2}\") from e2\n",
        "\n",
        "        # ensure we have uint8 HxWx3\n",
        "        if im_array is None:\n",
        "            plt.close(fig)\n",
        "            raise RuntimeError(\"Failed to capture figure image (no buffer produced).\")\n",
        "\n",
        "        # If we have 4 channels left for some reason, drop the last channel\n",
        "        if im_array.ndim == 3 and im_array.shape[2] == 4:\n",
        "            im_array = im_array[:, :, :3]\n",
        "\n",
        "        # If image is not contiguous / is read-only, copy it\n",
        "        im_array = np.ascontiguousarray(im_array, dtype=np.uint8)\n",
        "\n",
        "        images.append(im_array.copy())\n",
        "        plt.close(fig)\n",
        "        print(f\"Rendered {i}/{len(frames)}: {fr['name']}\")\n",
        "    return images\n",
        "\n",
        "\n",
        "def write_gif(images, out_path, fps=FPS):\n",
        "    duration = 1.0 / fps\n",
        "    imageio.mimsave(out_path, images, format=\"GIF\", duration=duration)\n",
        "    print(f\"GIF saved -> {out_path}\")\n",
        "\n",
        "def write_mp4(images, out_path, fps=FPS):\n",
        "    try:\n",
        "        import imageio_ffmpeg\n",
        "        writer = imageio.get_writer(out_path, fps=fps, codec=\"libx264\", quality=8, format=\"FFMPEG\")\n",
        "        for im in images:\n",
        "            writer.append_data(im)\n",
        "        writer.close()\n",
        "        print(f\"MP4 saved -> {out_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"MP4 failed (need ffmpeg): {e}\")\n",
        "\n",
        "# Main run\n",
        "if __name__ == \"__main__\":\n",
        "    frames = load_frames(STORM_DIR)\n",
        "    if not frames:\n",
        "        raise SystemExit(\"No plottable DHR frames found. Check adapters/filters.\")\n",
        "    images = render_frames(frames)\n",
        "    write_gif(images, OUT_GIF, fps=FPS)\n",
        "    write_mp4(images, OUT_MP4, fps=FPS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pacT8GMQjUK"
      },
      "source": [
        "#### **View NEXRAD Level-2 Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKK0nnAeO2QH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Afa4_HVwWVy"
      },
      "source": [
        "#### **View Synoptic Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9305
        },
        "id": "CGFEoadcwWVz",
        "outputId": "8849d0f7-9260-431e-d565-5a16e996013e"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Isolate building and send requests via synoptic API, so that we can manually test.\n",
        "'''\n",
        "\n",
        "\n",
        "################################################################ HELPER NETWORK-VIEWER FUNCTION #########################################################################################\n",
        "\n",
        "\n",
        "def summarize_networks(nets_json, top_n=None, min_reporting=0, exclude_restricted=True, show_example=10):\n",
        "    \"\"\"\n",
        "    nets_json : dict  -- the direct r.json() result from /v2/networks\n",
        "    Returns a dataframe and prints pretty output.\n",
        "    \"\"\"\n",
        "    # Accept multiple possible shapes: 'MNET' (your dump) or a direct list\n",
        "    raw_list = None\n",
        "    if isinstance(nets_json, dict):\n",
        "        if \"MNET\" in nets_json and isinstance(nets_json[\"MNET\"], list):\n",
        "            raw_list = nets_json[\"MNET\"]\n",
        "        elif \"NETWORKS\" in nets_json and isinstance(nets_json[\"NETWORKS\"], list):\n",
        "            raw_list = nets_json[\"NETWORKS\"]\n",
        "        else:\n",
        "            # fallback: find first list-of-dicts child\n",
        "            for v in nets_json.values():\n",
        "                if isinstance(v, list) and v and isinstance(v[0], dict):\n",
        "                    raw_list = v\n",
        "                    break\n",
        "    if raw_list is None:\n",
        "        raise ValueError(\"Can't find networks list in the JSON you passed. Print the JSON keys and re-run.\")\n",
        "\n",
        "    rows = []\n",
        "    for n in raw_list:\n",
        "        def g(key, default=None):\n",
        "            return n.get(key, default)\n",
        "\n",
        "        # Some fields are strings in the API; coerce safely\n",
        "        def to_int(x):\n",
        "            try:\n",
        "                return int(x)\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        # period_of_record may be a dict\n",
        "        period = g(\"PERIOD_OF_RECORD\") or {}\n",
        "\n",
        "        # safe percent parse\n",
        "        pa = g(\"PERCENT_ACTIVE\")\n",
        "        try:\n",
        "            pa_val = float(pa) if pa not in (None, \"\") else None\n",
        "        except Exception:\n",
        "            pa_val = None\n",
        "\n",
        "        rows.append({\n",
        "            \"ID\":                g(\"ID\"),\n",
        "            \"SHORTNAME\":         g(\"SHORTNAME\"),\n",
        "            \"LONGNAME\":          g(\"LONGNAME\"),\n",
        "            \"CATEGORY\":          g(\"CATEGORY\"),\n",
        "            \"LAST_OBSERVATION\":  g(\"LAST_OBSERVATION\"),\n",
        "            \"REPORTING_STATIONS\": to_int(g(\"REPORTING_STATIONS\")),\n",
        "            \"ACTIVE_STATIONS\":    to_int(g(\"ACTIVE_STATIONS\")),\n",
        "            \"TOTAL_STATIONS\":     to_int(g(\"TOTAL_STATIONS\")),\n",
        "            \"PERCENT_ACTIVE\":     pa_val,\n",
        "            \"ACTIVE_RESTRICTED\":  to_int(g(\"ACTIVE_RESTRICTED\") or 0),\n",
        "            \"TOTAL_RESTRICTED\":   to_int(g(\"TOTAL_RESTRICTED\") or 0),\n",
        "            \"PERIOD_START\":       period.get(\"start\"),\n",
        "            \"PERIOD_END\":         period.get(\"end\"),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # normalize types & sort\n",
        "    df[\"REPORTING_STATIONS\"] = df[\"REPORTING_STATIONS\"].fillna(0).astype(int)\n",
        "    df[\"ACTIVE_RESTRICTED\"]  = df[\"ACTIVE_RESTRICTED\"].fillna(0).astype(int)\n",
        "    df[\"TOTAL_RESTRICTED\"]   = df[\"TOTAL_RESTRICTED\"].fillna(0).astype(int)\n",
        "\n",
        "    df = df.sort_values(by=\"REPORTING_STATIONS\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Pretty print â€” trimmed columns\n",
        "    display_cols = [\"ID\",\"SHORTNAME\",\"LONGNAME\",\"REPORTING_STATIONS\",\"ACTIVE_STATIONS\",\n",
        "                    \"TOTAL_STATIONS\",\"PERCENT_ACTIVE\",\"ACTIVE_RESTRICTED\",\"TOTAL_RESTRICTED\",\"LAST_OBSERVATION\"]\n",
        "    print(\"\\nTop networks (sorted by REPORTING_STATIONS):\\n\")\n",
        "    print(df[display_cols].head(top_n or show_example).to_string(index=False))\n",
        "\n",
        "    # Candidate pick: dense and not restricted (configurable)\n",
        "    candidates = df[\n",
        "        (df[\"REPORTING_STATIONS\"] >= min_reporting)\n",
        "    ]\n",
        "    if exclude_restricted:\n",
        "        candidates = candidates[(candidates[\"ACTIVE_RESTRICTED\"] == 0) & (candidates[\"TOTAL_RESTRICTED\"] == 0)]\n",
        "\n",
        "    print(\"\\n\\nPromising candidate networks (dense & not restricted):\\n\")\n",
        "    if len(candidates):\n",
        "        print(candidates[[\"ID\",\"SHORTNAME\",\"REPORTING_STATIONS\",\"ACTIVE_RESTRICTED\",\"TOTAL_RESTRICTED\"]].head(30).to_string(index=False))\n",
        "    else:\n",
        "        print(\"  (none matched your filters â€” try lowering min_reporting or allow restricted networks)\")\n",
        "\n",
        "    return df, candidates\n",
        "\n",
        "\n",
        "############################################################################################################################################################################\n",
        "\n",
        "\n",
        "bounding_lat = (33.61583, 37.00230)\n",
        "bounding_lon = (-103.00257, -94.43121)\n",
        "center_lat, center_lon = 36.0568, -98.3178   # midpoint of those\n",
        "                  #  YYYY  MM DD HH MM\n",
        "scan_time = datetime(2024, 9, 6, 4, 0)      # an evening supercell time\n",
        "\n",
        "'''\n",
        "# Look at available variables\n",
        "r = requests.get(\n",
        "    \"https://api.synopticdata.com/v2/variables\",\n",
        "    params={\"token\": SYNOPTIC_TOKEN}\n",
        ")\n",
        "r.raise_for_status()\n",
        "\n",
        "# Parse the JSON body\n",
        "resp = r.json()\n",
        "\n",
        "# Pretty-print the JSON to explore its keys\n",
        "print(json.dumps(resp, indent=2))\n",
        "'''\n",
        "\n",
        "# Look at available networks\n",
        "r = requests.get(\n",
        "  \"https://api.synopticdata.com/v2/networks\",\n",
        "  params={\"token\": SYNOPTIC_TOKEN}\n",
        ")\n",
        "summarize_networks(r.json())\n",
        "\n",
        "# Manually call the function to test\n",
        "synoptic_test_df = filter_synoptic(\n",
        "    bounding_lat=bounding_lat,\n",
        "    bounding_lon=bounding_lon,\n",
        "    center_lat=center_lat,\n",
        "    center_lon=center_lon,\n",
        "    scan_time=scan_time,\n",
        "    time_window=timedelta(minutes=5),\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\nReturned DataFrame:\")\n",
        "display(synoptic_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9vDBMHkt_k"
      },
      "source": [
        "#### **View LSR Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iKePevnUkt_l",
        "outputId": "6f4e59d5-9bb0-4b6f-bdc3-9127c390dfc9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "#                YYYY MM DD\n",
        "start = datetime(2020, 4, 12)\n",
        "end =   datetime(2020, 4, 14)\n",
        "debug=True\n",
        "\n",
        "base_url = \"https://mesonet.agron.iastate.edu/cgi-bin/request/gis/lsr.py\"\n",
        "\n",
        "# Format start/end as ISO8601 with trailing Z\n",
        "sts = start.strftime('%Y-%m-%dT%H:%MZ')\n",
        "ets = (end + timedelta(seconds=1)).strftime('%Y-%m-%dT%H:%MZ')\n",
        "\n",
        "params = {\n",
        "    'wfo': 'ALL',\n",
        "    'sts': sts,\n",
        "    'ets': ets,\n",
        "    'fmt': 'csv'\n",
        "}\n",
        "\n",
        "if debug: print(f\"\\n Fetching LSRs from {sts} to {ets} with params {params} \\n\")\n",
        "\n",
        "# Fetch with requests\n",
        "resp = requests.get(base_url, params=params)\n",
        "resp.raise_for_status()\n",
        "if debug:\n",
        "    print(\"Fetch HTTP\", resp.status_code, \"| Response snippet:\",\n",
        "          resp.text[:200].replace('\\n',' '))\n",
        "\n",
        "# Read into pandas via StringIO\n",
        "df = pd.read_csv(\n",
        "    StringIO(resp.text),\n",
        "    parse_dates=['VALID'],\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "if debug: print(f\"{len(df)} total reports before filtering\")\n",
        "\n",
        "# Filter for thunderstorm-wind events with MG or EG\n",
        "if debug:     display(df['TYPETEXT'].value_counts())\n",
        "keep_codes = ['tstm wind',\n",
        "              'tstm wnd gst',\n",
        "              'non-tstm wnd gst',\n",
        "              'marine tstm wind',\n",
        "              'tornado'\n",
        "              ]\n",
        "df = df[df['TYPETEXT'].str.lower().isin(keep_codes)]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n",
        "################################################################################################################################################################################\n",
        "\n",
        "\n",
        "# count by state\n",
        "state_counts = df['STATE'].value_counts()\n",
        "\n",
        "# print â€œstate: amountâ€ style lines\n",
        "for state, cnt in state_counts.items():\n",
        "    print(f\"{state}: {cnt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5i3T41QWQ51"
      },
      "source": [
        "#### **View SPC Dataset Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "P7Ndb3tVWQ52",
        "outputId": "bc36554e-f0e3-4d16-e8e5-d46000cecaef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "############################################################################# RAW #############################################################################\n",
        "\n",
        "\n",
        "# Load storm reports into DataFrame\n",
        "csv_path = \"Datasets/surface_obs_datasets/spc_reports/2024_wind.csv\"\n",
        "spc_raw_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(spc_raw_df)\n",
        "\n",
        "\n",
        "############################################################################ LOADED ##############################################################################\n",
        "\n",
        "start=datetime(2024, 12, 27)\n",
        "end=  datetime(2024, 12, 29)\n",
        "\n",
        "# Load the lsr dataframe once, and save it\n",
        "spc_df = load_raw_spc(\n",
        "                # YYYY MM DD\n",
        "    start=start,\n",
        "    end  =end,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "############################################################################ FILTERED #############################################################################\n",
        "\n",
        "\n",
        "bounding_lat = (24.396308, 49.384358)\n",
        "bounding_lon = (-124.848974, -66.93457)\n",
        "center_lat, center_lon = 36.0568, -98.3178   # midpoint of those\n",
        "                  #  YYYY  MM DD HH MM\n",
        "scan_time = datetime(2024, 12, 28, 12, 0)      # an evening supercell time\n",
        "\n",
        "spc_df = filter_spc(\n",
        "    spc_df=spc_df,\n",
        "    bounding_lat=bounding_lat,\n",
        "    bounding_lon=bounding_lon,\n",
        "    center_lat=center_lat,\n",
        "    center_lon=center_lon,\n",
        "    scan_time=scan_time,\n",
        "    time_window=timedelta(minutes=1440),\n",
        "    debug=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ddavf8_kqYrP"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "storm250",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
